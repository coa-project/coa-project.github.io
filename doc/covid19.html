<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>coa.covid19 API documentation</title>
<meta name="description" content="Project : PyCoA
Date :
april 2020 - march 2022
Authors : Olivier Dadoun, Julien Browaeys, Tristan Beau
Copyright ©pycoa.fr
License: See joint …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>coa.covid19</code></h1>
</header>
<section id="section-intro">
<p>Project : PyCoA
Date :
april 2020 - march 2022
Authors : Olivier Dadoun, Julien Browaeys, Tristan Beau
Copyright ©pycoa.fr
License: See joint LICENSE file</p>
<p>Module : coa.covid19</p>
<h2 id="about">About :</h2>
<p>Main class definitions for covid19 dataset access. Currently, we are only using the JHU CSSE data.
The parser class gives a simplier access through an already filled dict of data</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
Project : PyCoA
Date :    april 2020 - march 2022
Authors : Olivier Dadoun, Julien Browaeys, Tristan Beau
Copyright ©pycoa.fr
License: See joint LICENSE file

Module : coa.covid19

About :
-------

Main class definitions for covid19 dataset access. Currently, we are only using the JHU CSSE data.
The parser class gives a simplier access through an already filled dict of data

&#34;&#34;&#34;

import pandas
from collections import defaultdict
import numpy as np
import pandas as pd
import datetime as dt

import sys
from coa.tools import info, verb, kwargs_test, get_local_from_url, fill_missing_dates, check_valid_date, week_to_date, get_db_list_dict

import coa.geo as coge
import coa.dbinfo as report
import coa.display as codisplay
from coa.error import *
from scipy import stats as sps
import random
from functools import reduce
import collections
from bs4 import BeautifulSoup
import json
import requests
import datetime
import math

class DataBase(object):
   &#34;&#34;&#34;
   DataBase class
   Parse a Covid-19 database and filled the pandas python objet : mainpandas
   It takes a string argument, which can be: &#39;jhu&#39;,&#39;spf&#39;, &#39;spfnational&#39;,&#39;owid&#39;, &#39;opencovid19&#39; and &#39;opencovid19national&#39;
   &#34;&#34;&#34;
   def __init__(self, db_name):
        &#34;&#34;&#34;
         Fill the pandas_datase
        &#34;&#34;&#34;
        verb(&#34;Init of covid19.DataBase()&#34;)
        self.database_name = list(get_db_list_dict().keys())
        self.database_type = get_db_list_dict()
        self.available_options = [&#39;nonneg&#39;, &#39;nofillnan&#39;, &#39;smooth7&#39;, &#39;sumall&#39;]
        self.available_keys_words = []
        self.dates = []
        self.database_columns_not_computed = {}
        self.db = db_name
        self.geo_all = &#39;&#39;
        self.database_url = []
        self.db_world=None
        self.databaseinfo = report
        if self.db not in self.database_name:
            raise CoaDbError(&#39;Unknown &#39; + self.db + &#39;. Available database so far in PyCoa are : &#39; + str(self.database_name), file=sys.stderr)
        else:
            try:
                if get_db_list_dict()[self.db][1] == &#39;nation&#39;: # world wide db
                    self.db_world = True
                    self.geo = coge.GeoManager(&#39;name&#39;)
                    self.geo_all = &#39;world&#39;
                else: # local db
                    self.db_world = False
                    self.geo = coge.GeoCountry(get_db_list_dict()[self.db][0])
                    if get_db_list_dict()[self.db][1] == &#39;region&#39;:
                        self.geo_all = self.geo.get_region_list()
                    elif get_db_list_dict()[self.db][1] == &#39;subregion&#39;:
                        self.geo_all = self.geo.get_subregion_list()
                    else:
                        CoaError(&#39;Granularity problem, neither region or subregion&#39;)
                self.set_display(self.db,self.geo)

                # specific reading of data according to the db
                if self.db == &#39;jhu&#39;:
                    info(&#39;JHU aka Johns Hopkins database selected ...&#39;)
                    self.return_jhu_pandas()
                elif self.db == &#39;jhu-usa&#39;: #USA
                    info(&#39;USA, JHU aka Johns Hopkins database selected ...&#39;)
                    self.return_jhu_pandas()
                elif self.db == &#39;imed&#39;:
                    info(&#39;Greece, imed database selected ...&#39;)
                    self.return_jhu_pandas()
                elif self.db == &#39;govcy&#39;: #CYP
                    info(&#39;Cyprus, govcy database selected ...&#39;)
                    rename_dict = {&#39;daily deaths&#39;: &#39;tot_deaths&#39;}
                    gov = self.csv2pandas(&#39;https://www.data.gov.cy/sites/default/files/CY%20Covid19%20Open%20Data%20-%20Extended%20-%20new_247.csv&#39;
                    ,separator=&#39;,&#39;)
                    columns_keeped = [&#39;tot_deaths&#39;]
                    gov[&#39;tot_deaths&#39;]=gov.groupby([&#39;location&#39;])[&#39;daily deaths&#39;].cumsum()
                    self.return_structured_pandas(gov, columns_keeped=columns_keeped)
                elif self.db == &#39;dpc&#39;: #ITA
                    info(&#39;ITA, Dipartimento della Protezione Civile database selected ...&#39;)
                    rename_dict = {&#39;data&#39;: &#39;date&#39;, &#39;denominazione_regione&#39;: &#39;location&#39;, &#39;totale_casi&#39;: &#39;tot_cases&#39;,&#39;deceduti&#39;:&#39;tot_deaths&#39;}
                    dpc1 = self.csv2pandas(&#39;https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv&#39;,\
                    rename_columns = rename_dict, separator=&#39;,&#39;)
                    #dpc1 = self.csv2pandas(&#34;https://github.com/pcm-dpc/COVID-19/raw/master/dati-province/dpc-covid19-ita-province.csv&#34;,\
                    columns_keeped = [&#39;tot_deaths&#39;,&#39;tot_cases&#39;]
                    self.return_structured_pandas(dpc1, columns_keeped=columns_keeped)
                elif self.db == &#39;rki&#39;: # DEU
                    info(&#39;DEU, Robert Koch Institut data selected ...&#39;)
                    self.return_jhu_pandas()
                elif self.db == &#39;dgs&#39;: # PRT
                    info(&#39;PRT, Direcção Geral de Saúde - Ministério da Saúde Português data selected ...&#39;)
                    rename_dict = {&#39;data&#39;: &#39;date&#39;,&#39;concelho&#39;:&#39;location&#39;,&#39;confirmados_1&#39;:&#39;tot_cases&#39;}
                    url=&#39;https://raw.githubusercontent.com/dssg-pt/covid19pt-data/master/data_concelhos_new.csv&#39;
                    prt_data=self.csv2pandas(url,separator=&#39;,&#39;,rename_columns = rename_dict)
                    columns_keeped = [&#39;tot_cases&#39;]
                    self.return_structured_pandas(prt_data, columns_keeped=columns_keeped)
                elif self.db == &#39;obepine&#39; : # FRA
                    info(&#39;FRA, réseau Obepine, surveillance Sars-Cov-2 dans les eaux usées&#39;)
                    url=&#39;https://www.data.gouv.fr/fr/datasets/r/69b8af15-c8c5-465a-bdb6-1ac73430e590&#39;
                    #url=&#39;https://www.data.gouv.fr/fr/datasets/r/89196725-56cf-4a83-bab0-170ad1e8ef85&#39;
                    rename_dict={&#39;Code_Region&#39;:&#39;location&#39;,&#39;Date&#39;:&#39;date&#39;,&#39;Indicateur\&#34;&#39;:&#39;idx_obepine&#39;}
                    cast = {&#39;Code_Region&#39;: &#39;string&#39;}
                    obepine_data=self.csv2pandas(url,cast=cast,separator=&#39;;&#39;,rename_columns=rename_dict)
                    obepine_data[&#39;idx_obepine&#39;]=obepine_data[&#39;idx_obepine&#39;].astype(float)
                    self.return_structured_pandas(obepine_data,columns_keeped=[&#39;idx_obepine&#39;])
                elif self.db == &#39;escovid19data&#39;: # ESP
                    info(&#39;ESP, EsCovid19Data ...&#39;)
                    rename_dict = {&#39;ine_code&#39;: &#39;location&#39;,\
                        &#39;deceased&#39;:&#39;tot_deaths&#39;,\
                        &#39;cases_accumulated_PCR&#39;:&#39;tot_cases&#39;,\
                        &#39;hospitalized&#39;:&#39;cur_hosp&#39;,\
                        &#39;hospitalized_accumulated&#39;:&#39;tot_hosp&#39;,\
                        &#39;intensive_care&#39;:&#39;cur_icu&#39;,\
                        &#39;recovered&#39;:&#39;tot_recovered&#39;,\
                        &#39;cases_per_cienmil&#39;:&#39;tot_cases_per100k&#39;,\
                        &#39;intensive_care_per_1000000&#39;:&#39;cur_icu_per1M&#39;,\
                        &#39;deceassed_per_100000&#39;:&#39;tot_deaths_per100k&#39;,\
                        &#39;hospitalized_per_100000&#39;:&#39;cur_hosp_per100k&#39;,\
                        &#39;ia14&#39;:&#39;incidence&#39;,\
                        &#39;poblacion&#39;:&#39;population&#39;,\
                    }
                    #url=&#39;https://github.com/montera34/escovid19data/raw/master/data/output/covid19-provincias-spain_consolidated.csv&#39;
                    url=&#39;https://raw.githubusercontent.com/montera34/escovid19data/master/data/output/covid19-provincias-spain_consolidated.csv&#39;
                    col_names = pd.read_csv(get_local_from_url(url), nrows=0).columns
                    cast={i:&#39;string&#39; for i in col_names[17:]}
                    esp_data=self.csv2pandas(url,\
                        separator=&#39;,&#39;,rename_columns = rename_dict,cast = cast)
                    #print(&#39;Available columns : &#39;)
                    #display(esp_data.columns)
                    esp_data[&#39;location&#39;]=esp_data.location.astype(str).str.zfill(2)
                    columns_keeped = list(rename_dict.values())
                    columns_keeped.remove(&#39;location&#39;)

                    for w in list(columns_keeped):
                            esp_data[w]=pd.to_numeric(esp_data[w], errors = &#39;coerce&#39;)

                    self.return_structured_pandas(esp_data,columns_keeped=columns_keeped)

                elif self.db == &#39;sciensano&#39;: #Belgian institute for health,
                    info(&#39;BEL, Sciensano Belgian institute for health data  ...&#39;)
                    rename_dict = { &#39;DATE&#39; : &#39;date&#39;,\
                    &#39;PROVINCE&#39;:&#39;location&#39;,\
                    &#39;TOTAL_IN&#39;:&#39;cur_hosp&#39;,
                    &#39;TOTAL_IN_ICU&#39;:&#39;cur_icu&#39;,
                    &#39;TOTAL_IN_RESP&#39;:&#39;cur_resp&#39;,
                    &#39;TOTAL_IN_ECMO&#39;:&#39;cur_ecmo&#39;}
                    url=&#39;https://epistat.sciensano.be/Data/COVID19BE_HOSP.csv&#39;
                    beldata=self.csv2pandas(url,separator=&#39;,&#39;,rename_columns=rename_dict)
                    [rename_dict.pop(i) for i in [&#39;DATE&#39;,&#39;PROVINCE&#39;]]
                    columns_keeped = list(rename_dict.values())
                    cvsloc2jsonloc={
                    &#39;BrabantWallon&#39;:&#39;Brabant wallon (le)&#39;,\
                    &#39;Brussels&#39;:&#39;Région de Bruxelles-Capitale&#39;,\
                    &#39;Limburg&#39;:&#39;Limbourg (le)&#39;,\
                    &#39;OostVlaanderen&#39;:&#39;Flandre orientale (la)&#39;,\
                    &#39;Hainaut&#39;:&#39;Hainaut (le)&#39;,\
                    &#39;VlaamsBrabant&#39;:&#39;Brabant flamand (le)&#39;,\
                    &#39;WestVlaanderen&#39;:&#39;Flandre occidentale (la)&#39;,\
                    }
                    beldata[&#34;location&#34;].replace(cvsloc2jsonloc, inplace=True)
                    beldata[&#39;date&#39;] = pandas.to_datetime(beldata[&#39;date&#39;],errors=&#39;coerce&#39;).dt.date
                    self.return_structured_pandas(beldata,columns_keeped=columns_keeped)
                elif self.db == &#39;phe&#39;: # GBR from owid
                    info(&#39;GBR, Public Health England data ...&#39;)
                    rename_dict = { &#39;areaCode&#39;:&#39;location&#39;,\
                        &#39;cumDeaths28DaysByDeathDate&#39;:&#39;tot_deaths&#39;,\
                        &#39;cumCasesBySpecimenDate&#39;:&#39;tot_cases&#39;,\
                        &#39;cumLFDTestsBySpecimenDate&#39;:&#39;tot_tests&#39;,\
                        &#39;cumPeopleVaccinatedFirstDoseByVaccinationDate&#39;:&#39;tot_vacc1&#39;,\
                        &#39;cumPeopleVaccinatedSecondDoseByVaccinationDate&#39;:&#39;tot_vacc2&#39;,\
                        #&#39;cumPeopleVaccinatedThirdInjectionByVaccinationDate&#39;:&#39;tot_vacc3&#39;,\
                        #&#39;covidOccupiedMVBeds&#39;:&#39;cur_icu&#39;,\
                        #&#39;cumPeopleVaccinatedFirstDoseByVaccinationDate&#39;:&#39;tot_dose1&#39;,\
                        #&#39;cumPeopleVaccinatedSecondDoseByVaccinationDate&#39;:&#39;tot_dose2&#39;,\
                        #&#39;hospitalCases&#39;:&#39;cur_hosp&#39;,\
                        }
                    url = &#39;https://api.coronavirus.data.gov.uk/v2/data?areaType=ltla&#39;
                    for w in rename_dict.keys():
                        if w not in [&#39;areaCode&#39;]:
                            url=url+&#39;&amp;metric=&#39;+w
                    url = url+&#39;&amp;format=csv&#39;
                    gbr_data = self.csv2pandas(url,separator=&#39;,&#39;,rename_columns=rename_dict)
                    constraints = {&#39;Lineage&#39;: &#39;B.1.617.2&#39;}
                    url = &#39;https://covid-surveillance-data.cog.sanger.ac.uk/download/lineages_by_ltla_and_week.tsv&#39;
                    gbrvar = self.csv2pandas(url,separator=&#39;\t&#39;,constraints=constraints,rename_columns = {&#39;WeekEndDate&#39;: &#39;date&#39;,&#39;LTLA&#39;:&#39;location&#39;})
                    varname =  &#39;B.1.617.2&#39;
                    gbr_data = pd.merge(gbr_data,gbrvar,how=&#34;outer&#34;,on=[&#39;location&#39;,&#39;date&#39;])
                    gbr_data = gbr_data.rename(columns={&#39;Count&#39;:&#39;cur_&#39;+varname})
                    columns_keeped = list(rename_dict.values())
                    columns_keeped.append(&#39;cur_&#39;+varname)
                    columns_keeped.remove(&#39;location&#39;)
                    self.return_structured_pandas(gbr_data,columns_keeped=columns_keeped)
                elif self.db == &#39;moh&#39;: # MYS
                    info(&#39;Malaysia moh covid19-public database selected ...&#39;)
                    rename_dict = {&#39;state&#39;: &#39;location&#39;}
                    moh1 = self.csv2pandas(&#34;https://raw.githubusercontent.com/MoH-Malaysia/covid19-public/main/epidemic/cases_state.csv&#34;,rename_columns=rename_dict,separator=&#39;,&#39;)
                    moh1[&#39;tot_cases&#39;]=moh1.groupby([&#39;location&#39;])[&#39;cases_new&#39;].cumsum()

                    moh2 = self.csv2pandas(&#34;https://raw.githubusercontent.com/MoH-Malaysia/covid19-public/main/epidemic/hospital.csv&#34;,rename_columns=rename_dict,separator=&#39;,&#39;)
                    moh3 = self.csv2pandas(&#34;https://raw.githubusercontent.com/MoH-Malaysia/covid19-public/main/epidemic/icu.csv&#34;,rename_columns=rename_dict,separator=&#39;,&#39;)
                    moh4 = self.csv2pandas(&#34;https://raw.githubusercontent.com/CITF-Malaysia/citf-public/main/vaccination/vax_state.csv&#34;,rename_columns=rename_dict,separator=&#39;,&#39;)

                    list_moh = [moh1,moh2,moh3,moh4]
                    result = reduce(lambda left, right: left.merge(right, how = &#39;outer&#39;, on=[&#39;location&#39;,&#39;date&#39;]), list_moh)
                    columns_keeped = [&#39;tot_cases&#39;,&#39;hosp_covid&#39;,&#39;daily_partial&#39;,&#39;daily_full&#39;,&#39;icu_covid&#39;,&#39;beds_icu_covid&#39;]
                    self.return_structured_pandas(result, columns_keeped = columns_keeped)
                elif self.db == &#39;minciencia&#39;: # CHL
                    info(&#39;Chile Ministerio de Ciencia, Tecnología, Conocimiento, e Innovación database selected ...&#39;)
                    cast = {&#39;Codigo comuna&#39;: &#39;string&#39;}
                    rename_dict = {&#39;Codigo comuna&#39;:&#39;location&#39;,&#39;Poblacion&#39;:&#39;population&#39;,&#39;Fecha&#39;:&#39;date&#39;,&#39;Casos confirmados&#39;:&#39;cases&#39;}
                    ciencia = self.csv2pandas(&#34;https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto1/Covid-19_std.csv&#34;,cast=cast,rename_columns=rename_dict,separator=&#39;,&#39;)
                    columns_keeped = [&#39;cases&#39;]
                    self.return_structured_pandas(ciencia, columns_keeped = columns_keeped)
                elif self.db == &#39;covid19india&#39;: # IND
                    info(&#39;COVID19India database selected ...&#39;)

                    columns_keeped = [&#39;Deceased&#39;, &#39;Confirmed&#39;, &#39;Recovered&#39;, &#39;Tested&#39;,]
                    rename_dict = {i:&#39;tot_&#39;+i for i in columns_keeped}
                    columns_keeped = list(rename_dict.values())
                    rename_dict.update({&#39;Date&#39;: &#39;date&#39;, &#39;State&#39;: &#39;location&#39;})
                    drop_field  = {&#39;State&#39;: [&#39;India&#39;, &#39;State Unassigned&#39;]}
                    indi = self.csv2pandas(&#34;https://api.covid19india.org/csv/latest/states.csv&#34;,drop_field=drop_field,rename_columns=rename_dict,separator=&#39;,&#39;)
                     # Removing &#39;Other&#39; data, not identified
                    indi[&#39;location&#39;] = indi[&#39;location&#39;].apply(lambda x: x.replace(&#39;Andaman and Nicobar Islands&#39;,&#39;Andaman and Nicobar&#39;))
                    locationvariant = self.geo.get_subregion_list()[&#39;variation_name_subregion&#39;].to_list()
                    locationgeo = self.geo.get_subregion_list()[&#39;name_subregion&#39;].to_list()
                    def fusion(pan, new, old):
                        tmp = (pan.loc[pan.location.isin([new, old])].groupby(&#39;date&#39;).sum())
                        tmp[&#39;location&#39;] = old
                        tmp = tmp.reset_index()
                        cols = tmp.columns.tolist()
                        cols = cols[0:1] + cols[-1:] + cols[1:-1]
                        tmp = tmp[cols]
                        pan = pan.loc[~pan.location.isin([new, old])]
                        pan = pan.append(tmp)
                        return pan

                    indi=fusion(indi, &#39;Telangana&#39;, &#39;Andhra Pradesh&#39;)
                    indi=fusion(indi,&#39;Ladakh&#39;, &#39;Jammu and Kashmir&#39;)
                    # change name according to json one
                    oldnew = {}
                    for i in indi.location.unique():
                        for k,l in zip(locationgeo,locationvariant):
                            if l.find(i) == 0:
                                oldnew[i] = k
                    indi[&#39;location&#39;] = indi[&#39;location&#39;].map(oldnew)
                    self.return_structured_pandas(indi,columns_keeped = columns_keeped)
                elif self.db == &#39;covidtracking&#39;:
                    info(&#39;USA, CovidTracking.com database selected... ...&#39;)
                    rename_dict = {&#39;state&#39;: &#39;location&#39;,
                            &#39;death&#39;: &#39;tot_death&#39;,
                            &#39;hospitalizedCumulative&#39;: &#39;tot_hosp&#39;,
                            &#39;hospitalizedCurrently&#39;: &#39;cur_hosp&#39;,
                            &#39;inIcuCumulative&#39;: &#39;tot_icu&#39;,
                            &#39;inIcuCurrently&#39;: &#39;cur_icu&#39;,
                            &#39;negative&#39;: &#39;tot_neg_test&#39;,
                            &#39;positive&#39;: &#39;tot_pos_test&#39;,
                            &#39;onVentilatorCumulative&#39;: &#39;tot_onVentilator&#39;,
                            &#39;onVentilatorCurrently&#39;: &#39;cur_onVentilator&#39;,
                            &#39;totalTestResults&#39;:&#39;tot_test&#39;,
                            }
                    ctusa = self.csv2pandas(&#34;https://covidtracking.com/data/download/all-states-history.csv&#34;,
                        rename_columns = rename_dict, separator = &#39;,&#39;)
                    columns_keeped = list(rename_dict.values())
                    columns_keeped.remove(&#39;location&#39;) # is already expected
                    self.return_structured_pandas(ctusa, columns_keeped = columns_keeped)
                elif self.db == &#39;spf&#39; or self.db == &#39;spfnational&#39;:
                    if self.db == &#39;spfnational&#39;:
                        rename_dict = {
                        &#39;patients_reanimation&#39;:&#39;cur_reanimation&#39;,
                        &#39;patients_hospitalises&#39;:&#39;cur_hospitalises&#39;
                        }
                        columns_keeped = [&#39;total_deces_hopital&#39;,&#39;cur_reanimation&#39;,&#39;cur_hospitalises&#39;,
                        &#39;total_cas_confirmes&#39;,&#39;total_patients_gueris&#39;,
                        &#39;total_deces_ehpad&#39;,&#39;total_cas_confirmes_ehpad&#39;,&#39;total_cas_possibles_ehpad&#39;]

                        spfnat = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/d3a98a30-893f-47f7-96c5-2f4bcaaa0d71&#34;,
                        rename_columns = rename_dict, separator = &#39;,&#39;)
                        colcast=[i for i in columns_keeped]

                        spfnat[colcast]=pd.to_numeric(spfnat[colcast].stack(),errors = &#39;coerce&#39;).unstack()
                        self.return_structured_pandas(spfnat, columns_keeped=columns_keeped) # with &#39;tot_dc&#39; first
                    else:
                        info(&#39;SPF aka Sante Publique France database selected (France departement granularity) ...&#39;)
                        info(&#39;... Nine different databases from SPF will be parsed ...&#39;)
                        # https://www.data.gouv.fr/fr/datasets/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/
                        # Parse and convert spf data structure to JHU one for historical raison
                        # hosp Number of people currently hospitalized
                        # rea  Number of people currently in resuscitation or critical care
                        # rad      Total amount of patient that returned home
                        # dc       Total amout of deaths at the hospital
                        # &#39;sexe&#39; == 0 male + female
                        cast = {&#39;dep&#39;: &#39;string&#39;}
                        rename = {&#39;jour&#39;: &#39;date&#39;, &#39;dep&#39;: &#39;location&#39;}
                        cast.update({&#39;HospConv&#39;:&#39;string&#39;,&#39;SSR_USLD&#39;:&#39;string&#39;,&#39;autres&#39;:&#39;string&#39;})
                        constraints = {&#39;sexe&#39;: 0}
                        spf1 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/63352e38-d353-4b54-bfd1-f1b3ee1cabd7&#34;,
                                      rename_columns = rename, constraints = constraints, cast = cast)
                        # https://www.data.gouv.fr/fr/datasets/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/
                        # All data are incidence. → integrated later in the code
                        # incid_hosp    string  Nombre quotidien de personnes nouvellement hospitalisées
                        # incid_rea     integer Nombre quotidien de nouvelles admissions en réanimation
                        # incid_dc      integer Nombre quotidien de personnes nouvellement décédées
                        # incid_rad     integer Nombre quotidien de nouveaux retours à domicile
                        spf2 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/6fadff46-9efd-4c53-942a-54aca783c30c&#34;,
                                      rename_columns = rename, cast = cast)
                        # https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-resultats-des-tests-virologiques-covid-19/
                        # T       Number of tests performed daily → integrated later
                        # P       Number of positive tests daily → integrated later
                        constraints = {&#39;cl_age90&#39;: 0}
                        spf3 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/406c6a23-e283-4300-9484-54e78c8ae675&#34;,
                                      rename_columns = rename, constraints = constraints, cast = cast)
                        # https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-personnes-vaccinees-contre-la-covid-19-1
                        # Les données issues du système d’information Vaccin Covid permettent de dénombrer en temps quasi réel
                        # (J-1), le nombre de personnes ayant reçu une injection de vaccin anti-covid en tenant compte du nombre
                        # de doses reçues, de l’âge, du sexe ainsi que du niveau géographique (national, régional et
                        # départemental).
                        constraints = {&#39;vaccin&#39;: 0} # 0 means all vaccines
                        # previously : https://www.data.gouv.fr/fr/datasets/r/4f39ec91-80d7-4602-befb-4b522804c0af
                        spf5 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/535f8686-d75d-43d9-94b3-da8cdf850634&#34;,
                            rename_columns = rename, constraints = constraints, separator = &#39;;&#39;, encoding = &#34;ISO-8859-1&#34;, cast = cast)
                        #print(spf5)
                        # https://www.data.gouv.fr/fr/datasets/indicateurs-de-suivi-de-lepidemie-de-covid-19/#_
                        # tension hospitaliere
                        #&#39;date&#39;, &#39;location&#39;, &#39;region&#39;, &#39;libelle_reg&#39;, &#39;libelle_dep&#39;, &#39;tx_incid&#39;,
                        # &#39;R&#39;, &#39;taux_occupation_sae&#39;, &#39;tx_pos&#39;, &#39;tx_incid_couleur&#39;, &#39;R_couleur&#39;,
                        # &#39;taux_occupation_sae_couleur&#39;, &#39;tx_pos_couleur&#39;, &#39;nb_orange&#39;,
                        # &#39;nb_rouge&#39;]
                        # Vert : taux d’occupation compris entre 0 et 40% ;
                        # Orange : taux d’occupation compris entre 40 et 60% ;
                        # Rouge : taux d&#39;occupation supérieur à 60%.
                        # R0
                        # vert : R0 entre 0 et 1 ;
                        # Orange : R0 entre 1 et 1,5 ;
                        # Rouge : R0 supérieur à 1,5.
                        cast = {&#39;departement&#39;: &#39;string&#39;}
                        rename = {&#39;extract_date&#39;: &#39;date&#39;, &#39;departement&#39;: &#39;location&#39;}
                        #columns_skipped=[&#39;region&#39;,&#39;libelle_reg&#39;,&#39;libelle_dep&#39;,&#39;tx_incid_couleur&#39;,&#39;R_couleur&#39;,\
                        #&#39;taux_occupation_sae_couleur&#39;,&#39;tx_pos_couleur&#39;,&#39;nb_orange&#39;,&#39;nb_rouge&#39;]
                        spf4 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/4acad602-d8b1-4516-bc71-7d5574d5f33e&#34;,
                                    rename_columns = rename, separator=&#39;,&#39;, encoding = &#34;ISO-8859-1&#34;, cast=cast)

                        #https://www.data.gouv.fr/fr/datasets/donnees-de-laboratoires-pour-le-depistage-indicateurs-sur-les-variants/
                        #Prc_tests_PCR_TA_crible = % de tests PCR criblés parmi les PCR positives
                        #Prc_susp_501Y_V1 = % de tests avec suspicion de variant 20I/501Y.V1 (UK)
                        #Prc_susp_501Y_V2_3 = % de tests avec suspicion de variant 20H/501Y.V2 (ZA) ou 20J/501Y.V3 (BR)
                        #Prc_susp_IND = % de tests avec une détection de variant mais non identifiable
                        #Prc_susp_ABS = % de tests avec une absence de détection de variant
                        #Royaume-Uni (UK): code Nexstrain= 20I/501Y.V1
                        #Afrique du Sud (ZA) : code Nexstrain= 20H/501Y.V2
                        #Brésil (BR) : code Nexstrain= 20J/501Y.V3

                        cast = {&#39;dep&#39;: &#39;string&#39;}
                        rename = {&#39;dep&#39;: &#39;location&#39;}
                        constraints = {&#39;cl_age90&#39;: 0}
                        spf6 =  self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/16f4fd03-797f-4616-bca9-78ff212d06e8&#34;,
                                     constraints = constraints,rename_columns = rename, separator=&#39;;&#39;, cast=cast)

                        constraints = {&#39;age_18ans&#39;: 0}
                        spf7 =  self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/c0f59f00-3ab2-4f31-8a05-d317b43e9055&#34;,
                                    constraints = constraints, rename_columns = rename, separator=&#39;;&#39;, cast=cast)
                        #Mutation d&#39;intérêt :
                        #A = E484K
                        #B = E484Q
                        #C = L452R
                        spf8 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/4d3e5a8b-9649-4c41-86ec-5420eb6b530c&#34;,
                        rename_columns = rename, separator=&#39;;&#39;,cast=cast)
                        #spf8keeped = list(spf8.columns)[2:]
                        rename = {&#39;date_de_passage&#39;:&#39;date&#39;,&#39;dep&#39;:&#39;location&#39;}
                        spf9 = self.csv2pandas(&#34;https://www.data.gouv.fr/en/datasets/r/eceb9fb4-3ebc-4da3-828d-f5939712600a&#34;,
                        rename_columns = rename, separator=&#39;;&#39;,cast=cast)

                        list_spf=[spf1, spf2, spf3, spf4, spf5, spf6, spf7,spf8,spf9]

                        #for i in list_spf:
                        #    i[&#39;date&#39;] = pd.to_datetime(i[&#39;date&#39;]).apply(lambda x: x if not pd.isnull(x) else &#39;&#39;)
                        #    print(i.loc[i.date==d1])
                        #dfs = [df.set_index([&#39;date&#39;, &#39;location&#39;]) for df in list_spf]
                        result = reduce(lambda left, right: left.merge(right, how = &#39;outer&#39;, on=[&#39;location&#39;,&#39;date&#39;]), list_spf)
                        result = result.loc[~result[&#39;location&#39;].isin([&#39;00&#39;])]
                        result = result.sort_values(by=[&#39;location&#39;,&#39;date&#39;])
                        result.loc[result[&#39;location&#39;].isin([&#39;975&#39;,&#39;977&#39;,&#39;978&#39;,&#39;986&#39;,&#39;987&#39;]),&#39;location&#39;]=&#39;980&#39;
                        result = result.drop_duplicates(subset=[&#39;location&#39;, &#39;date&#39;], keep=&#39;last&#39;)

                        for w in [&#39;incid_hosp&#39;, &#39;incid_rea&#39;, &#39;incid_rad&#39;, &#39;incid_dc&#39;, &#39;P&#39;, &#39;T&#39;, &#39;n_cum_dose1&#39;, &#39;n_cum_dose2&#39;,&#39;n_cum_dose3&#39;,&#39;n_cum_dose4&#39;,&#39;n_cum_rappel&#39;]:
                            result[w]=pd.to_numeric(result[w], errors = &#39;coerce&#39;)
                            if w.startswith(&#39;incid_&#39;):
                                ww = w[6:]
                                result[ww] = result.groupby(&#39;location&#39;)[ww].fillna(method = &#39;bfill&#39;)
                                result[&#39;incid_&#39;+ww] = result.groupby(&#39;location&#39;)[&#39;incid_&#39;+ww].fillna(method = &#39;bfill&#39;)
                                #result[&#39;offset_&#39;+w] = result.loc[result.date==min_date][ww]-result.loc[result.date==min_date][&#39;incid_&#39;+ww]
                                #result[&#39;offset_&#39;+w] = result.groupby(&#39;location&#39;)[&#39;offset_&#39;+w].fillna(method=&#39;ffill&#39;)
                            else:
                                pass
                                #result[&#39;offset_&#39;+w] = 0
                            if w not in [&#39;n_cum&#39;,&#39;incid_hosp&#39;, &#39;incid_rea&#39;, &#39;incid_rad&#39;, &#39;incid_dc&#39;]:
                                result[&#39;tot_&#39;+w]=result.groupby([&#39;location&#39;])[w].cumsum()#+result[&#39;offset_&#39;+w]

                        def dontneeeded():
                            for col in result.columns:
                                if col.startswith(&#39;Prc&#39;):
                                    result[col] /= 100.
                            for col in result.columns:
                                if col.startswith(&#39;ti&#39;):
                                    result[col] /= 7. #par
                            for col in result.columns:
                                if col.startswith(&#39;tp&#39;):
                                    result[col] /= 7. #par

                        rename_dict={
                            &#39;dc&#39;: &#39;tot_dc&#39;,
                            &#39;hosp&#39;: &#39;cur_hosp&#39;,
                            &#39;rad&#39;: &#39;tot_rad&#39;,
                            &#39;rea&#39;: &#39;cur_rea&#39;,
                            &#39;n_cum_dose1&#39;: &#39;tot_vacc1&#39;,
                            &#39;n_cum_dose2&#39;: &#39;tot_vacc2&#39;,
                            &#39;n_cum_dose3&#39;: &#39;tot_vacc3&#39;,
                            &#39;n_cum_dose4&#39;: &#39;tot_vacc4&#39;,
                            &#39;n_cum_rappel&#39;:&#39;tot_rappel_vacc&#39;,
                            &#39;tx_incid&#39;: &#39;cur_idx_tx_incid&#39;,
                            &#39;R&#39;: &#39;cur_idx_R&#39;,
                            &#39;taux_occupation_sae&#39;: &#39;cur_idx_taux_occupation_sae&#39;,
                            &#39;tx_pos&#39;: &#39;cur_taux_pos&#39;,
                            &#39;Prc_tests_PCR_TA_crible&#39;:&#39;cur_idx_Prc_tests_PCR_TA_crible&#39;,
                            &#39;Prc_susp_501Y_V1&#39;:&#39;cur_idx_Prc_susp_501Y_V1&#39;,
                            &#39;Prc_susp_501Y_V2_3&#39;:&#39;cur_idx_Prc_susp_501Y_V2_3&#39;,
                            &#39;Prc_susp_IND&#39;:&#39;cur_idx_Prc_susp_IND&#39;,
                            &#39;Prc_susp_ABS&#39;:&#39;cur_idx_Prc_susp_ABS&#39;,
                            &#39;ti&#39;:&#39;cur_idx_ti&#39;,
                            &#39;tp&#39;:&#39;cur_idx_tp&#39;,
                            &#39;tx_crib&#39; : &#39;cur_taux_crib&#39;,
                            &#39;tx_A1&#39;:&#39;cur_idx_tx_A1&#39;,
                            &#39;tx_B1&#39;:&#39;cur_idx_tx_B1&#39;,
                            &#39;tx_C1&#39;:&#39;cur_idx_tx_C1&#39;,
                            &#39;nbre_pass_corona&#39;:&#39;cur_nbre_pass_corona&#39;,
                            }
                        spf8keeped = [&#39;nb_A0&#39;,&#39;nb_A1&#39;, &#39;nb_B0&#39;, &#39;nb_B1&#39;, &#39;nb_C0&#39;, &#39;nb_C1&#39;]
                        rename_dict.update({i:&#39;cur_&#39;+i for i in spf8keeped})
                        result = result.rename(columns=rename_dict)
                        #coltocast=list(rename_dict.values())[:5]
                        #result[coltocast] = result[coltocast].astype(&#39;Int64&#39;)
                        rename_dict2={i:i.replace(&#39;incid_&#39;,&#39;tot_incid_&#39;) for i in [&#39;incid_hosp&#39;, &#39;incid_rea&#39;, &#39;incid_rad&#39;, &#39;incid_dc&#39;]}
                        result = result.rename(columns=rename_dict2)
                        columns_keeped  = list(rename_dict.values()) + list(rename_dict2.values()) + [&#39;tot_P&#39;, &#39;tot_T&#39;]
                        self.return_structured_pandas(result,columns_keeped=columns_keeped) # with &#39;tot_dc&#39; first
                elif self.db == &#39;opencovid19&#39; or  self.db == &#39;opencovid19national&#39;:
                    rename={&#39;maille_code&#39;:&#39;location&#39;}
                    cast={&#39;source_url&#39;:str,&#39;source_archive&#39;:str,&#39;source_type&#39;:str,&#39;nouvelles_hospitalisations&#39;:str,&#39;nouvelles_reanimations&#39;:str}
                    if self.db == &#39;opencovid19&#39;:
                        info(&#39;OPENCOVID19 (country granularity) selected ...&#39;)
                        drop_field  = {&#39;granularite&#39;:[&#39;pays&#39;,&#39;monde&#39;,&#39;region&#39;]}
                        dict_columns_keeped = {
                            &#39;deces&#39;:&#39;tot_deces&#39;,
                            &#39;cas_confirmes&#39;:&#39;tot_cas_confirmes&#39;,
                            &#39;reanimation&#39;:&#39;cur_reanimation&#39;,
                            &#39;hospitalises&#39;:&#39;cur_hospitalises&#39;,
                            &#39;gueris&#39;:&#39;tot_gueris&#39;
                            }
                    else:
                        info(&#39;OPENCOVID19 (national granularity) selected ...&#39;)
                        drop_field  = {&#39;granularite&#39;:[&#39;monde&#39;,&#39;region&#39;,&#39;departement&#39;]}
                        dict_columns_keeped = {
                        &#39;deces&#39;:&#39;tot_deces&#39;,
                        &#39;cas_confirmes&#39;:&#39;tot_cas_confirmes&#39;,
                        &#39;cas_ehpad&#39;:&#39;tot_cas_ehpad&#39;,
                        &#39;cas_confirmes_ehpad&#39;:&#39;tot_cas_confirmes_ehpad&#39;,
                        &#39;cas_possibles_ehpad&#39;:&#39;tot_cas_possibles_ehpad&#39;,
                        &#39;deces_ehpad&#39;:&#39;tot_deces_ehpad&#39;,
                        &#39;reanimation&#39;:&#39;cur_reanimation&#39;,
                        &#39;hospitalises&#39;:&#39;cur_hospitalises&#39;,
                        &#39;gueris&#39;:&#39;tot_gueris&#39;
                        }
                    opencovid19 = self.csv2pandas(&#39;https://raw.githubusercontent.com/opencovid19-fr/data/master/dist/chiffres-cles.csv&#39;,
                                drop_field=drop_field,rename_columns=rename,separator=&#39;,&#39;,cast=cast)

                    opencovid19[&#39;location&#39;] = opencovid19[&#39;location&#39;].apply(lambda x: x.replace(&#39;COM-&#39;,&#39;&#39;).replace(&#39;DEP-&#39;,&#39;&#39;).replace(&#39;FRA&#39;,&#39;France&#39;))
                    # integrating needed fields
                    if self.db == &#39;opencovid19national&#39;:
                        opencovid19 = opencovid19.loc[~opencovid19.granularite.isin([&#39;collectivite-outremer&#39;])]

                    column_to_integrate=[&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;]
                    opencovid19[column_to_integrate]=pd.to_numeric(opencovid19[column_to_integrate].stack(),errors = &#39;coerce&#39;).unstack()

                    for w in [&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;]:
                        opencovid19[&#39;tot_&#39;+w]=opencovid19.groupby([&#39;location&#39;])[w].cumsum()
                    #columns_skipped = [&#39;granularite&#39;,&#39;maille_nom&#39;,&#39;source_nom&#39;,&#39;source_url&#39;,&#39;source_archive&#39;,&#39;source_type&#39;]
                    self.return_structured_pandas(opencovid19.rename(columns=dict_columns_keeped),columns_keeped=list(dict_columns_keeped.values())+[&#39;tot_&#39;+c for c in column_to_integrate])
                elif self.db == &#39;owid&#39;:
                    variant = True
                    info(&#39;OWID aka \&#34;Our World in Data\&#34; database selected ...&#39;)
                    drop_field = {&#39;location&#39;:[&#39;International&#39;]}#, &#39;World&#39;]}
                    owid = self.csv2pandas(&#34;https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv&#34;,
                    separator=&#39;,&#39;,drop_field=drop_field)
                    # renaming some columns
                    col_to_rename1=[&#39;reproduction_rate&#39;,&#39;icu_patients&#39;,&#39;hosp_patients&#39;,&#39;weekly_hosp_admissions&#39;,&#39;positive_rate&#39;]
                    renamed_cols1=[&#39;cur_&#39;+c if c != &#39;positive_rate&#39; else &#39;cur_idx_&#39;+c for c in col_to_rename1]
                    col_to_rename2=[&#39;people_vaccinated&#39;,&#39;people_fully_vaccinated&#39;,&#39;people_fully_vaccinated_per_hundred&#39;,\
                    &#39;people_vaccinated_per_hundred&#39;,&#39;population&#39;,&#39;gdp_per_capita&#39;]
                    renamed_cols2=[&#39;total_&#39;+i for i in col_to_rename2]
                    col_to_rename = col_to_rename1+col_to_rename2
                    renamed_cols = renamed_cols1 +renamed_cols2
                    columns_keeped=[&#39;iso_code&#39;,&#39;total_deaths&#39;,&#39;total_cases&#39;,&#39;total_vaccinations&#39;,&#39;total_tests&#39;]
                    columns_keeped+=[&#39;total_cases_per_million&#39;,&#39;total_deaths_per_million&#39;,&#39;total_vaccinations_per_hundred&#39;,&#39;total_boosters&#39;]
                    #owid[&#39;total_tests_with_new_tests&#39;] = owid.groupby([&#39;location&#39;])[&#39;new_tests&#39;].cumsum()
                    uniq=list(owid.location.unique())
                    mask = (owid.loc[owid.location.isin(uniq)][&#39;total_tests&#39;].isnull() &amp;\
                                                owid.loc[owid.location.isin(uniq)][&#39;new_tests&#39;].isnull())
                    #sometimes is new_tests sometimes total_tests
                    owid_test         = owid[~mask]
                    owid_new_test     = owid_test[owid_test[&#39;total_tests&#39;].isnull()]
                    owid_total_test   = owid_test[~owid_test[&#39;total_tests&#39;].isnull()]
                    owid_new_test     = owid_new_test.drop(columns=&#39;total_tests&#39;)

                    owid_new_test.loc[:,&#39;total_tests&#39;] = owid_new_test.groupby([&#39;location&#39;])[&#39;new_tests&#39;].cumsum()
                    owid = pd.concat([owid[mask],owid_new_test,owid_total_test])
                    self.return_structured_pandas(owid.rename(columns=dict(zip(col_to_rename,renamed_cols))),columns_keeped=columns_keeped+renamed_cols)
                elif self.db == &#39;risklayer&#39;:
                    info(&#39;EUR, Who Europe from RiskLayer ...&#39;)
                    rename_dict = {&#39;UID&#39;: &#39;location&#39;,
                        &#39;CumulativePositive&#39;: &#39;tot_positive&#39;,
                        &#39;IncidenceCumulative&#39;: &#39;tot_incidence&#39;,
                        &#39;DateRpt&#39;:&#39;date&#39;}
                    deur = self.csv2pandas(&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vQ-JLawOH35vPyOk39w0tjn64YQLlahiD2AaNfjd82pgQ37Jr1K8KMHOqJbxoi4k2FZVYBGbZ-nsxhi/pub?output=csv&#34;,
                        rename_columns = rename_dict, separator = &#39;,&#39;)
                    columns_keeped = list(rename_dict.values())
                    columns_keeped.remove(&#39;location&#39;) # is already expected
                    columns_keeped.remove(&#39;date&#39;) # is already expected
                    self.return_structured_pandas(deur, columns_keeped = columns_keeped)
                elif self.db == &#39;europa&#39;:
                    info(&#39;EUR, Rationale for the JRC COVID-19 website - data monitoring and national measures ...&#39;)
                    rename_dict = {&#39;Region&#39;: &#39;location&#39;,
                        &#39;CumulativeDeceased&#39;:&#39;tot_deaths&#39;,
                        &#39;Hospitalized&#39;:&#39;cur_hosp&#39;,
                        &#39;IntensiveCare&#39;:&#39;cur_icu&#39;,
                        &#39;Date&#39;:&#39;date&#39;}
                    euro = self.csv2pandas(&#34;https://raw.githubusercontent.com/ec-jrc/COVID-19/master/data-by-region/jrc-covid-19-all-days-by-regions.csv&#34;,
                    rename_columns = rename_dict, separator = &#39;,&#39;)
                    euro=euro.loc[euro.EUcountry==True]
                    todrop=[&#39;Ciudad Autónoma de Melilla&#39;,&#39;Gorenjske&#39;,&#39;Goriške&#39;,&#39;Greenland&#39;,&#39;Itä-Savo&#39;,&#39;Jugovzhodne&#39;,&#39;Koroške&#39;,&#39;Länsi-Pohja&#39;,\
                            &#39;Mainland&#39;,&#39;NOT SPECIFIED&#39;,&#39;Obalno-kraške&#39;,&#39;Osrednjeslovenske&#39;,&#39;Podravske&#39;,&#39;Pomurske&#39;,&#39;Posavske&#39;,&#39;Primorsko-notranjske&#39;,\
                            &#39;Repatriierte&#39;,&#39;Savinjske&#39;,&#39;West North&#39;,&#39;Zasavske&#39;]
                    euro=euro.loc[~euro[&#39;location&#39;].isin(todrop)]
                    euro=euro.dropna(subset=[&#39;location&#39;])

                    euro[&#39;tot_positive&#39;]=euro.groupby(&#39;location&#39;)[&#39;CurrentlyPositive&#39;].cumsum()
                    columns_keeped = list(rename_dict.values())+[&#39;tot_positive&#39;]
                    columns_keeped.remove(&#39;location&#39;) # is already expected
                    columns_keeped.remove(&#39;date&#39;) # is already expected
                    self.return_structured_pandas(euro, columns_keeped = columns_keeped)
                elif self.db == &#39;insee&#39;:
                    since_year=2018 # Define the first year for stats
                    info(&#39;FRA, INSEE global deaths statistics...&#39;)
                    url = &#34;https://www.data.gouv.fr/fr/datasets/fichier-des-personnes-decedees/&#34;
                    with open(get_local_from_url(url,86400*7)) as fp: # update each week
                        soup = BeautifulSoup(fp,features=&#34;lxml&#34;)
                    ld_json=soup.find(&#39;script&#39;, {&#39;type&#39;:&#39;application/ld+json&#39;}).contents
                    data=json.loads(ld_json[0])
                    deces_url={}
                    for d in data[&#39;distribution&#39;]:
                        deces_url.update({d[&#39;name&#39;]:d[&#39;url&#39;]})
                    dc={}

                    current_year=datetime.date.today().year
                    current_month=datetime.date.today().month

                    # manage year between since_year-1 and current_year(excluded)
                    for y in range(since_year-1,current_year):
                        i=str(y) #  in string
                        filename=&#39;deces-&#39;+i+&#39;.txt&#39;
                        if filename not in list(deces_url.keys()):
                            continue
                        with open(get_local_from_url(deces_url[filename],86400*30)) as f:
                            dc.update({i:f.readlines()})

                    # manage months for the current_year
                    for m in range(current_month):
                        i=str(m+1).zfill(2) #  in string with leading 0
                        filename=&#39;deces-&#39;+str(current_year)+&#39;-m&#39;+i+&#39;.txt&#39;
                        if filename not in list(deces_url.keys()):
                            continue
                        with open(get_local_from_url(deces_url[filename],86400)) as f:
                            dc.update({i:f.readlines()})

                    def string_to_date(s):
                        date=None
                        y=int(s[0:4])
                        m=int(s[4:6])
                        d=int(s[6:8])
                        if m==0:
                            m=1
                        if d==0:
                            d=1
                        if y==0:
                            raise ValueError
                        try:
                            date=datetime.date(y,m,d)
                        except:
                            if m==2 and d==29:
                                d=28
                                date=datetime.date(y,m,d)
                                raise ValueError
                        return date

                    pdict={}
                    insee_pd=pd.DataFrame()
                    for i in list(dc.keys()):
                        data=[]

                        for l in dc[i]:
                            [last_name,first_name]=(l[0:80].split(&#34;/&#34;)[0]).split(&#34;*&#34;)
                            sex=int(l[80])
                            birthlocationcode=l[89:94]
                            birthlocationname=l[94:124].rstrip()
                            try:
                                birthdate=string_to_date(l[81:89])
                                deathdate=string_to_date(l[154:].strip()[0:8]) # sometimes, heading space
                                lbis=list(l[154:].strip()[0:8])
                                lbis[0:4]=list(&#39;2003&#39;)
                                lbis=&#39;&#39;.join(lbis)
                                deathdatebis=string_to_date(lbis)
                            except ValueError:
                                if lbis!=&#39;20030229&#39;:
                                    verb(&#39;Problem in a date parsing insee data for : &#39;,l,lbis)
                            deathlocationcode=l[162:167]
                            deathlocationshortcode=l[162:164]
                            deathid=l[167:176]
                            data.append([first_name,last_name,sex,birthdate,birthlocationcode,birthlocationname,deathdate,deathlocationcode,deathlocationshortcode,deathid,deathdatebis,1])
                        p=pd.DataFrame(data)
                        p.columns=[&#39;first_name&#39;,&#39;last_name&#39;,&#39;sex&#39;,&#39;birth_date&#39;,&#39;birth_location_code&#39;,&#39;birth_location_name&#39;,&#39;death_date&#39;,&#39;death_location_code&#39;,&#39;location&#39;,&#39;death_id&#39;,&#39;death_date_bis&#39;,&#39;i&#39;]
                        #p[&#34;age&#34;]=[k.days/365 for k in p[&#34;death_date&#34;]-p[&#34;birth_date&#34;]]
                        #p[&#34;age_class&#34;]=[math.floor(k/20) for k in p[&#34;age&#34;]]

                        #p=p[[&#39;location&#39;,&#39;death_date&#39;]].reset_index(drop=True)
                        #p[&#39;death_date&#39;]=pd.to_datetime(p[&#39;death_date&#39;]).dt.date
                        #p[&#39;location&#39;]=p[&#39;location&#39;].astype(str)
                        insee_pd=insee_pd.append(p)
                        #pdict.update({i:p})
                    insee_pd = insee_pd[[&#39;location&#39;,&#39;death_date&#39;]].reset_index(drop=True)
                    insee_pd = insee_pd.rename(columns={&#39;death_date&#39;:&#39;date&#39;})
                    insee_pd[&#39;date&#39;]=pd.to_datetime(insee_pd[&#39;date&#39;]).dt.date
                    insee_pd[&#39;location&#39;]=insee_pd[&#39;location&#39;].astype(str)
                    insee_pd = insee_pd.groupby([&#39;date&#39;,&#39;location&#39;]).size().reset_index(name=&#39;daily_number_of_deaths&#39;)

                    since_date=str(since_year)+&#39;-01-01&#39;
                    insee_pd = insee_pd[insee_pd.date&gt;=datetime.date.fromisoformat(since_date)].reset_index(drop=True)
                    insee_pd[&#39;tot_deaths_since_&#39;+since_date]=insee_pd.groupby(&#39;location&#39;)[&#39;daily_number_of_deaths&#39;].cumsum()
                    self.return_structured_pandas(insee_pd,columns_keeped=[&#39;tot_deaths_since_&#39;+since_date])
            except:
                raise CoaDbError(&#34;An error occured while parsing data of &#34;+self.get_db()+&#34;. This may be due to a data format modification. &#34;
                    &#34;You may contact support@pycoa.fr. Thanks.&#34;)
            # some info
            info(&#39;Few information concernant the selected database : &#39;, self.get_db())
            info(&#39;Available key-words, which ∈&#39;,self.get_available_keys_words())
            info(&#39;Example of location : &#39;,  &#39;, &#39;.join(random.choices(self.get_locations(), k=min(5,len(self.get_locations() ))   )), &#39; ...&#39;)
            info(&#39;Last date data &#39;, self.get_dates().max())

   @staticmethod
   def factory(db_name):
       &#39;&#39;&#39;
        Return an instance to DataBase and to CocoDisplay methods
        This is recommended to avoid mismatch in labeled figures
       &#39;&#39;&#39;
       datab = DataBase(db_name)
       return  datab, datab.get_display()

   def set_display(self,db,geo):
       &#39;&#39;&#39; Set the CocoDisplay &#39;&#39;&#39;
       self.codisp = codisplay.CocoDisplay(db, geo)

   def get_display(self):
       &#39;&#39;&#39; Return the instance of CocoDisplay initialized by factory&#39;&#39;&#39;
       return self.codisp

   def get_db(self):
        &#39;&#39;&#39;
        Return the current covid19 database selected. See get_available_database() for full list
        &#39;&#39;&#39;
        return self.db

   def get_available_database(self):
        &#39;&#39;&#39;
        Return all the available Covid19 database
        &#39;&#39;&#39;
        return self.database_name

   def get_available_options(self):
        &#39;&#39;&#39;
        Return available options for the get_stats method
        &#39;&#39;&#39;
        o=self.available_options
        return o

   def get_available_keys_words(self):
        &#39;&#39;&#39;
        Return all the available keyswords for the database selected
        Key-words are for:
        - jhu : [&#39;deaths&#39;,&#39;confirmed&#39;,&#39;recovered&#39;]
                            * the data are cumulative i.e for a date it represents the total cases
            For more information please have a look to https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data
        - &#39;owid&#39; : [&#39;total_deaths&#39;,&#39;total_cases&#39;,&#39;reproduction_rate&#39;,&#39;icu_patients&#39;,&#39;hosp_patients&#39;,&#39;total_tests&#39;,
                    &#39;positive_rate&#39;,&#39;total_vaccinations&#39;]
        For more information please have a look to https://github.com/owid/covid-19-data/tree/master/public/data/
        - &#39;spf&#39; : [&#39;hosp&#39;, &#39;rea&#39;, &#39;rad&#39;, &#39;dc&#39;, &#39;incid_hosp&#39;, &#39;incid_rea&#39;, &#39;incid_dc&#39;,
                    &#39;incid_rad&#39;, &#39;P&#39;, &#39;T&#39;, &#39;tx_incid&#39;, &#39;R&#39;, &#39;taux_occupation_sae&#39;, &#39;tx_pos&#39;]
            No translation have been done for french keywords data
        For more information please have a look to  https://www.data.gouv.fr/fr/organizations/sante-publique-france/
        - &#39;opencovid19&#39; :[&#39;cas_confirmes&#39;, &#39;deces&#39;,
        &#39;reanimation&#39;, &#39;hospitalises&#39;,&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;, &#39;gueris&#39;, &#39;depistes&#39;]
        - &#39;opencovid19national&#39; :[&#39;cas_confirmes&#39;, &#39;cas_ehpad&#39;, &#39;cas_confirmes_ehpad&#39;, &#39;cas_possibles_ehpad&#39;, &#39;deces&#39;, &#39;deces_ehpad&#39;,
        &#39;reanimation&#39;, &#39;hospitalises&#39;,&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;, &#39;gueris&#39;, &#39;depistes&#39;]

        No translation have been done for french keywords data
        For more information please have a look to https://github.com/opencovid19-fr
        &#39;&#39;&#39;
        return self.available_keys_words

   def get_keyword_definition(self,keys):
       &#39;&#39;&#39;
            Return definition on the selected keword
       &#39;&#39;&#39;
       value = self.databaseinfo.generic_info(self.get_db(),keys)[0]
       return value

   def get_keyword_url(self,keys):
       &#39;&#39;&#39;
        Return url where the keyword have been parsed
       &#39;&#39;&#39;
       value = self.databaseinfo.generic_info(self.get_db(),keys)[1]
       master  = self.databaseinfo.generic_info(self.get_db(),keys)[2]
       return value, master


   def return_jhu_pandas(self):
        &#39;&#39;&#39; For center for Systems Science and Engineering (CSSE) at Johns Hopkins University
            COVID-19 Data Repository by the see homepage: https://github.com/CSSEGISandData/COVID-19
            return a structure : pandas location - date - keywords
            for jhu location are countries (location uses geo standard)
            for jhu-usa location are Province_State (location uses geo standard)
            &#39;&#39;&#39;
        base_url = &#34;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/&#34;+\
                                &#34;csse_covid_19_data/csse_covid_19_time_series/&#34;
        base_name = &#34;time_series_covid19_&#34;
        # previous are default for actual jhu db

        pandas_jhu = {}

        if self.db == &#39;jhu&#39;: # worldwide
            extension =  &#34;_global.csv&#34;
            jhu_files_ext = [&#39;deaths&#39;, &#39;confirmed&#39;]
        elif self.db == &#39;jhu-usa&#39;: # &#39;USA&#39;
            extension = &#34;_US.csv&#34;
            jhu_files_ext = [&#39;deaths&#39;,&#39;confirmed&#39;]
        elif self.db == &#39;rki&#39;: # &#39;DEU&#39;
            base_url = &#39;https://github.com/jgehrcke/covid-19-germany-gae/raw/master/&#39;
            jhu_files_ext = [&#39;deaths&#39;,&#39;cases&#39;]
            extension = &#39;-rki-by-ags.csv&#39;
            base_name = &#39;&#39;
        elif self.db == &#39;imed&#39;: # &#39;GRC&#39;
            base_url = &#39;https://raw.githubusercontent.com/iMEdD-Lab/open-data/master/COVID-19/greece_&#39;
            jhu_files_ext = [&#39;deaths&#39;,&#39;cases&#39;]
            extension = &#39;_v2.csv&#39;
            base_name = &#39;&#39;
        else:
            raise CoaDbError(&#39;Unknown JHU like db &#39;+str(self.db))

        self.available_keys_words = []
        if self.db == &#39;rki&#39;:
                self.available_keys_words = [&#39;tot_deaths&#39;,&#39;tot_cases&#39;]
        pandas_list = []
        for ext in jhu_files_ext:
            fileName = base_name + ext + extension
            url = base_url + fileName
            self.database_url.append(url)
            pandas_jhu_db = pandas.read_csv(get_local_from_url(url,7200), sep = &#39;,&#39;) # cached for 2 hours
            if self.db == &#39;jhu&#39;:
                pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;Country/Region&#39;:&#39;location&#39;})
                pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;Province/State&#39;,&#39;Lat&#39;,&#39;Long&#39;])
                pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
                pandas_jhu_db = pandas_jhu_db.loc[~pandas_jhu_db.location.isin([&#39;Diamond Princess&#39;])]
            elif self.db == &#39;jhu-usa&#39;:
                pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;Province_State&#39;:&#39;location&#39;})
                pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;UID&#39;,&#39;iso2&#39;,&#39;iso3&#39;,&#39;code3&#39;,&#39;FIPS&#39;,
                                    &#39;Admin2&#39;,&#39;Country_Region&#39;,&#39;Lat&#39;,&#39;Long_&#39;,&#39;Combined_Key&#39;])
                if &#39;Population&#39; in pandas_jhu_db.columns:
                    pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;,&#39;Population&#39;],var_name=&#34;date&#34;,value_name=ext)
                else:
                    pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
                removethose=[&#39;American Samoa&#39;,&#39;Diamond Princess&#39;,&#39;Grand Princess&#39;,&#39;Guam&#39;,
                &#39;Northern Mariana Islands&#39;,&#39;Puerto Rico&#39;,&#39;Virgin Islands&#39;]
                pandas_jhu_db = pandas_jhu_db.loc[~pandas_jhu_db.location.isin(removethose)]
            elif self.db == &#39;rki&#39;:
                pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;sum_&#39;+ext])
                pandas_jhu_db = pandas_jhu_db.set_index(&#39;time_iso8601&#39;).T.reset_index().rename(columns={&#39;index&#39;:&#39;location&#39;})
                pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
                pandas_jhu_db[&#39;location&#39;] = pandas_jhu_db.location.astype(str)
                pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;deaths&#39;:&#39;tot_deaths&#39;,&#39;cases&#39;:&#39;tot_cases&#39;})
            elif self.db == &#39;imed&#39;:
                pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;county_normalized&#39;:&#39;location&#39;})
                pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;Γεωγραφικό Διαμέρισμα&#39;,&#39;Περιφέρεια&#39;,&#39;county&#39;,&#39;pop_11&#39;])
                ext=&#39;tot_&#39;+ext
                pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
                self.available_keys_words += [ext]
            else:
                raise CoaTypeError(&#39;jhu nor jhu-usa database selected ... &#39;)

            pandas_jhu_db=pandas_jhu_db.groupby([&#39;location&#39;,&#39;date&#39;]).sum().reset_index()
            pandas_list.append(pandas_jhu_db)

        if &#39;jhu&#39; in self.db:
            pandas_list = [pan.rename(columns={i:&#39;tot_&#39;+i for i in jhu_files_ext}) for pan in pandas_list]
            self.available_keys_words = [&#39;tot_&#39;+i for i in jhu_files_ext]
        uniqloc = list(pandas_list[0][&#39;location&#39;].unique())
        oldloc = uniqloc
        codedico={}
        toremove = None
        newloc = None
        location_is_code = False
        if self.db_world:
            d_loc_s = collections.OrderedDict(zip(uniqloc,self.geo.to_standard(uniqloc,output=&#39;list&#39;,db=self.get_db(),interpret_region=True)))
            self.slocation = list(d_loc_s.values())
            g=coge.GeoManager(&#39;iso3&#39;)
            codename = collections.OrderedDict(zip(self.slocation,g.to_standard(self.slocation,output=&#39;list&#39;,db=self.get_db(),interpret_region=True)))
        else:
            if self.database_type[self.db][1] == &#39;subregion&#39;:
                pdcodename = self.geo.get_subregion_list()
                self.slocation = uniqloc
                codename = collections.OrderedDict(zip(self.slocation,list(pdcodename.loc[pdcodename.code_subregion.isin(self.slocation)][&#39;name_subregion&#39;])))
                if self.db == &#39;jhu-usa&#39;:
                    d_loc_s = collections.OrderedDict(zip(uniqloc,list(pdcodename.loc[pdcodename.name_subregion.isin(uniqloc)][&#39;code_subregion&#39;])))
                    self.slocation = list(d_loc_s.keys())
                    codename = d_loc_s
                if self.db == &#39;rki&#39;:
                    d_loc_s = collections.OrderedDict(zip(uniqloc,list(pdcodename.loc[pdcodename.code_subregion.isin(uniqloc)][&#39;name_subregion&#39;])))
                    self.slocation = list(d_loc_s.values())
                    codename = d_loc_s
                    location_is_code = True
                    def notuse():
                        count_values=collections.Counter(d_loc_s.values())
                        duplicates_location = list({k:v for k,v in count_values.items() if v&gt;1}.keys())
                        def findkeywithvalue(dico,what):
                            a=[]
                            for k,v in dico.items():
                                if v == what:
                                    a.append(k)
                            return a
                        codedupli={i:findkeywithvalue(d_loc_s,i) for i in duplicates_location}
            elif self.database_type[self.db][1] == &#39;region&#39;:
                codename = self.geo.get_data().set_index(&#39;name_region&#39;)[&#39;code_region&#39;].to_dict()
                self.slocation = list(codename.keys())


        result = reduce(lambda x, y: pd.merge(x, y, on = [&#39;location&#39;,&#39;date&#39;]), pandas_list)

        if location_is_code:
            result[&#39;codelocation&#39;] = result[&#39;location&#39;]
            result[&#39;location&#39;] = result[&#39;location&#39;].map(codename)
        else:
            if self.db == &#39;jhu&#39;:
                result[&#39;location&#39;] = result[&#39;location&#39;].map(d_loc_s)
            result[&#39;codelocation&#39;] = result[&#39;location&#39;].map(codename)
        result = result.loc[result.location.isin(self.slocation)]

        tmp = pd.DataFrame()
        if &#39;Kosovo&#39; in uniqloc:
            #Kosovo is Serbia ! with geo.to_standard
            tmp=(result.loc[result.location.isin([&#39;Serbia&#39;])]).groupby(&#39;date&#39;).sum().reset_index()
            tmp[&#39;location&#39;] = &#39;Serbia&#39;
            tmp[&#39;codelocation&#39;] = &#39;SRB&#39;
            kw = [i for i in self.available_keys_words]
            colpos=[&#39;location&#39;, &#39;date&#39;] + kw + [&#39;codelocation&#39;]
            tmp = tmp[colpos]
            result = result.loc[~result.location.isin([&#39;Serbia&#39;])]
            result = result.append(tmp)

        result[&#39;date&#39;] = pd.to_datetime(result[&#39;date&#39;],errors=&#39;coerce&#39;).dt.date
        result = result.sort_values(by=[&#39;location&#39;,&#39;date&#39;])
        result = result.reset_index(drop=True)
        self.mainpandas = fill_missing_dates(result)
        self.dates  = self.mainpandas[&#39;date&#39;]

   def csv2pandas(self,url,**kwargs):
        &#39;&#39;&#39;
        Parse and convert the database cvs file to a pandas structure
        &#39;&#39;&#39;
        self.database_url.append(url)
        kwargs_test(kwargs,[&#39;cast&#39;,&#39;separator&#39;,&#39;encoding&#39;,&#39;constraints&#39;,&#39;rename_columns&#39;,&#39;drop_field&#39;,&#39;quotechar&#39;],
            &#39;Bad args used in the csv2pandas() function.&#39;)

        cast = kwargs.get(&#39;cast&#39;, None)
        dico_cast = {}
        if cast:
            for key,val in cast.items():
                dico_cast[key] = val
        separator = kwargs.get(&#39;separator&#39;, &#39;;&#39;)
        if separator:
            separator = separator
        encoding = kwargs.get(&#39;encoding&#39;, None)
        if encoding:
            encoding = encoding
        quoting=0
        if self.db == &#39;obepine&#39;:
              quoting=3
        pandas_db = pandas.read_csv(get_local_from_url(url,7200),sep=separator,dtype=dico_cast, encoding = encoding,
            keep_default_na=False,na_values=&#39;&#39;,header=0,quoting=quoting) # cached for 2 hours

        #pandas_db = pandas.read_csv(self.database_url,sep=separator,dtype=dico_cast, encoding = encoding )
        constraints = kwargs.get(&#39;constraints&#39;, None)
        rename_columns = kwargs.get(&#39;rename_columns&#39;, None)
        drop_field = kwargs.get(&#39;drop_field&#39;, None)
        if self.db == &#39;obepine&#39;:
            pandas_db = pandas_db.rename(columns=rename_columns)
            pandas_db = pandas_db.applymap(lambda x: x.replace(&#39;&#34;&#39;, &#39;&#39;))
        if constraints:
            for key,val in constraints.items():
                pandas_db = pandas_db.loc[pandas_db[key] == val]
                pandas_db = pandas_db.drop(columns=key)
        if drop_field:
            for key,val in drop_field.items():
                for i in val:
                    pandas_db =  pandas_db[pandas_db[key] != i ]
        if rename_columns:
            for key,val in rename_columns.items():
                pandas_db = pandas_db.rename(columns={key:val})
        if &#39;semaine&#39; in  pandas_db.columns:
            pandas_db[&#39;semaine&#39;] = [ week_to_date(i) for i in pandas_db[&#39;semaine&#39;]]
            #pandas_db = pandas_db.drop_duplicates(subset=[&#39;semaine&#39;])
            pandas_db = pandas_db.rename(columns={&#39;semaine&#39;:&#39;date&#39;})
        pandas_db[&#39;date&#39;] = pandas.to_datetime(pandas_db[&#39;date&#39;],errors=&#39;coerce&#39;).dt.date
        #self.dates  = pandas_db[&#39;date&#39;]
        if self.database_type[self.db][1] == &#39;nation&#39; and  self.database_type[self.db][0] in [&#39;FRA&#39;,&#39;CYP&#39;]:
            pandas_db[&#39;location&#39;] = self.database_type[self.db][2]
        pandas_db = pandas_db.sort_values([&#39;location&#39;,&#39;date&#39;])
        return pandas_db

   def return_structured_pandas(self,mypandas,**kwargs):
        &#39;&#39;&#39;
        Return the mainpandas core of the PyCoA structure
        &#39;&#39;&#39;
        kwargs_test(kwargs,[&#39;columns_skipped&#39;,&#39;columns_keeped&#39;],
            &#39;Bad args used in the return_structured_pandas function.&#39;)
        columns_skipped = kwargs.get(&#39;columns_skipped&#39;, None)
        absolutlyneeded = [&#39;date&#39;,&#39;location&#39;]
        defaultkeept = list(set(mypandas.columns.to_list()) - set(absolutlyneeded))
        columns_keeped  = kwargs.get(&#39;columns_keeped&#39;, defaultkeept)
        if columns_skipped:
            columns_keeped = [x for x in mypandas.columns.values.tolist() if x not in columns_skipped + absolutlyneeded]
        mypandas = mypandas[absolutlyneeded + columns_keeped]

        self.available_keys_words = columns_keeped #+ absolutlyneeded
        not_un_nation_dict={&#39;Kosovo&#39;:&#39;Serbia&#39;}
        for subpart_country, main_country in not_un_nation_dict.items() :
            tmp=(mypandas.loc[mypandas.location.isin([subpart_country,main_country])].groupby(&#39;date&#39;).sum())
            tmp[&#39;location&#39;]=main_country
            mypandas = mypandas.loc[~mypandas.location.isin([subpart_country,main_country])]
            tmp = tmp.reset_index()
            cols = tmp.columns.tolist()
            cols = cols[0:1] + cols[-1:] + cols[1:-1]
            tmp = tmp[cols]
            mypandas = mypandas.append(tmp)
        if &#39;iso_code&#39; in mypandas.columns:
            mypandas[&#39;iso_code&#39;] = mypandas[&#39;iso_code&#39;].dropna().astype(str)
            mypandasori=mypandas.copy()
            strangeiso3tokick = [i for i in mypandasori[&#39;iso_code&#39;].dropna().unique() if not len(i)==3 ]
            mypandasori = mypandas.loc[~mypandas.iso_code.isin(strangeiso3tokick)]
            self.available_keys_words.remove(&#39;iso_code&#39;)
            mypandasori = mypandasori.drop(columns=[&#39;location&#39;])
            mypandasori = mypandasori.rename(columns={&#39;iso_code&#39;:&#39;location&#39;})
            if self.db == &#39;owid&#39;:
                onlyowid = mypandas.loc[mypandas.iso_code.isin(strangeiso3tokick)]
                onlyowid = onlyowid.copy()
                onlyowid.loc[:,&#39;location&#39;] = onlyowid[&#39;location&#39;].apply(lambda x : &#39;owid_&#39;+x)
            mypandas = mypandasori

        if self.db == &#39;dpc&#39;:
            gd = self.geo.get_data()[[&#39;name_region&#39;,&#39;code_region&#39;]]
            A=[&#39;P.A. Bolzano&#39;,&#39;P.A. Trento&#39;]
            tmp=mypandas.loc[mypandas.location.isin(A)].groupby(&#39;date&#39;).sum()
            tmp[&#39;location&#39;]=&#39;Trentino-Alto Adige&#39;
            mypandas = mypandas.loc[~mypandas.location.isin(A)]
            tmp = tmp.reset_index()
            mypandas = mypandas.append(tmp)
            uniqloc = list(mypandas[&#39;location&#39;].unique())
            sub2reg = dict(gd.values)
            #collections.OrderedDict(zip(uniqloc,list(gd.loc[gd.name_region.isin(uniqloc)][&#39;code_region&#39;])))
            mypandas[&#39;codelocation&#39;] = mypandas[&#39;location&#39;].map(sub2reg)
        if self.db == &#39;dgs&#39;:
            gd = self.geo.get_data()[[&#39;name_region&#39;,&#39;name_region&#39;]]
            mypandas = mypandas.reset_index(drop=True)
            mypandas[&#39;location&#39;] = mypandas[&#39;location&#39;].apply(lambda x: x.title().replace(&#39;Do&#39;, &#39;do&#39;).replace(&#39;Da&#39;,&#39;da&#39;).replace(&#39;De&#39;,&#39;de&#39;))
            uniqloc = list(mypandas[&#39;location&#39;].unique())
            sub2reg = dict(gd.values)
            #sub2reg = collections.OrderedDict(zip(uniqloc,list(gd.loc[gd.name_subregion.isin(uniqloc)][&#39;name_region&#39;])))
            mypandas[&#39;location&#39;] = mypandas[&#39;location&#39;].map(sub2reg)
            mypandas = mypandas.loc[~mypandas.location.isnull()]

         # filling subregions.
            gd = self.geo.get_data()[[&#39;code_region&#39;,&#39;name_region&#39;]]
            uniqloc = list(mypandas[&#39;location&#39;].unique())
            name2code = collections.OrderedDict(zip(uniqloc,list(gd.loc[gd.name_region.isin(uniqloc)][&#39;code_region&#39;])))
            mypandas = mypandas.loc[~mypandas.location.isnull()]

        codename = None
        location_is_code = False
        uniqloc = list(mypandas[&#39;location&#39;].unique()) # if possible location from csv are codelocation

        if self.db_world:
            uniqloc = [s for s in uniqloc if &#39;OWID_&#39; not in s]
            db=self.get_db()
            if self.db == &#39;govcy&#39;:
                db=None
            codename = collections.OrderedDict(zip(uniqloc,self.geo.to_standard(uniqloc,output=&#39;list&#39;,db=db,interpret_region=True)))
            self.slocation = list(codename.values())
            location_is_code = True
        else:
            if self.database_type[self.db][1] == &#39;region&#39; :
                if self.db == &#39;covid19india&#39;:
                    mypandas = mypandas.loc[~mypandas.location.isnull()]
                    uniqloc = list(mypandas[&#39;location&#39;].unique())
                temp = self.geo.get_region_list()[[&#39;name_region&#39;,&#39;code_region&#39;]]
                #codename = collections.OrderedDict(zip(uniqloc,list(temp.loc[temp.name_region.isin(uniqloc)][&#39;code_region&#39;])))
                codename=dict(temp.values)
                self.slocation = uniqloc
                if self.db == &#39;obepine&#39;:
                    codename = {v:k for k,v in codename.items()}
                    location_is_code = True

            elif self.database_type[self.db][1] == &#39;subregion&#39;:
                temp = self.geo_all[[&#39;code_subregion&#39;,&#39;name_subregion&#39;]]
                codename=dict(temp.loc[temp.code_subregion.isin(uniqloc)].values)
                if self.db in [&#39;phe&#39;,&#39;covidtracking&#39;,&#39;spf&#39;,&#39;escovid19data&#39;,&#39;opencovid19&#39;,&#39;minciencia&#39;,&#39;moh&#39;,&#39;risklayer&#39;,&#39;insee&#39;]:
                    #codename={i:list(temp.loc[temp.code_subregion.isin([i])][&#39;name_subregion&#39;])[0] for i in uniqloc if not temp.loc[temp.code_subregion.isin([i])][&#39;name_subregion&#39;].empty }
                    #codename = collections.OrderedDict(zip(uniqloc,list(temp.loc[temp.code_subregion.isin(uniqloc)][&#39;name_subregion&#39;])))
                    self.slocation = list(codename.values())
                    location_is_code = True
                else:
                    #codename=dict(temp.loc[temp.code_subregion.isin(uniqloc)][[&#39;code_subregion&#39;,&#39;name_subregion&#39;]].values)
                    #codename={i:list(temp.loc[temp.code_subregion.isin([i])][&#39;code_subregion&#39;])[0] for i in uniqloc if not temp.loc[temp.code_subregion.isin([i])][&#39;code_subregion&#39;].empty }
                    #codename = collections.OrderedDict(zip(uniqloc,list(temp.loc[temp.name_subregion.isin(uniqloc)][&#39;code_subregion&#39;])))
                    #print(codename)
                    self.slocation = uniqloc
            else:
                CoaDbError(&#39;Granularity problem , neither region nor sub_region ...&#39;)

        if self.db == &#39;dgs&#39;:
            mypandas = mypandas.reset_index(drop=True)

        if self.db != &#39;spfnational&#39;:
            mypandas = mypandas.groupby([&#39;location&#39;,&#39;date&#39;]).sum(min_count=1).reset_index() # summing in case of multiple dates (e.g. in opencovid19 data). But keep nan if any

        if self.db == &#39;govcy&#39;:
            location_is_code=False

        mypandas = fill_missing_dates(mypandas)

        if location_is_code:
            if self.db != &#39;dgs&#39;:
                mypandas[&#39;codelocation&#39;] =  mypandas[&#39;location&#39;].astype(str)
            mypandas[&#39;location&#39;] = mypandas[&#39;location&#39;].map(codename)
            if self.db == &#39;obepine&#39;:
                mypandas = mypandas.dropna(subset=[&#39;location&#39;])
                self.slocation = list(mypandas.codelocation.unique())
            mypandas = mypandas.loc[~mypandas.location.isnull()]
        else:
            mypandas[&#39;codelocation&#39;] =  mypandas[&#39;location&#39;].map(codename).astype(str)
        if self.db == &#39;owid&#39;:
            onlyowid[&#39;codelocation&#39;] = onlyowid[&#39;location&#39;]
            mypandas = mypandas.append(onlyowid)
        self.mainpandas  = mypandas
        self.dates  = self.mainpandas[&#39;date&#39;]

   def get_mainpandas(self,**kwargs):
       &#39;&#39;&#39;
            * defaut :
                 - location = None
                 - date = None
                 - selected_col = None
                Return the csv file to the mainpandas structure
                index | location              | date      | keywords1       |  keywords2    | ...| keywordsn
                -----------------------------------------------------------------------------------------
                0     |        location1      |    1      |  l1-val1-1      |  l1-val2-1    | ...|  l1-valn-1
                1     |        location1      |    2      |  l1-val1-2      |  l1-val2-2    | ...|  l1-valn-2
                2     |        location1      |    3      |  l1-val1-3      |  l1-val2-3    | ...|  l1-valn-3
                                 ...
                p     |       locationp       |    1      |   lp-val1-1     |  lp-val2-1    | ...| lp-valn-1
                ...
            * location : list of location (None : all location)
            * date : latest date to retrieve (None : max date)
            * selected_col: column to keep according to get_available_keys_words (None : all get_available_keys_words)
                            N.B. location column is added
        &#39;&#39;&#39;
       kwargs_test(kwargs,[&#39;location&#39;, &#39;date&#39;, &#39;selected_col&#39;],
                    &#39;Bad args used in the get_stats() function.&#39;)

       location = kwargs.get(&#39;location&#39;, None)
       selected_col = kwargs.get(&#39;selected_col&#39;, None)
       watch_date = kwargs.get(&#39;date&#39;, None)
       if location:
            if not isinstance(location, list):
                clist = ([location]).copy()
            else:
                clist = (location).copy()
            if not all(isinstance(c, str) for c in clist):
                raise CoaWhereError(&#34;Location via the where keyword should be given as strings. &#34;)
            if self.db_world:
                self.geo.set_standard(&#39;name&#39;)
                if self.db == &#39;owid&#39;:
                    owid_name = [c for c in clist if c.startswith(&#39;owid_&#39;)]
                    clist = [c for c in clist if not c.startswith(&#39;owid_&#39;)]
                clist=self.geo.to_standard(clist,output=&#39;list&#39;, interpret_region=True)
            else:
                clist=clist+self.geo.get_subregions_from_list_of_region_names(clist)
                if clist in [&#39;FRA&#39;,&#39;USA&#39;,&#39;ITA&#39;] :
                    clist=self.geo_all[&#39;code_subregion&#39;].to_list()

            clist=list(set(clist)) # to suppress duplicate countries
            diff_locations = list(set(clist) - set(self.get_locations()))
            clist = [i for i in clist if i not in diff_locations]
            filtered_pandas = self.mainpandas.copy()
            if len(clist) == 0 and len(owid_name) == 0:
                raise CoaWhereError(&#39;Not a correct location found according to the where option given.&#39;)
            if self.db == &#39;owid&#39;:
                clist+=owid_name
            filtered_pandas = filtered_pandas.loc[filtered_pandas.location.isin(clist)]
            if watch_date:
                check_valid_date(watch_date)
                mydate = pd.to_datetime(watch_date).date()
            else :
                mydate = filtered_pandas.date.max()
            filtered_pandas = filtered_pandas.loc[filtered_pandas.date==mydate].reset_index(drop=True)
            if selected_col:
                l = selected_col
            else:
                l=list(self.get_available_keys_words())
            l.insert(0, &#39;location&#39;)
            filtered_pandas = filtered_pandas[l]
            return filtered_pandas
       self.mainpandas = self.mainpandas.reset_index(drop=True)
       return self.mainpandas

   @staticmethod
   def flat_list(matrix):
        &#39;&#39;&#39; Flatten list function used in covid19 methods&#39;&#39;&#39;
        flatten_matrix = []
        for sublist in matrix:
            if isinstance(sublist,list):
                for val in sublist:
                    flatten_matrix.append(val)
            else:
                flatten_matrix.append(sublist)
        return flatten_matrix

   def get_dates(self):
        &#39;&#39;&#39; Return all dates available in the current database as datetime format&#39;&#39;&#39;
        return self.dates.values

   def get_locations(self):
        &#39;&#39;&#39; Return available location countries / regions in the current database
            Using the geo method standardization
        &#39;&#39;&#39;
        return self.slocation

   def return_nonan_dates_pandas(self, df = None, field = None):
         &#39;&#39;&#39; Check if for last date all values are nan, if yes check previous date and loop until false&#39;&#39;&#39;
         watchdate = df.date.max()
         boolval = True
         j = 0
         while (boolval):
             boolval = df.loc[df.date == (watchdate - dt.timedelta(days=j))][field].dropna().empty
             j += 1
         df = df.loc[df.date &lt;= watchdate - dt.timedelta(days=j - 1)]
         boolval = True
         j = 0
         watchdate = df.date.min()
         while (boolval):
             boolval = df.loc[df.date == (watchdate + dt.timedelta(days=j))][field].dropna().empty
             j += 1
         df = df.loc[df.date &gt;= watchdate - dt.timedelta(days=j - 1)]
         return df

   def get_stats(self, **kwargs):
        &#39;&#39;&#39;
        Return the pandas pandas_datase
         - index: only an incremental value
         - location: list of location used in the database selected (using geo standardization)
         - &#39;which&#39; :  return the keyword values selected from the avalailable keywords keepted seems
            self.get_available_keys_words()

         - &#39;option&#39; :default none
            * &#39;nonneg&#39; In some cases negatives values can appeared due to a database updated, nonneg option
                will smooth the curve during all the period considered
            * &#39;nofillnan&#39; if you do not want that NaN values are filled, which is the default behaviour
            * &#39;smooth7&#39; moving average, window of 7 days
            * &#39;sumall&#39; sum data over all locations

        keys are keyswords from the selected database
                location        | date      | keywords          |  daily            |  weekly
                -----------------------------------------------------------------------
                location1       |    1      |  val1-1           |  daily1-1          |  diff1-1
                location1       |    2      |  val1-2           |  daily1-2          |  diff1-2
                location1       |    3      |  val1-3           |  daily1-3          |  diff1-3
                    ...             ...                     ...
                location1       | last-date |  val1-lastdate    |  cumul1-lastdate   |   diff1-lastdate
                    ...
                location-i      |    1      |  vali-1           |  dailyi-1          |  diffi-1
                location-i      |    2      |  vali-1           |  daily1i-2         |  diffi-2
                location-i      |    3      |  vali-1           |  daily1i-3         |  diffi-3
                    ...

        &#39;&#39;&#39;
        kwargs_test(kwargs,[&#39;location&#39;,&#39;which&#39;,&#39;option&#39;],
            &#39;Bad args used in the get_stats() function.&#39;)
        wallname = None
        if not &#39;location&#39; in kwargs or kwargs[&#39;location&#39;] is None.__class__ or kwargs[&#39;location&#39;] == None:
            if get_db_list_dict()[self.db][0] == &#39;WW&#39;:
                kwargs[&#39;location&#39;] = &#39;world&#39;
            else:
                kwargs[&#39;location&#39;] = self.slocation #self.geo_all[&#39;code_subregion&#39;].to_list()
            wallname = get_db_list_dict()[self.db][2]
        else:
            kwargs[&#39;location&#39;] = kwargs[&#39;location&#39;]

        option = kwargs.get(&#39;option&#39;, &#39;fillnan&#39;)
        fillnan = True # default
        sumall = False # default
        sumallandsmooth7 = False
        if kwargs[&#39;which&#39;] not in self.get_available_keys_words():
            raise CoaKeyError(kwargs[&#39;which&#39;]+&#39; is not a available for &#39; + self.db + &#39; database name. &#39;
            &#39;See get_available_keys_words() for the full list.&#39;)

        #while for last date all values are nan previous date
        mainpandas = self.return_nonan_dates_pandas(self.get_mainpandas(),kwargs[&#39;which&#39;])
        devorigclist = None
        origclistlist = None
        origlistlistloc = None
        if option and &#39;sumall&#39; in option:
            if not isinstance(kwargs[&#39;location&#39;], list):
                kwargs[&#39;location&#39;] = [[kwargs[&#39;location&#39;]]]
            else:
                if isinstance(kwargs[&#39;location&#39;][0], list):
                    kwargs[&#39;location&#39;] = kwargs[&#39;location&#39;]
                else:
                    kwargs[&#39;location&#39;] = [kwargs[&#39;location&#39;]]
        if not isinstance(kwargs[&#39;location&#39;], list):
            listloc = ([kwargs[&#39;location&#39;]]).copy()
            if not all(isinstance(c, str) for c in listloc):
                raise CoaWhereError(&#34;Location via the where keyword should be given as strings. &#34;)
            origclist = listloc
        else:
            listloc = (kwargs[&#39;location&#39;]).copy()
            origclist = listloc
            if any(isinstance(c, list) for c in listloc):
                if all(isinstance(c, list) for c in listloc):
                    origlistlistloc = listloc
                else:
                    raise CoaWhereError(&#34;In the case of sumall all locations must have the same types i.e\
                    list or string but both is not accepted, could be confusing&#34;)
        owid_name=&#39;&#39;
        if self.db_world:
            self.geo.set_standard(&#39;name&#39;)
            if origlistlistloc != None:
                #fulllist = [ i if isinstance(i, list) else [i] for i in origclist ]
                fulllist = []
                for deploy in origlistlistloc:
                    d=[]
                    for i in deploy:
                        if not self.geo.get_GeoRegion().is_region(i):
                            d.append(self.geo.to_standard(i,output=&#39;list&#39;,interpret_region=True)[0])
                        else:
                            d.append(self.geo.get_GeoRegion().is_region(i))
                    fulllist.append(d)
                dicooriglist = { &#39;,&#39;.join(i):self.geo.to_standard(i,output=&#39;list&#39;,interpret_region=True) for i in fulllist}
                location_exploded = list(dicooriglist.values())
            else:
                owid_name = [c for c in origclist if c.startswith(&#39;owid_&#39;)]
                clist = [c for c in origclist if not c.startswith(&#39;owid_&#39;)]
                location_exploded = self.geo.to_standard(listloc,output=&#39;list&#39;,interpret_region=True)
                if len(owid_name) !=0 :
                    location_exploded += owid_name
        else:
            def explosion(listloc,typeloc=&#39;subregion&#39;):
                exploded = []
                a=self.geo.get_data()
                for i in listloc:
                    if typeloc == &#39;subregion&#39;:
                        if self.geo.is_region(i):
                            i = [self.geo.is_region(i)]
                            tmp = self.geo.get_subregions_from_list_of_region_names(i,output=&#39;name&#39;)
                        elif self.geo.is_subregion(i):
                           tmp = self.geo.is_subregion(i)
                        else:
                            raise CoaTypeError(i + &#39;: not subregion nor region ... what is it ?&#39;)
                    elif typeloc == &#39;region&#39;:
                        tmp = self.geo.get_region_list()
                        if i.isdigit():
                            tmp = list(tmp.loc[tmp.code_region==i][&#39;name_region&#39;])
                        elif self.geo.is_region(i):
                            tmp = self.geo.get_regions_from_macroregion(name=i,output=&#39;name&#39;)
                            if get_db_list_dict()[self.db][0] in [&#39;USA, FRA, ESP, PRT&#39;]:
                                tmp = tmp[:-1]
                        else:
                            if self.geo.is_subregion(i):
                                raise CoaTypeError(i+ &#39; is a subregion ... not compatible with a region DB granularity?&#39;)
                            else:
                                raise CoaTypeError(i + &#39;: not subregion nor region ... what is it ?&#39;)
                    else:
                        raise CoaTypeError(&#39;Not subregion nor region requested, don\&#39;t know what to do ?&#39;)
                    if exploded:
                        exploded.append(tmp)
                    else:
                        exploded=[tmp]
                return DataBase.flat_list(exploded)

            if origlistlistloc != None:
                dicooriglist={&#39;,&#39;.join(i):explosion(i,self.database_type[self.db][1]) for i in origlistlistloc}
                #origlistlistloc = DataBase.flat_list(list(dicooriglist.values()))
                #location_exploded = origlistlistloc
            else:
                listloc = explosion(listloc,self.database_type[self.db][1])
                listloc = DataBase.flat_list(listloc)
                location_exploded = listloc
        def sticky(lname):
            if len(lname)&gt;0:
                tmp=&#39;&#39;
                for i in lname:
                    tmp += i+&#39;, &#39;
                lname=tmp[:-2]
            return [lname]

        pdcluster = pd.DataFrame()
        j=0

        if origlistlistloc != None:
            for k,v in dicooriglist.items():
                tmp  = mainpandas.copy()
                if any(isinstance(c, list) for c in v):
                    v=v[0]
                tmp = tmp.loc[tmp.location.isin(v)]
                code = tmp.codelocation.unique()
                tmp[&#39;clustername&#39;] = [k]*len(tmp)
                if pdcluster.empty:
                    pdcluster = tmp
                else:
                    pdcluster = pdcluster.append(tmp)
                j+=1
            pdfiltered = pdcluster[[&#39;location&#39;,&#39;date&#39;,&#39;codelocation&#39;,kwargs[&#39;which&#39;],&#39;clustername&#39;]]
        else:
            pdfiltered = mainpandas.loc[mainpandas.location.isin(location_exploded)]
            pdfiltered = pdfiltered[[&#39;location&#39;,&#39;date&#39;,&#39;codelocation&#39;,kwargs[&#39;which&#39;]]]
            pdfiltered[&#39;clustername&#39;] = pdfiltered[&#39;location&#39;].copy()
        if not isinstance(option,list):
            option=[option]
        if &#39;fillnan&#39; not in option and &#39;nofillnan&#39; not in option:
            option.insert(0, &#39;fillnan&#39;)
        if &#39;nonneg&#39; in option:
            option.remove(&#39;nonneg&#39;)
            option.insert(0, &#39;nonneg&#39;)
        if &#39;smooth7&#39; in  option and &#39;sumall&#39; in  option:
            option.remove(&#39;sumall&#39;)
            option.remove(&#39;smooth7&#39;)
            option+=[&#39;sumallandsmooth7&#39;]
        for o in option:
            if o == &#39;nonneg&#39;:
                if kwargs[&#39;which&#39;].startswith(&#39;cur_&#39;):
                    raise CoaKeyError(&#39;The option nonneg cannot be used with instantaneous data, such as cur_ which variables.&#39;)
                cluster=list(pdfiltered.clustername.unique())
                separated = [ pdfiltered.loc[pdfiltered.clustername==i] for i in cluster]
                reconstructed = pd.DataFrame()
                for sub in separated:
                    location = list(sub.location.unique())
                    for loca in location:
                        pdloc = sub.loc[sub.location == loca][kwargs[&#39;which&#39;]]
                        try:
                            y0=pdloc.values[0] # integrated offset at t=0
                        except:
                            y0=0
                        if np.isnan(y0):
                            y0=0
                        pa = pdloc.diff()
                        yy = pa.values
                        ind = list(pa.index)
                        where_nan = np.isnan(yy)
                        yy[where_nan] = 0.
                        indices=np.where(yy &lt; 0)[0]
                        for kk in np.where(yy &lt; 0)[0]:
                            k = int(kk)
                            val_to_repart = -yy[k]
                            if k &lt; np.size(yy)-1:
                                yy[k] = (yy[k+1]+yy[k-1])/2
                            else:
                                yy[k] = yy[k-1]
                            val_to_repart = val_to_repart + yy[k]
                            s = np.nansum(yy[0:k])
                            if not any([i !=0 for i in yy[0:k]]) == True and s == 0:
                                yy[0:k] = 0.
                            elif s == 0:
                                yy[0:k] = np.nan*np.ones(k)
                            else:
                                yy[0:k] = yy[0:k]*(1-float(val_to_repart)/s)
                        sub=sub.copy()
                        sub.loc[ind,kwargs[&#39;which&#39;]]=np.cumsum(yy)+y0 # do not forget the offset
                    if reconstructed.empty:
                        reconstructed = sub
                    else:
                        reconstructed=reconstructed.append(sub)
                    pdfiltered = reconstructed
            elif o == &#39;nofillnan&#39;:
                pdfiltered_nofillnan = pdfiltered.copy().reset_index(drop=True)
                fillnan=False
            elif o == &#39;fillnan&#39;:
                fillnan=True
                # fill with previous value
                pdfiltered = pdfiltered.reset_index(drop=True)
                pdfiltered_nofillnan = pdfiltered.copy()

                pdfiltered.loc[:,kwargs[&#39;which&#39;]] =\
                pdfiltered.groupby([&#39;location&#39;,&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.bfill())
                #if kwargs[&#39;which&#39;].startswith(&#39;total_&#39;) or kwargs[&#39;which&#39;].startswith(&#39;tot_&#39;):
                #    pdfiltered.loc[:,kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.ffill())
                if pdfiltered.loc[pdfiltered.date == pdfiltered.date.max()][kwargs[&#39;which&#39;]].isnull().values.any():
                    print(kwargs[&#39;which&#39;], &#34;has been selected. Some missing data has been interpolated from previous data.&#34;)
                    print(&#34;This warning appear right now due to some missing values at the latest date &#34;, pdfiltered.date.max(),&#34;.&#34;)
                    print(&#34;Use the option=&#39;nofillnan&#39; if you want to only display the original data&#34;)
                    pdfiltered.loc[:,kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;location&#39;,&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.ffill())
                    pdfiltered = pdfiltered[pdfiltered[kwargs[&#39;which&#39;]].notna()]
            elif o == &#39;smooth7&#39;:
                pdfiltered[kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;location&#39;])[kwargs[&#39;which&#39;]].rolling(7,min_periods=7).mean().reset_index(level=0,drop=True)
                inx7=pdfiltered.groupby(&#39;location&#39;).head(7).index
                pdfiltered.loc[inx7, kwargs[&#39;which&#39;]] = pdfiltered[kwargs[&#39;which&#39;]].fillna(method=&#34;bfill&#34;)
                fillnan=True
            elif o == &#39;sumall&#39;:
                sumall = True
            elif o == &#39;sumallandsmooth7&#39;:
                sumall = True
                sumallandsmooth7 = True
            elif o != None and o != &#39;&#39; and o != &#39;sumallandsmooth7&#39;:
                raise CoaKeyError(&#39;The option &#39;+o+&#39; is not recognized in get_stats. See get_available_options() for list.&#39;)
        pdfiltered = pdfiltered.reset_index(drop=True)

        # if sumall set, return only integrate val
        tmppandas=pd.DataFrame()
        if sumall:
            if origlistlistloc != None:
               uniqcluster = pdfiltered.clustername.unique()
               if kwargs[&#39;which&#39;].startswith(&#39;cur_idx_&#39;):
                  tmp = pdfiltered.groupby([&#39;clustername&#39;,&#39;date&#39;]).mean().reset_index()
               else:
                  tmp = pdfiltered.groupby([&#39;clustername&#39;,&#39;date&#39;]).sum().reset_index()#.loc[pdfiltered.clustername.isin(uniqcluster)].\

               codescluster = {i:list(pdfiltered.loc[pdfiltered.clustername==i][&#39;codelocation&#39;].unique()) for i in uniqcluster}
               namescluster = {i:list(pdfiltered.loc[pdfiltered.clustername==i][&#39;location&#39;].unique()) for i in uniqcluster}
               tmp[&#39;codelocation&#39;] = tmp[&#39;clustername&#39;].map(codescluster)
               tmp[&#39;location&#39;] = tmp[&#39;clustername&#39;].map(namescluster)

               pdfiltered = tmp
               pdfiltered = pdfiltered.drop_duplicates([&#39;date&#39;,&#39;clustername&#39;])
               if sumallandsmooth7:
                   pdfiltered[kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;clustername&#39;])[kwargs[&#39;which&#39;]].rolling(7,min_periods=7).mean().reset_index(level=0,drop=True)
                   pdfiltered.loc[:,kwargs[&#39;which&#39;]] =\
                   pdfiltered.groupby([&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.bfill())
            # computing daily, cumul and weekly
            else:
                if kwargs[&#39;which&#39;].startswith(&#39;cur_idx_&#39;):
                    tmp = pdfiltered.groupby([&#39;date&#39;]).mean().reset_index()
                else:
                    tmp = pdfiltered.groupby([&#39;date&#39;]).sum().reset_index()
                uniqloc = list(pdfiltered.location.unique())
                uniqcodeloc = list(pdfiltered.codelocation.unique())
                tmp.loc[:,&#39;location&#39;] = [&#39;dummy&#39;]*len(tmp)
                tmp.loc[:,&#39;codelocation&#39;] = [&#39;dummy&#39;]*len(tmp)
                tmp.loc[:,&#39;clustername&#39;] = [&#39;dummy&#39;]*len(tmp)
                for i in range(len(tmp)):
                    tmp.at[i,&#39;location&#39;] = uniqloc #sticky(uniqloc)
                    tmp.at[i,&#39;codelocation&#39;] = uniqcodeloc #sticky(uniqcodeloc)
                    tmp.at[i,&#39;clustername&#39;] =  sticky(uniqloc)[0]
                pdfiltered = tmp
        else:
            if self.db_world :
                pdfiltered[&#39;clustername&#39;] = pdfiltered[&#39;location&#39;].apply(lambda x: self.geo.to_standard(x)[0] if not x.startswith(&#34;owid_&#34;) else x)
            else:
                pdfiltered[&#39;clustername&#39;] = pdfiltered[&#39;location&#39;]

        if &#39;cur_&#39; in kwargs[&#39;which&#39;] or &#39;total_&#39; in kwargs[&#39;which&#39;] or &#39;tot_&#39; in kwargs[&#39;which&#39;]:
            pdfiltered[&#39;cumul&#39;] = pdfiltered[kwargs[&#39;which&#39;]]
        else:
            pdfiltered[&#39;cumul&#39;] = pdfiltered_nofillnan.groupby(&#39;clustername&#39;)[kwargs[&#39;which&#39;]].cumsum()
            if fillnan:
                pdfiltered.loc[:,&#39;cumul&#39;] =\
                pdfiltered.groupby(&#39;clustername&#39;)[&#39;cumul&#39;].apply(lambda x: x.ffill())

        pdfiltered[&#39;daily&#39;] = pdfiltered.groupby(&#39;clustername&#39;)[&#39;cumul&#39;].diff()
        inx = pdfiltered.groupby(&#39;clustername&#39;).head(1).index
        pdfiltered[&#39;weekly&#39;] = pdfiltered.groupby(&#39;clustername&#39;)[&#39;cumul&#39;].diff(7)
        inx7=pdfiltered.groupby(&#39;clustername&#39;).head(7).index
        #First value of diff is always NaN
        pdfiltered.loc[inx, &#39;daily&#39;] = pdfiltered[&#39;daily&#39;].fillna(method=&#34;bfill&#34;)
        pdfiltered.loc[inx7, &#39;weekly&#39;] = pdfiltered[&#39;weekly&#39;].fillna(method=&#34;bfill&#34;)

        unifiedposition=[&#39;location&#39;, &#39;date&#39;, kwargs[&#39;which&#39;], &#39;daily&#39;, &#39;cumul&#39;, &#39;weekly&#39;, &#39;codelocation&#39;,&#39;clustername&#39;]
        pdfiltered = pdfiltered[unifiedposition]

        if wallname != None and sumall == True:
               pdfiltered.loc[:,&#39;clustername&#39;] = wallname

        pdfiltered = pdfiltered.drop(columns=&#39;cumul&#39;)
        verb(&#34;Here the information I\&#39;ve got on &#34;, kwargs[&#39;which&#39;],&#34; : &#34;, self.get_keyword_definition(kwargs[&#39;which&#39;]))
        return pdfiltered

   def merger(self,**kwargs):
        &#39;&#39;&#39;
        Merge two or more pycoa pandas from get_stats operation
        &#39;coapandas&#39;: list (min 2D) of pandas from stats
        &#39;&#39;&#39;

        coapandas = kwargs.get(&#39;coapandas&#39;, None)

        if coapandas is None or not isinstance(coapandas, list) or len(coapandas)&lt;=1:
            raise CoaKeyError(&#39;coapandas value must be at least a list of 2 elements ... &#39;)

        def renamecol(pandy):
            torename=[&#39;daily&#39;,&#39;cumul&#39;,&#39;weekly&#39;]
            return pandy.rename(columns={i:pandy.columns[2]+&#39;_&#39;+i  for i in torename})
        base = coapandas[0].copy()
        coapandas = [ renamecol(p) for p in coapandas ]
        base = coapandas[0].copy()
        if not &#39;clustername&#39; in base.columns:
            raise CoaKeyError(&#39;No &#34;clustername&#34; in your pandas columns ... don\&#39;t know what to do &#39;)

        j=1
        for p in coapandas[1:]:
            [ p.drop([i],axis=1, inplace=True) for i in [&#39;location&#39;,&#39;where&#39;,&#39;codelocation&#39;] if i in p.columns ]
            #p.drop([&#39;location&#39;,&#39;codelocation&#39;],axis=1, inplace=True)
            base = pd.merge(base,p,on=[&#39;date&#39;,&#39;clustername&#39;],how=&#34;inner&#34;)#,suffixes=(&#39;&#39;, &#39;_drop&#39;))
            #base.drop([col for col in base.columns if &#39;drop&#39; in col], axis=1, inplace=True)
        return base

   def appender(self,**kwargs):
      &#39;&#39;&#39;
      Append two or more pycoa pandas from get_stats operation
      &#39;coapandas&#39;: list (min 2D) of pandas from stats
      &#39;&#39;&#39;

      coapandas = kwargs.get(&#39;coapandas&#39;, None)
      if coapandas is None or not isinstance(coapandas, list) or len(coapandas)&lt;=1:
          raise CoaKeyError(&#39;coapandas value must be at least a list of 2 elements ... &#39;)

      coapandas = [ p.rename(columns={p.columns[2]:&#39;cases&#39;}) for p in coapandas ]
      m = pd.concat(coapandas).reset_index(drop=True)
      #m[&#39;clustername&#39;]=m.m(&#39;location&#39;)[&#39;clustername&#39;].fillna(method=&#39;bfill&#39;)
      #m[&#39;codelocation&#39;]=m.groupby(&#39;location&#39;)[&#39;codelocation&#39;].fillna(method=&#39;bfill&#39;)
      m=m.drop(columns=[&#39;codelocation&#39;,&#39;clustername&#39;])
      return fill_missing_dates(m)

   def saveoutput(self,**kwargs):
       &#39;&#39;&#39;
       saveoutput pycoas pandas as an  output file selected by output argument
       &#39;pandas&#39;: pycoa pandas
       &#39;saveformat&#39;: excel or csv (default excel)
       &#39;savename&#39;: pycoaout (default)
       &#39;&#39;&#39;
       possibleformat=[&#39;excel&#39;,&#39;csv&#39;]
       saveformat = &#39;excel&#39;
       savename = &#39;pycoaout&#39;
       pandyori = &#39;&#39;
       if &#39;saveformat&#39; in kwargs:
            saveformat = kwargs[&#39;saveformat&#39;]
       if saveformat not in possibleformat:
           raise CoaKeyError(&#39;Output option &#39;+saveformat+&#39; is not recognized.&#39;)
       if &#39;savename&#39; in kwargs and kwargs[&#39;savename&#39;] != &#39;&#39;:
          savename = kwargs[&#39;savename&#39;]

       if not &#39;pandas&#39; in kwargs:
          raise CoaKeyError(&#39;Absolute needed variable : the pandas desired &#39;)
       else:
          pandyori = kwargs[&#39;pandas&#39;]
       pandy = pandyori
       pandy[&#39;date&#39;] = pd.to_datetime(pandy[&#39;date&#39;])
       pandy[&#39;date&#39;]=pandy[&#39;date&#39;].apply(lambda x: x.strftime(&#39;%Y-%m-%d&#39;))
       if saveformat == &#39;excel&#39;:
           pandy.to_excel(savename+&#39;.xlsx&#39;,index=False, na_rep=&#39;NAN&#39;)
       elif saveformat == &#39;csv&#39;:
           pandy.to_csv(savename+&#39;.csv&#39;, encoding=&#39;utf-8&#39;, index=False, float_format=&#39;%.4f&#39;,na_rep=&#39;NAN&#39;)

   ## https://www.kaggle.com/freealf/estimation-of-rt-from-cases
   def smooth_cases(self,cases):
        new_cases = cases

        smoothed = new_cases.rolling(7,
            win_type=&#39;gaussian&#39;,
            min_periods=1,
            center=True).mean(std=2).round()
            #center=False).mean(std=2).round()

        zeros = smoothed.index[smoothed.eq(0)]
        if len(zeros) == 0:
            idx_start = 0
        else:
            last_zero = zeros.max()
            idx_start = smoothed.index.get_loc(last_zero) + 1
        smoothed = smoothed.iloc[idx_start:]
        original = new_cases.loc[smoothed.index]

        return smoothed
   def get_posteriors(self,sr, window=7, min_periods=1):
        # We create an array for every possible value of Rt
        R_T_MAX = 12
        r_t_range = np.linspace(0, R_T_MAX, R_T_MAX*100+1)

        # Gamma is 1/serial interval
        # https://wwwnc.cdc.gov/eid/article/26/6/20-0357_article
        GAMMA = 1/7

        lam = sr[:-1].values * np.exp(GAMMA * (r_t_range[:, None] - 1))

        # Note: if you want to have a Uniform prior you can use the following line instead.
        # I chose the gamma distribution because of our prior knowledge of the likely value
        # of R_t.

        # prior0 = np.full(len(r_t_range), np.log(1/len(r_t_range)))
        prior0 = np.log(sps.gamma(a=3).pdf(r_t_range) + 1e-14)

        likelihoods = pd.DataFrame(
            # Short-hand way of concatenating the prior and likelihoods
            data = np.c_[prior0, sps.poisson.logpmf(sr[1:].values, lam)],
            index = r_t_range,
            columns = sr.index)

        # Perform a rolling sum of log likelihoods. This is the equivalent
        # of multiplying the original distributions. Exponentiate to move
        # out of log.
        posteriors = likelihoods.rolling(window,
                                     axis=1,
                                     min_periods=min_periods).sum()
        posteriors = np.exp(posteriors)

        # Normalize to 1.0
        posteriors = posteriors.div(posteriors.sum(axis=0), axis=1)

        return posteriors</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="coa.covid19.DataBase"><code class="flex name class">
<span>class <span class="ident">DataBase</span></span>
<span>(</span><span>db_name)</span>
</code></dt>
<dd>
<div class="desc"><p>DataBase class
Parse a Covid-19 database and filled the pandas python objet : mainpandas
It takes a string argument, which can be: 'jhu','spf', 'spfnational','owid', 'opencovid19' and 'opencovid19national'</p>
<p>Fill the pandas_datase</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataBase(object):
   &#34;&#34;&#34;
   DataBase class
   Parse a Covid-19 database and filled the pandas python objet : mainpandas
   It takes a string argument, which can be: &#39;jhu&#39;,&#39;spf&#39;, &#39;spfnational&#39;,&#39;owid&#39;, &#39;opencovid19&#39; and &#39;opencovid19national&#39;
   &#34;&#34;&#34;
   def __init__(self, db_name):
        &#34;&#34;&#34;
         Fill the pandas_datase
        &#34;&#34;&#34;
        verb(&#34;Init of covid19.DataBase()&#34;)
        self.database_name = list(get_db_list_dict().keys())
        self.database_type = get_db_list_dict()
        self.available_options = [&#39;nonneg&#39;, &#39;nofillnan&#39;, &#39;smooth7&#39;, &#39;sumall&#39;]
        self.available_keys_words = []
        self.dates = []
        self.database_columns_not_computed = {}
        self.db = db_name
        self.geo_all = &#39;&#39;
        self.database_url = []
        self.db_world=None
        self.databaseinfo = report
        if self.db not in self.database_name:
            raise CoaDbError(&#39;Unknown &#39; + self.db + &#39;. Available database so far in PyCoa are : &#39; + str(self.database_name), file=sys.stderr)
        else:
            try:
                if get_db_list_dict()[self.db][1] == &#39;nation&#39;: # world wide db
                    self.db_world = True
                    self.geo = coge.GeoManager(&#39;name&#39;)
                    self.geo_all = &#39;world&#39;
                else: # local db
                    self.db_world = False
                    self.geo = coge.GeoCountry(get_db_list_dict()[self.db][0])
                    if get_db_list_dict()[self.db][1] == &#39;region&#39;:
                        self.geo_all = self.geo.get_region_list()
                    elif get_db_list_dict()[self.db][1] == &#39;subregion&#39;:
                        self.geo_all = self.geo.get_subregion_list()
                    else:
                        CoaError(&#39;Granularity problem, neither region or subregion&#39;)
                self.set_display(self.db,self.geo)

                # specific reading of data according to the db
                if self.db == &#39;jhu&#39;:
                    info(&#39;JHU aka Johns Hopkins database selected ...&#39;)
                    self.return_jhu_pandas()
                elif self.db == &#39;jhu-usa&#39;: #USA
                    info(&#39;USA, JHU aka Johns Hopkins database selected ...&#39;)
                    self.return_jhu_pandas()
                elif self.db == &#39;imed&#39;:
                    info(&#39;Greece, imed database selected ...&#39;)
                    self.return_jhu_pandas()
                elif self.db == &#39;govcy&#39;: #CYP
                    info(&#39;Cyprus, govcy database selected ...&#39;)
                    rename_dict = {&#39;daily deaths&#39;: &#39;tot_deaths&#39;}
                    gov = self.csv2pandas(&#39;https://www.data.gov.cy/sites/default/files/CY%20Covid19%20Open%20Data%20-%20Extended%20-%20new_247.csv&#39;
                    ,separator=&#39;,&#39;)
                    columns_keeped = [&#39;tot_deaths&#39;]
                    gov[&#39;tot_deaths&#39;]=gov.groupby([&#39;location&#39;])[&#39;daily deaths&#39;].cumsum()
                    self.return_structured_pandas(gov, columns_keeped=columns_keeped)
                elif self.db == &#39;dpc&#39;: #ITA
                    info(&#39;ITA, Dipartimento della Protezione Civile database selected ...&#39;)
                    rename_dict = {&#39;data&#39;: &#39;date&#39;, &#39;denominazione_regione&#39;: &#39;location&#39;, &#39;totale_casi&#39;: &#39;tot_cases&#39;,&#39;deceduti&#39;:&#39;tot_deaths&#39;}
                    dpc1 = self.csv2pandas(&#39;https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv&#39;,\
                    rename_columns = rename_dict, separator=&#39;,&#39;)
                    #dpc1 = self.csv2pandas(&#34;https://github.com/pcm-dpc/COVID-19/raw/master/dati-province/dpc-covid19-ita-province.csv&#34;,\
                    columns_keeped = [&#39;tot_deaths&#39;,&#39;tot_cases&#39;]
                    self.return_structured_pandas(dpc1, columns_keeped=columns_keeped)
                elif self.db == &#39;rki&#39;: # DEU
                    info(&#39;DEU, Robert Koch Institut data selected ...&#39;)
                    self.return_jhu_pandas()
                elif self.db == &#39;dgs&#39;: # PRT
                    info(&#39;PRT, Direcção Geral de Saúde - Ministério da Saúde Português data selected ...&#39;)
                    rename_dict = {&#39;data&#39;: &#39;date&#39;,&#39;concelho&#39;:&#39;location&#39;,&#39;confirmados_1&#39;:&#39;tot_cases&#39;}
                    url=&#39;https://raw.githubusercontent.com/dssg-pt/covid19pt-data/master/data_concelhos_new.csv&#39;
                    prt_data=self.csv2pandas(url,separator=&#39;,&#39;,rename_columns = rename_dict)
                    columns_keeped = [&#39;tot_cases&#39;]
                    self.return_structured_pandas(prt_data, columns_keeped=columns_keeped)
                elif self.db == &#39;obepine&#39; : # FRA
                    info(&#39;FRA, réseau Obepine, surveillance Sars-Cov-2 dans les eaux usées&#39;)
                    url=&#39;https://www.data.gouv.fr/fr/datasets/r/69b8af15-c8c5-465a-bdb6-1ac73430e590&#39;
                    #url=&#39;https://www.data.gouv.fr/fr/datasets/r/89196725-56cf-4a83-bab0-170ad1e8ef85&#39;
                    rename_dict={&#39;Code_Region&#39;:&#39;location&#39;,&#39;Date&#39;:&#39;date&#39;,&#39;Indicateur\&#34;&#39;:&#39;idx_obepine&#39;}
                    cast = {&#39;Code_Region&#39;: &#39;string&#39;}
                    obepine_data=self.csv2pandas(url,cast=cast,separator=&#39;;&#39;,rename_columns=rename_dict)
                    obepine_data[&#39;idx_obepine&#39;]=obepine_data[&#39;idx_obepine&#39;].astype(float)
                    self.return_structured_pandas(obepine_data,columns_keeped=[&#39;idx_obepine&#39;])
                elif self.db == &#39;escovid19data&#39;: # ESP
                    info(&#39;ESP, EsCovid19Data ...&#39;)
                    rename_dict = {&#39;ine_code&#39;: &#39;location&#39;,\
                        &#39;deceased&#39;:&#39;tot_deaths&#39;,\
                        &#39;cases_accumulated_PCR&#39;:&#39;tot_cases&#39;,\
                        &#39;hospitalized&#39;:&#39;cur_hosp&#39;,\
                        &#39;hospitalized_accumulated&#39;:&#39;tot_hosp&#39;,\
                        &#39;intensive_care&#39;:&#39;cur_icu&#39;,\
                        &#39;recovered&#39;:&#39;tot_recovered&#39;,\
                        &#39;cases_per_cienmil&#39;:&#39;tot_cases_per100k&#39;,\
                        &#39;intensive_care_per_1000000&#39;:&#39;cur_icu_per1M&#39;,\
                        &#39;deceassed_per_100000&#39;:&#39;tot_deaths_per100k&#39;,\
                        &#39;hospitalized_per_100000&#39;:&#39;cur_hosp_per100k&#39;,\
                        &#39;ia14&#39;:&#39;incidence&#39;,\
                        &#39;poblacion&#39;:&#39;population&#39;,\
                    }
                    #url=&#39;https://github.com/montera34/escovid19data/raw/master/data/output/covid19-provincias-spain_consolidated.csv&#39;
                    url=&#39;https://raw.githubusercontent.com/montera34/escovid19data/master/data/output/covid19-provincias-spain_consolidated.csv&#39;
                    col_names = pd.read_csv(get_local_from_url(url), nrows=0).columns
                    cast={i:&#39;string&#39; for i in col_names[17:]}
                    esp_data=self.csv2pandas(url,\
                        separator=&#39;,&#39;,rename_columns = rename_dict,cast = cast)
                    #print(&#39;Available columns : &#39;)
                    #display(esp_data.columns)
                    esp_data[&#39;location&#39;]=esp_data.location.astype(str).str.zfill(2)
                    columns_keeped = list(rename_dict.values())
                    columns_keeped.remove(&#39;location&#39;)

                    for w in list(columns_keeped):
                            esp_data[w]=pd.to_numeric(esp_data[w], errors = &#39;coerce&#39;)

                    self.return_structured_pandas(esp_data,columns_keeped=columns_keeped)

                elif self.db == &#39;sciensano&#39;: #Belgian institute for health,
                    info(&#39;BEL, Sciensano Belgian institute for health data  ...&#39;)
                    rename_dict = { &#39;DATE&#39; : &#39;date&#39;,\
                    &#39;PROVINCE&#39;:&#39;location&#39;,\
                    &#39;TOTAL_IN&#39;:&#39;cur_hosp&#39;,
                    &#39;TOTAL_IN_ICU&#39;:&#39;cur_icu&#39;,
                    &#39;TOTAL_IN_RESP&#39;:&#39;cur_resp&#39;,
                    &#39;TOTAL_IN_ECMO&#39;:&#39;cur_ecmo&#39;}
                    url=&#39;https://epistat.sciensano.be/Data/COVID19BE_HOSP.csv&#39;
                    beldata=self.csv2pandas(url,separator=&#39;,&#39;,rename_columns=rename_dict)
                    [rename_dict.pop(i) for i in [&#39;DATE&#39;,&#39;PROVINCE&#39;]]
                    columns_keeped = list(rename_dict.values())
                    cvsloc2jsonloc={
                    &#39;BrabantWallon&#39;:&#39;Brabant wallon (le)&#39;,\
                    &#39;Brussels&#39;:&#39;Région de Bruxelles-Capitale&#39;,\
                    &#39;Limburg&#39;:&#39;Limbourg (le)&#39;,\
                    &#39;OostVlaanderen&#39;:&#39;Flandre orientale (la)&#39;,\
                    &#39;Hainaut&#39;:&#39;Hainaut (le)&#39;,\
                    &#39;VlaamsBrabant&#39;:&#39;Brabant flamand (le)&#39;,\
                    &#39;WestVlaanderen&#39;:&#39;Flandre occidentale (la)&#39;,\
                    }
                    beldata[&#34;location&#34;].replace(cvsloc2jsonloc, inplace=True)
                    beldata[&#39;date&#39;] = pandas.to_datetime(beldata[&#39;date&#39;],errors=&#39;coerce&#39;).dt.date
                    self.return_structured_pandas(beldata,columns_keeped=columns_keeped)
                elif self.db == &#39;phe&#39;: # GBR from owid
                    info(&#39;GBR, Public Health England data ...&#39;)
                    rename_dict = { &#39;areaCode&#39;:&#39;location&#39;,\
                        &#39;cumDeaths28DaysByDeathDate&#39;:&#39;tot_deaths&#39;,\
                        &#39;cumCasesBySpecimenDate&#39;:&#39;tot_cases&#39;,\
                        &#39;cumLFDTestsBySpecimenDate&#39;:&#39;tot_tests&#39;,\
                        &#39;cumPeopleVaccinatedFirstDoseByVaccinationDate&#39;:&#39;tot_vacc1&#39;,\
                        &#39;cumPeopleVaccinatedSecondDoseByVaccinationDate&#39;:&#39;tot_vacc2&#39;,\
                        #&#39;cumPeopleVaccinatedThirdInjectionByVaccinationDate&#39;:&#39;tot_vacc3&#39;,\
                        #&#39;covidOccupiedMVBeds&#39;:&#39;cur_icu&#39;,\
                        #&#39;cumPeopleVaccinatedFirstDoseByVaccinationDate&#39;:&#39;tot_dose1&#39;,\
                        #&#39;cumPeopleVaccinatedSecondDoseByVaccinationDate&#39;:&#39;tot_dose2&#39;,\
                        #&#39;hospitalCases&#39;:&#39;cur_hosp&#39;,\
                        }
                    url = &#39;https://api.coronavirus.data.gov.uk/v2/data?areaType=ltla&#39;
                    for w in rename_dict.keys():
                        if w not in [&#39;areaCode&#39;]:
                            url=url+&#39;&amp;metric=&#39;+w
                    url = url+&#39;&amp;format=csv&#39;
                    gbr_data = self.csv2pandas(url,separator=&#39;,&#39;,rename_columns=rename_dict)
                    constraints = {&#39;Lineage&#39;: &#39;B.1.617.2&#39;}
                    url = &#39;https://covid-surveillance-data.cog.sanger.ac.uk/download/lineages_by_ltla_and_week.tsv&#39;
                    gbrvar = self.csv2pandas(url,separator=&#39;\t&#39;,constraints=constraints,rename_columns = {&#39;WeekEndDate&#39;: &#39;date&#39;,&#39;LTLA&#39;:&#39;location&#39;})
                    varname =  &#39;B.1.617.2&#39;
                    gbr_data = pd.merge(gbr_data,gbrvar,how=&#34;outer&#34;,on=[&#39;location&#39;,&#39;date&#39;])
                    gbr_data = gbr_data.rename(columns={&#39;Count&#39;:&#39;cur_&#39;+varname})
                    columns_keeped = list(rename_dict.values())
                    columns_keeped.append(&#39;cur_&#39;+varname)
                    columns_keeped.remove(&#39;location&#39;)
                    self.return_structured_pandas(gbr_data,columns_keeped=columns_keeped)
                elif self.db == &#39;moh&#39;: # MYS
                    info(&#39;Malaysia moh covid19-public database selected ...&#39;)
                    rename_dict = {&#39;state&#39;: &#39;location&#39;}
                    moh1 = self.csv2pandas(&#34;https://raw.githubusercontent.com/MoH-Malaysia/covid19-public/main/epidemic/cases_state.csv&#34;,rename_columns=rename_dict,separator=&#39;,&#39;)
                    moh1[&#39;tot_cases&#39;]=moh1.groupby([&#39;location&#39;])[&#39;cases_new&#39;].cumsum()

                    moh2 = self.csv2pandas(&#34;https://raw.githubusercontent.com/MoH-Malaysia/covid19-public/main/epidemic/hospital.csv&#34;,rename_columns=rename_dict,separator=&#39;,&#39;)
                    moh3 = self.csv2pandas(&#34;https://raw.githubusercontent.com/MoH-Malaysia/covid19-public/main/epidemic/icu.csv&#34;,rename_columns=rename_dict,separator=&#39;,&#39;)
                    moh4 = self.csv2pandas(&#34;https://raw.githubusercontent.com/CITF-Malaysia/citf-public/main/vaccination/vax_state.csv&#34;,rename_columns=rename_dict,separator=&#39;,&#39;)

                    list_moh = [moh1,moh2,moh3,moh4]
                    result = reduce(lambda left, right: left.merge(right, how = &#39;outer&#39;, on=[&#39;location&#39;,&#39;date&#39;]), list_moh)
                    columns_keeped = [&#39;tot_cases&#39;,&#39;hosp_covid&#39;,&#39;daily_partial&#39;,&#39;daily_full&#39;,&#39;icu_covid&#39;,&#39;beds_icu_covid&#39;]
                    self.return_structured_pandas(result, columns_keeped = columns_keeped)
                elif self.db == &#39;minciencia&#39;: # CHL
                    info(&#39;Chile Ministerio de Ciencia, Tecnología, Conocimiento, e Innovación database selected ...&#39;)
                    cast = {&#39;Codigo comuna&#39;: &#39;string&#39;}
                    rename_dict = {&#39;Codigo comuna&#39;:&#39;location&#39;,&#39;Poblacion&#39;:&#39;population&#39;,&#39;Fecha&#39;:&#39;date&#39;,&#39;Casos confirmados&#39;:&#39;cases&#39;}
                    ciencia = self.csv2pandas(&#34;https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto1/Covid-19_std.csv&#34;,cast=cast,rename_columns=rename_dict,separator=&#39;,&#39;)
                    columns_keeped = [&#39;cases&#39;]
                    self.return_structured_pandas(ciencia, columns_keeped = columns_keeped)
                elif self.db == &#39;covid19india&#39;: # IND
                    info(&#39;COVID19India database selected ...&#39;)

                    columns_keeped = [&#39;Deceased&#39;, &#39;Confirmed&#39;, &#39;Recovered&#39;, &#39;Tested&#39;,]
                    rename_dict = {i:&#39;tot_&#39;+i for i in columns_keeped}
                    columns_keeped = list(rename_dict.values())
                    rename_dict.update({&#39;Date&#39;: &#39;date&#39;, &#39;State&#39;: &#39;location&#39;})
                    drop_field  = {&#39;State&#39;: [&#39;India&#39;, &#39;State Unassigned&#39;]}
                    indi = self.csv2pandas(&#34;https://api.covid19india.org/csv/latest/states.csv&#34;,drop_field=drop_field,rename_columns=rename_dict,separator=&#39;,&#39;)
                     # Removing &#39;Other&#39; data, not identified
                    indi[&#39;location&#39;] = indi[&#39;location&#39;].apply(lambda x: x.replace(&#39;Andaman and Nicobar Islands&#39;,&#39;Andaman and Nicobar&#39;))
                    locationvariant = self.geo.get_subregion_list()[&#39;variation_name_subregion&#39;].to_list()
                    locationgeo = self.geo.get_subregion_list()[&#39;name_subregion&#39;].to_list()
                    def fusion(pan, new, old):
                        tmp = (pan.loc[pan.location.isin([new, old])].groupby(&#39;date&#39;).sum())
                        tmp[&#39;location&#39;] = old
                        tmp = tmp.reset_index()
                        cols = tmp.columns.tolist()
                        cols = cols[0:1] + cols[-1:] + cols[1:-1]
                        tmp = tmp[cols]
                        pan = pan.loc[~pan.location.isin([new, old])]
                        pan = pan.append(tmp)
                        return pan

                    indi=fusion(indi, &#39;Telangana&#39;, &#39;Andhra Pradesh&#39;)
                    indi=fusion(indi,&#39;Ladakh&#39;, &#39;Jammu and Kashmir&#39;)
                    # change name according to json one
                    oldnew = {}
                    for i in indi.location.unique():
                        for k,l in zip(locationgeo,locationvariant):
                            if l.find(i) == 0:
                                oldnew[i] = k
                    indi[&#39;location&#39;] = indi[&#39;location&#39;].map(oldnew)
                    self.return_structured_pandas(indi,columns_keeped = columns_keeped)
                elif self.db == &#39;covidtracking&#39;:
                    info(&#39;USA, CovidTracking.com database selected... ...&#39;)
                    rename_dict = {&#39;state&#39;: &#39;location&#39;,
                            &#39;death&#39;: &#39;tot_death&#39;,
                            &#39;hospitalizedCumulative&#39;: &#39;tot_hosp&#39;,
                            &#39;hospitalizedCurrently&#39;: &#39;cur_hosp&#39;,
                            &#39;inIcuCumulative&#39;: &#39;tot_icu&#39;,
                            &#39;inIcuCurrently&#39;: &#39;cur_icu&#39;,
                            &#39;negative&#39;: &#39;tot_neg_test&#39;,
                            &#39;positive&#39;: &#39;tot_pos_test&#39;,
                            &#39;onVentilatorCumulative&#39;: &#39;tot_onVentilator&#39;,
                            &#39;onVentilatorCurrently&#39;: &#39;cur_onVentilator&#39;,
                            &#39;totalTestResults&#39;:&#39;tot_test&#39;,
                            }
                    ctusa = self.csv2pandas(&#34;https://covidtracking.com/data/download/all-states-history.csv&#34;,
                        rename_columns = rename_dict, separator = &#39;,&#39;)
                    columns_keeped = list(rename_dict.values())
                    columns_keeped.remove(&#39;location&#39;) # is already expected
                    self.return_structured_pandas(ctusa, columns_keeped = columns_keeped)
                elif self.db == &#39;spf&#39; or self.db == &#39;spfnational&#39;:
                    if self.db == &#39;spfnational&#39;:
                        rename_dict = {
                        &#39;patients_reanimation&#39;:&#39;cur_reanimation&#39;,
                        &#39;patients_hospitalises&#39;:&#39;cur_hospitalises&#39;
                        }
                        columns_keeped = [&#39;total_deces_hopital&#39;,&#39;cur_reanimation&#39;,&#39;cur_hospitalises&#39;,
                        &#39;total_cas_confirmes&#39;,&#39;total_patients_gueris&#39;,
                        &#39;total_deces_ehpad&#39;,&#39;total_cas_confirmes_ehpad&#39;,&#39;total_cas_possibles_ehpad&#39;]

                        spfnat = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/d3a98a30-893f-47f7-96c5-2f4bcaaa0d71&#34;,
                        rename_columns = rename_dict, separator = &#39;,&#39;)
                        colcast=[i for i in columns_keeped]

                        spfnat[colcast]=pd.to_numeric(spfnat[colcast].stack(),errors = &#39;coerce&#39;).unstack()
                        self.return_structured_pandas(spfnat, columns_keeped=columns_keeped) # with &#39;tot_dc&#39; first
                    else:
                        info(&#39;SPF aka Sante Publique France database selected (France departement granularity) ...&#39;)
                        info(&#39;... Nine different databases from SPF will be parsed ...&#39;)
                        # https://www.data.gouv.fr/fr/datasets/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/
                        # Parse and convert spf data structure to JHU one for historical raison
                        # hosp Number of people currently hospitalized
                        # rea  Number of people currently in resuscitation or critical care
                        # rad      Total amount of patient that returned home
                        # dc       Total amout of deaths at the hospital
                        # &#39;sexe&#39; == 0 male + female
                        cast = {&#39;dep&#39;: &#39;string&#39;}
                        rename = {&#39;jour&#39;: &#39;date&#39;, &#39;dep&#39;: &#39;location&#39;}
                        cast.update({&#39;HospConv&#39;:&#39;string&#39;,&#39;SSR_USLD&#39;:&#39;string&#39;,&#39;autres&#39;:&#39;string&#39;})
                        constraints = {&#39;sexe&#39;: 0}
                        spf1 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/63352e38-d353-4b54-bfd1-f1b3ee1cabd7&#34;,
                                      rename_columns = rename, constraints = constraints, cast = cast)
                        # https://www.data.gouv.fr/fr/datasets/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/
                        # All data are incidence. → integrated later in the code
                        # incid_hosp    string  Nombre quotidien de personnes nouvellement hospitalisées
                        # incid_rea     integer Nombre quotidien de nouvelles admissions en réanimation
                        # incid_dc      integer Nombre quotidien de personnes nouvellement décédées
                        # incid_rad     integer Nombre quotidien de nouveaux retours à domicile
                        spf2 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/6fadff46-9efd-4c53-942a-54aca783c30c&#34;,
                                      rename_columns = rename, cast = cast)
                        # https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-resultats-des-tests-virologiques-covid-19/
                        # T       Number of tests performed daily → integrated later
                        # P       Number of positive tests daily → integrated later
                        constraints = {&#39;cl_age90&#39;: 0}
                        spf3 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/406c6a23-e283-4300-9484-54e78c8ae675&#34;,
                                      rename_columns = rename, constraints = constraints, cast = cast)
                        # https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-personnes-vaccinees-contre-la-covid-19-1
                        # Les données issues du système d’information Vaccin Covid permettent de dénombrer en temps quasi réel
                        # (J-1), le nombre de personnes ayant reçu une injection de vaccin anti-covid en tenant compte du nombre
                        # de doses reçues, de l’âge, du sexe ainsi que du niveau géographique (national, régional et
                        # départemental).
                        constraints = {&#39;vaccin&#39;: 0} # 0 means all vaccines
                        # previously : https://www.data.gouv.fr/fr/datasets/r/4f39ec91-80d7-4602-befb-4b522804c0af
                        spf5 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/535f8686-d75d-43d9-94b3-da8cdf850634&#34;,
                            rename_columns = rename, constraints = constraints, separator = &#39;;&#39;, encoding = &#34;ISO-8859-1&#34;, cast = cast)
                        #print(spf5)
                        # https://www.data.gouv.fr/fr/datasets/indicateurs-de-suivi-de-lepidemie-de-covid-19/#_
                        # tension hospitaliere
                        #&#39;date&#39;, &#39;location&#39;, &#39;region&#39;, &#39;libelle_reg&#39;, &#39;libelle_dep&#39;, &#39;tx_incid&#39;,
                        # &#39;R&#39;, &#39;taux_occupation_sae&#39;, &#39;tx_pos&#39;, &#39;tx_incid_couleur&#39;, &#39;R_couleur&#39;,
                        # &#39;taux_occupation_sae_couleur&#39;, &#39;tx_pos_couleur&#39;, &#39;nb_orange&#39;,
                        # &#39;nb_rouge&#39;]
                        # Vert : taux d’occupation compris entre 0 et 40% ;
                        # Orange : taux d’occupation compris entre 40 et 60% ;
                        # Rouge : taux d&#39;occupation supérieur à 60%.
                        # R0
                        # vert : R0 entre 0 et 1 ;
                        # Orange : R0 entre 1 et 1,5 ;
                        # Rouge : R0 supérieur à 1,5.
                        cast = {&#39;departement&#39;: &#39;string&#39;}
                        rename = {&#39;extract_date&#39;: &#39;date&#39;, &#39;departement&#39;: &#39;location&#39;}
                        #columns_skipped=[&#39;region&#39;,&#39;libelle_reg&#39;,&#39;libelle_dep&#39;,&#39;tx_incid_couleur&#39;,&#39;R_couleur&#39;,\
                        #&#39;taux_occupation_sae_couleur&#39;,&#39;tx_pos_couleur&#39;,&#39;nb_orange&#39;,&#39;nb_rouge&#39;]
                        spf4 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/4acad602-d8b1-4516-bc71-7d5574d5f33e&#34;,
                                    rename_columns = rename, separator=&#39;,&#39;, encoding = &#34;ISO-8859-1&#34;, cast=cast)

                        #https://www.data.gouv.fr/fr/datasets/donnees-de-laboratoires-pour-le-depistage-indicateurs-sur-les-variants/
                        #Prc_tests_PCR_TA_crible = % de tests PCR criblés parmi les PCR positives
                        #Prc_susp_501Y_V1 = % de tests avec suspicion de variant 20I/501Y.V1 (UK)
                        #Prc_susp_501Y_V2_3 = % de tests avec suspicion de variant 20H/501Y.V2 (ZA) ou 20J/501Y.V3 (BR)
                        #Prc_susp_IND = % de tests avec une détection de variant mais non identifiable
                        #Prc_susp_ABS = % de tests avec une absence de détection de variant
                        #Royaume-Uni (UK): code Nexstrain= 20I/501Y.V1
                        #Afrique du Sud (ZA) : code Nexstrain= 20H/501Y.V2
                        #Brésil (BR) : code Nexstrain= 20J/501Y.V3

                        cast = {&#39;dep&#39;: &#39;string&#39;}
                        rename = {&#39;dep&#39;: &#39;location&#39;}
                        constraints = {&#39;cl_age90&#39;: 0}
                        spf6 =  self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/16f4fd03-797f-4616-bca9-78ff212d06e8&#34;,
                                     constraints = constraints,rename_columns = rename, separator=&#39;;&#39;, cast=cast)

                        constraints = {&#39;age_18ans&#39;: 0}
                        spf7 =  self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/c0f59f00-3ab2-4f31-8a05-d317b43e9055&#34;,
                                    constraints = constraints, rename_columns = rename, separator=&#39;;&#39;, cast=cast)
                        #Mutation d&#39;intérêt :
                        #A = E484K
                        #B = E484Q
                        #C = L452R
                        spf8 = self.csv2pandas(&#34;https://www.data.gouv.fr/fr/datasets/r/4d3e5a8b-9649-4c41-86ec-5420eb6b530c&#34;,
                        rename_columns = rename, separator=&#39;;&#39;,cast=cast)
                        #spf8keeped = list(spf8.columns)[2:]
                        rename = {&#39;date_de_passage&#39;:&#39;date&#39;,&#39;dep&#39;:&#39;location&#39;}
                        spf9 = self.csv2pandas(&#34;https://www.data.gouv.fr/en/datasets/r/eceb9fb4-3ebc-4da3-828d-f5939712600a&#34;,
                        rename_columns = rename, separator=&#39;;&#39;,cast=cast)

                        list_spf=[spf1, spf2, spf3, spf4, spf5, spf6, spf7,spf8,spf9]

                        #for i in list_spf:
                        #    i[&#39;date&#39;] = pd.to_datetime(i[&#39;date&#39;]).apply(lambda x: x if not pd.isnull(x) else &#39;&#39;)
                        #    print(i.loc[i.date==d1])
                        #dfs = [df.set_index([&#39;date&#39;, &#39;location&#39;]) for df in list_spf]
                        result = reduce(lambda left, right: left.merge(right, how = &#39;outer&#39;, on=[&#39;location&#39;,&#39;date&#39;]), list_spf)
                        result = result.loc[~result[&#39;location&#39;].isin([&#39;00&#39;])]
                        result = result.sort_values(by=[&#39;location&#39;,&#39;date&#39;])
                        result.loc[result[&#39;location&#39;].isin([&#39;975&#39;,&#39;977&#39;,&#39;978&#39;,&#39;986&#39;,&#39;987&#39;]),&#39;location&#39;]=&#39;980&#39;
                        result = result.drop_duplicates(subset=[&#39;location&#39;, &#39;date&#39;], keep=&#39;last&#39;)

                        for w in [&#39;incid_hosp&#39;, &#39;incid_rea&#39;, &#39;incid_rad&#39;, &#39;incid_dc&#39;, &#39;P&#39;, &#39;T&#39;, &#39;n_cum_dose1&#39;, &#39;n_cum_dose2&#39;,&#39;n_cum_dose3&#39;,&#39;n_cum_dose4&#39;,&#39;n_cum_rappel&#39;]:
                            result[w]=pd.to_numeric(result[w], errors = &#39;coerce&#39;)
                            if w.startswith(&#39;incid_&#39;):
                                ww = w[6:]
                                result[ww] = result.groupby(&#39;location&#39;)[ww].fillna(method = &#39;bfill&#39;)
                                result[&#39;incid_&#39;+ww] = result.groupby(&#39;location&#39;)[&#39;incid_&#39;+ww].fillna(method = &#39;bfill&#39;)
                                #result[&#39;offset_&#39;+w] = result.loc[result.date==min_date][ww]-result.loc[result.date==min_date][&#39;incid_&#39;+ww]
                                #result[&#39;offset_&#39;+w] = result.groupby(&#39;location&#39;)[&#39;offset_&#39;+w].fillna(method=&#39;ffill&#39;)
                            else:
                                pass
                                #result[&#39;offset_&#39;+w] = 0
                            if w not in [&#39;n_cum&#39;,&#39;incid_hosp&#39;, &#39;incid_rea&#39;, &#39;incid_rad&#39;, &#39;incid_dc&#39;]:
                                result[&#39;tot_&#39;+w]=result.groupby([&#39;location&#39;])[w].cumsum()#+result[&#39;offset_&#39;+w]

                        def dontneeeded():
                            for col in result.columns:
                                if col.startswith(&#39;Prc&#39;):
                                    result[col] /= 100.
                            for col in result.columns:
                                if col.startswith(&#39;ti&#39;):
                                    result[col] /= 7. #par
                            for col in result.columns:
                                if col.startswith(&#39;tp&#39;):
                                    result[col] /= 7. #par

                        rename_dict={
                            &#39;dc&#39;: &#39;tot_dc&#39;,
                            &#39;hosp&#39;: &#39;cur_hosp&#39;,
                            &#39;rad&#39;: &#39;tot_rad&#39;,
                            &#39;rea&#39;: &#39;cur_rea&#39;,
                            &#39;n_cum_dose1&#39;: &#39;tot_vacc1&#39;,
                            &#39;n_cum_dose2&#39;: &#39;tot_vacc2&#39;,
                            &#39;n_cum_dose3&#39;: &#39;tot_vacc3&#39;,
                            &#39;n_cum_dose4&#39;: &#39;tot_vacc4&#39;,
                            &#39;n_cum_rappel&#39;:&#39;tot_rappel_vacc&#39;,
                            &#39;tx_incid&#39;: &#39;cur_idx_tx_incid&#39;,
                            &#39;R&#39;: &#39;cur_idx_R&#39;,
                            &#39;taux_occupation_sae&#39;: &#39;cur_idx_taux_occupation_sae&#39;,
                            &#39;tx_pos&#39;: &#39;cur_taux_pos&#39;,
                            &#39;Prc_tests_PCR_TA_crible&#39;:&#39;cur_idx_Prc_tests_PCR_TA_crible&#39;,
                            &#39;Prc_susp_501Y_V1&#39;:&#39;cur_idx_Prc_susp_501Y_V1&#39;,
                            &#39;Prc_susp_501Y_V2_3&#39;:&#39;cur_idx_Prc_susp_501Y_V2_3&#39;,
                            &#39;Prc_susp_IND&#39;:&#39;cur_idx_Prc_susp_IND&#39;,
                            &#39;Prc_susp_ABS&#39;:&#39;cur_idx_Prc_susp_ABS&#39;,
                            &#39;ti&#39;:&#39;cur_idx_ti&#39;,
                            &#39;tp&#39;:&#39;cur_idx_tp&#39;,
                            &#39;tx_crib&#39; : &#39;cur_taux_crib&#39;,
                            &#39;tx_A1&#39;:&#39;cur_idx_tx_A1&#39;,
                            &#39;tx_B1&#39;:&#39;cur_idx_tx_B1&#39;,
                            &#39;tx_C1&#39;:&#39;cur_idx_tx_C1&#39;,
                            &#39;nbre_pass_corona&#39;:&#39;cur_nbre_pass_corona&#39;,
                            }
                        spf8keeped = [&#39;nb_A0&#39;,&#39;nb_A1&#39;, &#39;nb_B0&#39;, &#39;nb_B1&#39;, &#39;nb_C0&#39;, &#39;nb_C1&#39;]
                        rename_dict.update({i:&#39;cur_&#39;+i for i in spf8keeped})
                        result = result.rename(columns=rename_dict)
                        #coltocast=list(rename_dict.values())[:5]
                        #result[coltocast] = result[coltocast].astype(&#39;Int64&#39;)
                        rename_dict2={i:i.replace(&#39;incid_&#39;,&#39;tot_incid_&#39;) for i in [&#39;incid_hosp&#39;, &#39;incid_rea&#39;, &#39;incid_rad&#39;, &#39;incid_dc&#39;]}
                        result = result.rename(columns=rename_dict2)
                        columns_keeped  = list(rename_dict.values()) + list(rename_dict2.values()) + [&#39;tot_P&#39;, &#39;tot_T&#39;]
                        self.return_structured_pandas(result,columns_keeped=columns_keeped) # with &#39;tot_dc&#39; first
                elif self.db == &#39;opencovid19&#39; or  self.db == &#39;opencovid19national&#39;:
                    rename={&#39;maille_code&#39;:&#39;location&#39;}
                    cast={&#39;source_url&#39;:str,&#39;source_archive&#39;:str,&#39;source_type&#39;:str,&#39;nouvelles_hospitalisations&#39;:str,&#39;nouvelles_reanimations&#39;:str}
                    if self.db == &#39;opencovid19&#39;:
                        info(&#39;OPENCOVID19 (country granularity) selected ...&#39;)
                        drop_field  = {&#39;granularite&#39;:[&#39;pays&#39;,&#39;monde&#39;,&#39;region&#39;]}
                        dict_columns_keeped = {
                            &#39;deces&#39;:&#39;tot_deces&#39;,
                            &#39;cas_confirmes&#39;:&#39;tot_cas_confirmes&#39;,
                            &#39;reanimation&#39;:&#39;cur_reanimation&#39;,
                            &#39;hospitalises&#39;:&#39;cur_hospitalises&#39;,
                            &#39;gueris&#39;:&#39;tot_gueris&#39;
                            }
                    else:
                        info(&#39;OPENCOVID19 (national granularity) selected ...&#39;)
                        drop_field  = {&#39;granularite&#39;:[&#39;monde&#39;,&#39;region&#39;,&#39;departement&#39;]}
                        dict_columns_keeped = {
                        &#39;deces&#39;:&#39;tot_deces&#39;,
                        &#39;cas_confirmes&#39;:&#39;tot_cas_confirmes&#39;,
                        &#39;cas_ehpad&#39;:&#39;tot_cas_ehpad&#39;,
                        &#39;cas_confirmes_ehpad&#39;:&#39;tot_cas_confirmes_ehpad&#39;,
                        &#39;cas_possibles_ehpad&#39;:&#39;tot_cas_possibles_ehpad&#39;,
                        &#39;deces_ehpad&#39;:&#39;tot_deces_ehpad&#39;,
                        &#39;reanimation&#39;:&#39;cur_reanimation&#39;,
                        &#39;hospitalises&#39;:&#39;cur_hospitalises&#39;,
                        &#39;gueris&#39;:&#39;tot_gueris&#39;
                        }
                    opencovid19 = self.csv2pandas(&#39;https://raw.githubusercontent.com/opencovid19-fr/data/master/dist/chiffres-cles.csv&#39;,
                                drop_field=drop_field,rename_columns=rename,separator=&#39;,&#39;,cast=cast)

                    opencovid19[&#39;location&#39;] = opencovid19[&#39;location&#39;].apply(lambda x: x.replace(&#39;COM-&#39;,&#39;&#39;).replace(&#39;DEP-&#39;,&#39;&#39;).replace(&#39;FRA&#39;,&#39;France&#39;))
                    # integrating needed fields
                    if self.db == &#39;opencovid19national&#39;:
                        opencovid19 = opencovid19.loc[~opencovid19.granularite.isin([&#39;collectivite-outremer&#39;])]

                    column_to_integrate=[&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;]
                    opencovid19[column_to_integrate]=pd.to_numeric(opencovid19[column_to_integrate].stack(),errors = &#39;coerce&#39;).unstack()

                    for w in [&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;]:
                        opencovid19[&#39;tot_&#39;+w]=opencovid19.groupby([&#39;location&#39;])[w].cumsum()
                    #columns_skipped = [&#39;granularite&#39;,&#39;maille_nom&#39;,&#39;source_nom&#39;,&#39;source_url&#39;,&#39;source_archive&#39;,&#39;source_type&#39;]
                    self.return_structured_pandas(opencovid19.rename(columns=dict_columns_keeped),columns_keeped=list(dict_columns_keeped.values())+[&#39;tot_&#39;+c for c in column_to_integrate])
                elif self.db == &#39;owid&#39;:
                    variant = True
                    info(&#39;OWID aka \&#34;Our World in Data\&#34; database selected ...&#39;)
                    drop_field = {&#39;location&#39;:[&#39;International&#39;]}#, &#39;World&#39;]}
                    owid = self.csv2pandas(&#34;https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv&#34;,
                    separator=&#39;,&#39;,drop_field=drop_field)
                    # renaming some columns
                    col_to_rename1=[&#39;reproduction_rate&#39;,&#39;icu_patients&#39;,&#39;hosp_patients&#39;,&#39;weekly_hosp_admissions&#39;,&#39;positive_rate&#39;]
                    renamed_cols1=[&#39;cur_&#39;+c if c != &#39;positive_rate&#39; else &#39;cur_idx_&#39;+c for c in col_to_rename1]
                    col_to_rename2=[&#39;people_vaccinated&#39;,&#39;people_fully_vaccinated&#39;,&#39;people_fully_vaccinated_per_hundred&#39;,\
                    &#39;people_vaccinated_per_hundred&#39;,&#39;population&#39;,&#39;gdp_per_capita&#39;]
                    renamed_cols2=[&#39;total_&#39;+i for i in col_to_rename2]
                    col_to_rename = col_to_rename1+col_to_rename2
                    renamed_cols = renamed_cols1 +renamed_cols2
                    columns_keeped=[&#39;iso_code&#39;,&#39;total_deaths&#39;,&#39;total_cases&#39;,&#39;total_vaccinations&#39;,&#39;total_tests&#39;]
                    columns_keeped+=[&#39;total_cases_per_million&#39;,&#39;total_deaths_per_million&#39;,&#39;total_vaccinations_per_hundred&#39;,&#39;total_boosters&#39;]
                    #owid[&#39;total_tests_with_new_tests&#39;] = owid.groupby([&#39;location&#39;])[&#39;new_tests&#39;].cumsum()
                    uniq=list(owid.location.unique())
                    mask = (owid.loc[owid.location.isin(uniq)][&#39;total_tests&#39;].isnull() &amp;\
                                                owid.loc[owid.location.isin(uniq)][&#39;new_tests&#39;].isnull())
                    #sometimes is new_tests sometimes total_tests
                    owid_test         = owid[~mask]
                    owid_new_test     = owid_test[owid_test[&#39;total_tests&#39;].isnull()]
                    owid_total_test   = owid_test[~owid_test[&#39;total_tests&#39;].isnull()]
                    owid_new_test     = owid_new_test.drop(columns=&#39;total_tests&#39;)

                    owid_new_test.loc[:,&#39;total_tests&#39;] = owid_new_test.groupby([&#39;location&#39;])[&#39;new_tests&#39;].cumsum()
                    owid = pd.concat([owid[mask],owid_new_test,owid_total_test])
                    self.return_structured_pandas(owid.rename(columns=dict(zip(col_to_rename,renamed_cols))),columns_keeped=columns_keeped+renamed_cols)
                elif self.db == &#39;risklayer&#39;:
                    info(&#39;EUR, Who Europe from RiskLayer ...&#39;)
                    rename_dict = {&#39;UID&#39;: &#39;location&#39;,
                        &#39;CumulativePositive&#39;: &#39;tot_positive&#39;,
                        &#39;IncidenceCumulative&#39;: &#39;tot_incidence&#39;,
                        &#39;DateRpt&#39;:&#39;date&#39;}
                    deur = self.csv2pandas(&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vQ-JLawOH35vPyOk39w0tjn64YQLlahiD2AaNfjd82pgQ37Jr1K8KMHOqJbxoi4k2FZVYBGbZ-nsxhi/pub?output=csv&#34;,
                        rename_columns = rename_dict, separator = &#39;,&#39;)
                    columns_keeped = list(rename_dict.values())
                    columns_keeped.remove(&#39;location&#39;) # is already expected
                    columns_keeped.remove(&#39;date&#39;) # is already expected
                    self.return_structured_pandas(deur, columns_keeped = columns_keeped)
                elif self.db == &#39;europa&#39;:
                    info(&#39;EUR, Rationale for the JRC COVID-19 website - data monitoring and national measures ...&#39;)
                    rename_dict = {&#39;Region&#39;: &#39;location&#39;,
                        &#39;CumulativeDeceased&#39;:&#39;tot_deaths&#39;,
                        &#39;Hospitalized&#39;:&#39;cur_hosp&#39;,
                        &#39;IntensiveCare&#39;:&#39;cur_icu&#39;,
                        &#39;Date&#39;:&#39;date&#39;}
                    euro = self.csv2pandas(&#34;https://raw.githubusercontent.com/ec-jrc/COVID-19/master/data-by-region/jrc-covid-19-all-days-by-regions.csv&#34;,
                    rename_columns = rename_dict, separator = &#39;,&#39;)
                    euro=euro.loc[euro.EUcountry==True]
                    todrop=[&#39;Ciudad Autónoma de Melilla&#39;,&#39;Gorenjske&#39;,&#39;Goriške&#39;,&#39;Greenland&#39;,&#39;Itä-Savo&#39;,&#39;Jugovzhodne&#39;,&#39;Koroške&#39;,&#39;Länsi-Pohja&#39;,\
                            &#39;Mainland&#39;,&#39;NOT SPECIFIED&#39;,&#39;Obalno-kraške&#39;,&#39;Osrednjeslovenske&#39;,&#39;Podravske&#39;,&#39;Pomurske&#39;,&#39;Posavske&#39;,&#39;Primorsko-notranjske&#39;,\
                            &#39;Repatriierte&#39;,&#39;Savinjske&#39;,&#39;West North&#39;,&#39;Zasavske&#39;]
                    euro=euro.loc[~euro[&#39;location&#39;].isin(todrop)]
                    euro=euro.dropna(subset=[&#39;location&#39;])

                    euro[&#39;tot_positive&#39;]=euro.groupby(&#39;location&#39;)[&#39;CurrentlyPositive&#39;].cumsum()
                    columns_keeped = list(rename_dict.values())+[&#39;tot_positive&#39;]
                    columns_keeped.remove(&#39;location&#39;) # is already expected
                    columns_keeped.remove(&#39;date&#39;) # is already expected
                    self.return_structured_pandas(euro, columns_keeped = columns_keeped)
                elif self.db == &#39;insee&#39;:
                    since_year=2018 # Define the first year for stats
                    info(&#39;FRA, INSEE global deaths statistics...&#39;)
                    url = &#34;https://www.data.gouv.fr/fr/datasets/fichier-des-personnes-decedees/&#34;
                    with open(get_local_from_url(url,86400*7)) as fp: # update each week
                        soup = BeautifulSoup(fp,features=&#34;lxml&#34;)
                    ld_json=soup.find(&#39;script&#39;, {&#39;type&#39;:&#39;application/ld+json&#39;}).contents
                    data=json.loads(ld_json[0])
                    deces_url={}
                    for d in data[&#39;distribution&#39;]:
                        deces_url.update({d[&#39;name&#39;]:d[&#39;url&#39;]})
                    dc={}

                    current_year=datetime.date.today().year
                    current_month=datetime.date.today().month

                    # manage year between since_year-1 and current_year(excluded)
                    for y in range(since_year-1,current_year):
                        i=str(y) #  in string
                        filename=&#39;deces-&#39;+i+&#39;.txt&#39;
                        if filename not in list(deces_url.keys()):
                            continue
                        with open(get_local_from_url(deces_url[filename],86400*30)) as f:
                            dc.update({i:f.readlines()})

                    # manage months for the current_year
                    for m in range(current_month):
                        i=str(m+1).zfill(2) #  in string with leading 0
                        filename=&#39;deces-&#39;+str(current_year)+&#39;-m&#39;+i+&#39;.txt&#39;
                        if filename not in list(deces_url.keys()):
                            continue
                        with open(get_local_from_url(deces_url[filename],86400)) as f:
                            dc.update({i:f.readlines()})

                    def string_to_date(s):
                        date=None
                        y=int(s[0:4])
                        m=int(s[4:6])
                        d=int(s[6:8])
                        if m==0:
                            m=1
                        if d==0:
                            d=1
                        if y==0:
                            raise ValueError
                        try:
                            date=datetime.date(y,m,d)
                        except:
                            if m==2 and d==29:
                                d=28
                                date=datetime.date(y,m,d)
                                raise ValueError
                        return date

                    pdict={}
                    insee_pd=pd.DataFrame()
                    for i in list(dc.keys()):
                        data=[]

                        for l in dc[i]:
                            [last_name,first_name]=(l[0:80].split(&#34;/&#34;)[0]).split(&#34;*&#34;)
                            sex=int(l[80])
                            birthlocationcode=l[89:94]
                            birthlocationname=l[94:124].rstrip()
                            try:
                                birthdate=string_to_date(l[81:89])
                                deathdate=string_to_date(l[154:].strip()[0:8]) # sometimes, heading space
                                lbis=list(l[154:].strip()[0:8])
                                lbis[0:4]=list(&#39;2003&#39;)
                                lbis=&#39;&#39;.join(lbis)
                                deathdatebis=string_to_date(lbis)
                            except ValueError:
                                if lbis!=&#39;20030229&#39;:
                                    verb(&#39;Problem in a date parsing insee data for : &#39;,l,lbis)
                            deathlocationcode=l[162:167]
                            deathlocationshortcode=l[162:164]
                            deathid=l[167:176]
                            data.append([first_name,last_name,sex,birthdate,birthlocationcode,birthlocationname,deathdate,deathlocationcode,deathlocationshortcode,deathid,deathdatebis,1])
                        p=pd.DataFrame(data)
                        p.columns=[&#39;first_name&#39;,&#39;last_name&#39;,&#39;sex&#39;,&#39;birth_date&#39;,&#39;birth_location_code&#39;,&#39;birth_location_name&#39;,&#39;death_date&#39;,&#39;death_location_code&#39;,&#39;location&#39;,&#39;death_id&#39;,&#39;death_date_bis&#39;,&#39;i&#39;]
                        #p[&#34;age&#34;]=[k.days/365 for k in p[&#34;death_date&#34;]-p[&#34;birth_date&#34;]]
                        #p[&#34;age_class&#34;]=[math.floor(k/20) for k in p[&#34;age&#34;]]

                        #p=p[[&#39;location&#39;,&#39;death_date&#39;]].reset_index(drop=True)
                        #p[&#39;death_date&#39;]=pd.to_datetime(p[&#39;death_date&#39;]).dt.date
                        #p[&#39;location&#39;]=p[&#39;location&#39;].astype(str)
                        insee_pd=insee_pd.append(p)
                        #pdict.update({i:p})
                    insee_pd = insee_pd[[&#39;location&#39;,&#39;death_date&#39;]].reset_index(drop=True)
                    insee_pd = insee_pd.rename(columns={&#39;death_date&#39;:&#39;date&#39;})
                    insee_pd[&#39;date&#39;]=pd.to_datetime(insee_pd[&#39;date&#39;]).dt.date
                    insee_pd[&#39;location&#39;]=insee_pd[&#39;location&#39;].astype(str)
                    insee_pd = insee_pd.groupby([&#39;date&#39;,&#39;location&#39;]).size().reset_index(name=&#39;daily_number_of_deaths&#39;)

                    since_date=str(since_year)+&#39;-01-01&#39;
                    insee_pd = insee_pd[insee_pd.date&gt;=datetime.date.fromisoformat(since_date)].reset_index(drop=True)
                    insee_pd[&#39;tot_deaths_since_&#39;+since_date]=insee_pd.groupby(&#39;location&#39;)[&#39;daily_number_of_deaths&#39;].cumsum()
                    self.return_structured_pandas(insee_pd,columns_keeped=[&#39;tot_deaths_since_&#39;+since_date])
            except:
                raise CoaDbError(&#34;An error occured while parsing data of &#34;+self.get_db()+&#34;. This may be due to a data format modification. &#34;
                    &#34;You may contact support@pycoa.fr. Thanks.&#34;)
            # some info
            info(&#39;Few information concernant the selected database : &#39;, self.get_db())
            info(&#39;Available key-words, which ∈&#39;,self.get_available_keys_words())
            info(&#39;Example of location : &#39;,  &#39;, &#39;.join(random.choices(self.get_locations(), k=min(5,len(self.get_locations() ))   )), &#39; ...&#39;)
            info(&#39;Last date data &#39;, self.get_dates().max())

   @staticmethod
   def factory(db_name):
       &#39;&#39;&#39;
        Return an instance to DataBase and to CocoDisplay methods
        This is recommended to avoid mismatch in labeled figures
       &#39;&#39;&#39;
       datab = DataBase(db_name)
       return  datab, datab.get_display()

   def set_display(self,db,geo):
       &#39;&#39;&#39; Set the CocoDisplay &#39;&#39;&#39;
       self.codisp = codisplay.CocoDisplay(db, geo)

   def get_display(self):
       &#39;&#39;&#39; Return the instance of CocoDisplay initialized by factory&#39;&#39;&#39;
       return self.codisp

   def get_db(self):
        &#39;&#39;&#39;
        Return the current covid19 database selected. See get_available_database() for full list
        &#39;&#39;&#39;
        return self.db

   def get_available_database(self):
        &#39;&#39;&#39;
        Return all the available Covid19 database
        &#39;&#39;&#39;
        return self.database_name

   def get_available_options(self):
        &#39;&#39;&#39;
        Return available options for the get_stats method
        &#39;&#39;&#39;
        o=self.available_options
        return o

   def get_available_keys_words(self):
        &#39;&#39;&#39;
        Return all the available keyswords for the database selected
        Key-words are for:
        - jhu : [&#39;deaths&#39;,&#39;confirmed&#39;,&#39;recovered&#39;]
                            * the data are cumulative i.e for a date it represents the total cases
            For more information please have a look to https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data
        - &#39;owid&#39; : [&#39;total_deaths&#39;,&#39;total_cases&#39;,&#39;reproduction_rate&#39;,&#39;icu_patients&#39;,&#39;hosp_patients&#39;,&#39;total_tests&#39;,
                    &#39;positive_rate&#39;,&#39;total_vaccinations&#39;]
        For more information please have a look to https://github.com/owid/covid-19-data/tree/master/public/data/
        - &#39;spf&#39; : [&#39;hosp&#39;, &#39;rea&#39;, &#39;rad&#39;, &#39;dc&#39;, &#39;incid_hosp&#39;, &#39;incid_rea&#39;, &#39;incid_dc&#39;,
                    &#39;incid_rad&#39;, &#39;P&#39;, &#39;T&#39;, &#39;tx_incid&#39;, &#39;R&#39;, &#39;taux_occupation_sae&#39;, &#39;tx_pos&#39;]
            No translation have been done for french keywords data
        For more information please have a look to  https://www.data.gouv.fr/fr/organizations/sante-publique-france/
        - &#39;opencovid19&#39; :[&#39;cas_confirmes&#39;, &#39;deces&#39;,
        &#39;reanimation&#39;, &#39;hospitalises&#39;,&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;, &#39;gueris&#39;, &#39;depistes&#39;]
        - &#39;opencovid19national&#39; :[&#39;cas_confirmes&#39;, &#39;cas_ehpad&#39;, &#39;cas_confirmes_ehpad&#39;, &#39;cas_possibles_ehpad&#39;, &#39;deces&#39;, &#39;deces_ehpad&#39;,
        &#39;reanimation&#39;, &#39;hospitalises&#39;,&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;, &#39;gueris&#39;, &#39;depistes&#39;]

        No translation have been done for french keywords data
        For more information please have a look to https://github.com/opencovid19-fr
        &#39;&#39;&#39;
        return self.available_keys_words

   def get_keyword_definition(self,keys):
       &#39;&#39;&#39;
            Return definition on the selected keword
       &#39;&#39;&#39;
       value = self.databaseinfo.generic_info(self.get_db(),keys)[0]
       return value

   def get_keyword_url(self,keys):
       &#39;&#39;&#39;
        Return url where the keyword have been parsed
       &#39;&#39;&#39;
       value = self.databaseinfo.generic_info(self.get_db(),keys)[1]
       master  = self.databaseinfo.generic_info(self.get_db(),keys)[2]
       return value, master


   def return_jhu_pandas(self):
        &#39;&#39;&#39; For center for Systems Science and Engineering (CSSE) at Johns Hopkins University
            COVID-19 Data Repository by the see homepage: https://github.com/CSSEGISandData/COVID-19
            return a structure : pandas location - date - keywords
            for jhu location are countries (location uses geo standard)
            for jhu-usa location are Province_State (location uses geo standard)
            &#39;&#39;&#39;
        base_url = &#34;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/&#34;+\
                                &#34;csse_covid_19_data/csse_covid_19_time_series/&#34;
        base_name = &#34;time_series_covid19_&#34;
        # previous are default for actual jhu db

        pandas_jhu = {}

        if self.db == &#39;jhu&#39;: # worldwide
            extension =  &#34;_global.csv&#34;
            jhu_files_ext = [&#39;deaths&#39;, &#39;confirmed&#39;]
        elif self.db == &#39;jhu-usa&#39;: # &#39;USA&#39;
            extension = &#34;_US.csv&#34;
            jhu_files_ext = [&#39;deaths&#39;,&#39;confirmed&#39;]
        elif self.db == &#39;rki&#39;: # &#39;DEU&#39;
            base_url = &#39;https://github.com/jgehrcke/covid-19-germany-gae/raw/master/&#39;
            jhu_files_ext = [&#39;deaths&#39;,&#39;cases&#39;]
            extension = &#39;-rki-by-ags.csv&#39;
            base_name = &#39;&#39;
        elif self.db == &#39;imed&#39;: # &#39;GRC&#39;
            base_url = &#39;https://raw.githubusercontent.com/iMEdD-Lab/open-data/master/COVID-19/greece_&#39;
            jhu_files_ext = [&#39;deaths&#39;,&#39;cases&#39;]
            extension = &#39;_v2.csv&#39;
            base_name = &#39;&#39;
        else:
            raise CoaDbError(&#39;Unknown JHU like db &#39;+str(self.db))

        self.available_keys_words = []
        if self.db == &#39;rki&#39;:
                self.available_keys_words = [&#39;tot_deaths&#39;,&#39;tot_cases&#39;]
        pandas_list = []
        for ext in jhu_files_ext:
            fileName = base_name + ext + extension
            url = base_url + fileName
            self.database_url.append(url)
            pandas_jhu_db = pandas.read_csv(get_local_from_url(url,7200), sep = &#39;,&#39;) # cached for 2 hours
            if self.db == &#39;jhu&#39;:
                pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;Country/Region&#39;:&#39;location&#39;})
                pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;Province/State&#39;,&#39;Lat&#39;,&#39;Long&#39;])
                pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
                pandas_jhu_db = pandas_jhu_db.loc[~pandas_jhu_db.location.isin([&#39;Diamond Princess&#39;])]
            elif self.db == &#39;jhu-usa&#39;:
                pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;Province_State&#39;:&#39;location&#39;})
                pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;UID&#39;,&#39;iso2&#39;,&#39;iso3&#39;,&#39;code3&#39;,&#39;FIPS&#39;,
                                    &#39;Admin2&#39;,&#39;Country_Region&#39;,&#39;Lat&#39;,&#39;Long_&#39;,&#39;Combined_Key&#39;])
                if &#39;Population&#39; in pandas_jhu_db.columns:
                    pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;,&#39;Population&#39;],var_name=&#34;date&#34;,value_name=ext)
                else:
                    pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
                removethose=[&#39;American Samoa&#39;,&#39;Diamond Princess&#39;,&#39;Grand Princess&#39;,&#39;Guam&#39;,
                &#39;Northern Mariana Islands&#39;,&#39;Puerto Rico&#39;,&#39;Virgin Islands&#39;]
                pandas_jhu_db = pandas_jhu_db.loc[~pandas_jhu_db.location.isin(removethose)]
            elif self.db == &#39;rki&#39;:
                pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;sum_&#39;+ext])
                pandas_jhu_db = pandas_jhu_db.set_index(&#39;time_iso8601&#39;).T.reset_index().rename(columns={&#39;index&#39;:&#39;location&#39;})
                pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
                pandas_jhu_db[&#39;location&#39;] = pandas_jhu_db.location.astype(str)
                pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;deaths&#39;:&#39;tot_deaths&#39;,&#39;cases&#39;:&#39;tot_cases&#39;})
            elif self.db == &#39;imed&#39;:
                pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;county_normalized&#39;:&#39;location&#39;})
                pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;Γεωγραφικό Διαμέρισμα&#39;,&#39;Περιφέρεια&#39;,&#39;county&#39;,&#39;pop_11&#39;])
                ext=&#39;tot_&#39;+ext
                pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
                self.available_keys_words += [ext]
            else:
                raise CoaTypeError(&#39;jhu nor jhu-usa database selected ... &#39;)

            pandas_jhu_db=pandas_jhu_db.groupby([&#39;location&#39;,&#39;date&#39;]).sum().reset_index()
            pandas_list.append(pandas_jhu_db)

        if &#39;jhu&#39; in self.db:
            pandas_list = [pan.rename(columns={i:&#39;tot_&#39;+i for i in jhu_files_ext}) for pan in pandas_list]
            self.available_keys_words = [&#39;tot_&#39;+i for i in jhu_files_ext]
        uniqloc = list(pandas_list[0][&#39;location&#39;].unique())
        oldloc = uniqloc
        codedico={}
        toremove = None
        newloc = None
        location_is_code = False
        if self.db_world:
            d_loc_s = collections.OrderedDict(zip(uniqloc,self.geo.to_standard(uniqloc,output=&#39;list&#39;,db=self.get_db(),interpret_region=True)))
            self.slocation = list(d_loc_s.values())
            g=coge.GeoManager(&#39;iso3&#39;)
            codename = collections.OrderedDict(zip(self.slocation,g.to_standard(self.slocation,output=&#39;list&#39;,db=self.get_db(),interpret_region=True)))
        else:
            if self.database_type[self.db][1] == &#39;subregion&#39;:
                pdcodename = self.geo.get_subregion_list()
                self.slocation = uniqloc
                codename = collections.OrderedDict(zip(self.slocation,list(pdcodename.loc[pdcodename.code_subregion.isin(self.slocation)][&#39;name_subregion&#39;])))
                if self.db == &#39;jhu-usa&#39;:
                    d_loc_s = collections.OrderedDict(zip(uniqloc,list(pdcodename.loc[pdcodename.name_subregion.isin(uniqloc)][&#39;code_subregion&#39;])))
                    self.slocation = list(d_loc_s.keys())
                    codename = d_loc_s
                if self.db == &#39;rki&#39;:
                    d_loc_s = collections.OrderedDict(zip(uniqloc,list(pdcodename.loc[pdcodename.code_subregion.isin(uniqloc)][&#39;name_subregion&#39;])))
                    self.slocation = list(d_loc_s.values())
                    codename = d_loc_s
                    location_is_code = True
                    def notuse():
                        count_values=collections.Counter(d_loc_s.values())
                        duplicates_location = list({k:v for k,v in count_values.items() if v&gt;1}.keys())
                        def findkeywithvalue(dico,what):
                            a=[]
                            for k,v in dico.items():
                                if v == what:
                                    a.append(k)
                            return a
                        codedupli={i:findkeywithvalue(d_loc_s,i) for i in duplicates_location}
            elif self.database_type[self.db][1] == &#39;region&#39;:
                codename = self.geo.get_data().set_index(&#39;name_region&#39;)[&#39;code_region&#39;].to_dict()
                self.slocation = list(codename.keys())


        result = reduce(lambda x, y: pd.merge(x, y, on = [&#39;location&#39;,&#39;date&#39;]), pandas_list)

        if location_is_code:
            result[&#39;codelocation&#39;] = result[&#39;location&#39;]
            result[&#39;location&#39;] = result[&#39;location&#39;].map(codename)
        else:
            if self.db == &#39;jhu&#39;:
                result[&#39;location&#39;] = result[&#39;location&#39;].map(d_loc_s)
            result[&#39;codelocation&#39;] = result[&#39;location&#39;].map(codename)
        result = result.loc[result.location.isin(self.slocation)]

        tmp = pd.DataFrame()
        if &#39;Kosovo&#39; in uniqloc:
            #Kosovo is Serbia ! with geo.to_standard
            tmp=(result.loc[result.location.isin([&#39;Serbia&#39;])]).groupby(&#39;date&#39;).sum().reset_index()
            tmp[&#39;location&#39;] = &#39;Serbia&#39;
            tmp[&#39;codelocation&#39;] = &#39;SRB&#39;
            kw = [i for i in self.available_keys_words]
            colpos=[&#39;location&#39;, &#39;date&#39;] + kw + [&#39;codelocation&#39;]
            tmp = tmp[colpos]
            result = result.loc[~result.location.isin([&#39;Serbia&#39;])]
            result = result.append(tmp)

        result[&#39;date&#39;] = pd.to_datetime(result[&#39;date&#39;],errors=&#39;coerce&#39;).dt.date
        result = result.sort_values(by=[&#39;location&#39;,&#39;date&#39;])
        result = result.reset_index(drop=True)
        self.mainpandas = fill_missing_dates(result)
        self.dates  = self.mainpandas[&#39;date&#39;]

   def csv2pandas(self,url,**kwargs):
        &#39;&#39;&#39;
        Parse and convert the database cvs file to a pandas structure
        &#39;&#39;&#39;
        self.database_url.append(url)
        kwargs_test(kwargs,[&#39;cast&#39;,&#39;separator&#39;,&#39;encoding&#39;,&#39;constraints&#39;,&#39;rename_columns&#39;,&#39;drop_field&#39;,&#39;quotechar&#39;],
            &#39;Bad args used in the csv2pandas() function.&#39;)

        cast = kwargs.get(&#39;cast&#39;, None)
        dico_cast = {}
        if cast:
            for key,val in cast.items():
                dico_cast[key] = val
        separator = kwargs.get(&#39;separator&#39;, &#39;;&#39;)
        if separator:
            separator = separator
        encoding = kwargs.get(&#39;encoding&#39;, None)
        if encoding:
            encoding = encoding
        quoting=0
        if self.db == &#39;obepine&#39;:
              quoting=3
        pandas_db = pandas.read_csv(get_local_from_url(url,7200),sep=separator,dtype=dico_cast, encoding = encoding,
            keep_default_na=False,na_values=&#39;&#39;,header=0,quoting=quoting) # cached for 2 hours

        #pandas_db = pandas.read_csv(self.database_url,sep=separator,dtype=dico_cast, encoding = encoding )
        constraints = kwargs.get(&#39;constraints&#39;, None)
        rename_columns = kwargs.get(&#39;rename_columns&#39;, None)
        drop_field = kwargs.get(&#39;drop_field&#39;, None)
        if self.db == &#39;obepine&#39;:
            pandas_db = pandas_db.rename(columns=rename_columns)
            pandas_db = pandas_db.applymap(lambda x: x.replace(&#39;&#34;&#39;, &#39;&#39;))
        if constraints:
            for key,val in constraints.items():
                pandas_db = pandas_db.loc[pandas_db[key] == val]
                pandas_db = pandas_db.drop(columns=key)
        if drop_field:
            for key,val in drop_field.items():
                for i in val:
                    pandas_db =  pandas_db[pandas_db[key] != i ]
        if rename_columns:
            for key,val in rename_columns.items():
                pandas_db = pandas_db.rename(columns={key:val})
        if &#39;semaine&#39; in  pandas_db.columns:
            pandas_db[&#39;semaine&#39;] = [ week_to_date(i) for i in pandas_db[&#39;semaine&#39;]]
            #pandas_db = pandas_db.drop_duplicates(subset=[&#39;semaine&#39;])
            pandas_db = pandas_db.rename(columns={&#39;semaine&#39;:&#39;date&#39;})
        pandas_db[&#39;date&#39;] = pandas.to_datetime(pandas_db[&#39;date&#39;],errors=&#39;coerce&#39;).dt.date
        #self.dates  = pandas_db[&#39;date&#39;]
        if self.database_type[self.db][1] == &#39;nation&#39; and  self.database_type[self.db][0] in [&#39;FRA&#39;,&#39;CYP&#39;]:
            pandas_db[&#39;location&#39;] = self.database_type[self.db][2]
        pandas_db = pandas_db.sort_values([&#39;location&#39;,&#39;date&#39;])
        return pandas_db

   def return_structured_pandas(self,mypandas,**kwargs):
        &#39;&#39;&#39;
        Return the mainpandas core of the PyCoA structure
        &#39;&#39;&#39;
        kwargs_test(kwargs,[&#39;columns_skipped&#39;,&#39;columns_keeped&#39;],
            &#39;Bad args used in the return_structured_pandas function.&#39;)
        columns_skipped = kwargs.get(&#39;columns_skipped&#39;, None)
        absolutlyneeded = [&#39;date&#39;,&#39;location&#39;]
        defaultkeept = list(set(mypandas.columns.to_list()) - set(absolutlyneeded))
        columns_keeped  = kwargs.get(&#39;columns_keeped&#39;, defaultkeept)
        if columns_skipped:
            columns_keeped = [x for x in mypandas.columns.values.tolist() if x not in columns_skipped + absolutlyneeded]
        mypandas = mypandas[absolutlyneeded + columns_keeped]

        self.available_keys_words = columns_keeped #+ absolutlyneeded
        not_un_nation_dict={&#39;Kosovo&#39;:&#39;Serbia&#39;}
        for subpart_country, main_country in not_un_nation_dict.items() :
            tmp=(mypandas.loc[mypandas.location.isin([subpart_country,main_country])].groupby(&#39;date&#39;).sum())
            tmp[&#39;location&#39;]=main_country
            mypandas = mypandas.loc[~mypandas.location.isin([subpart_country,main_country])]
            tmp = tmp.reset_index()
            cols = tmp.columns.tolist()
            cols = cols[0:1] + cols[-1:] + cols[1:-1]
            tmp = tmp[cols]
            mypandas = mypandas.append(tmp)
        if &#39;iso_code&#39; in mypandas.columns:
            mypandas[&#39;iso_code&#39;] = mypandas[&#39;iso_code&#39;].dropna().astype(str)
            mypandasori=mypandas.copy()
            strangeiso3tokick = [i for i in mypandasori[&#39;iso_code&#39;].dropna().unique() if not len(i)==3 ]
            mypandasori = mypandas.loc[~mypandas.iso_code.isin(strangeiso3tokick)]
            self.available_keys_words.remove(&#39;iso_code&#39;)
            mypandasori = mypandasori.drop(columns=[&#39;location&#39;])
            mypandasori = mypandasori.rename(columns={&#39;iso_code&#39;:&#39;location&#39;})
            if self.db == &#39;owid&#39;:
                onlyowid = mypandas.loc[mypandas.iso_code.isin(strangeiso3tokick)]
                onlyowid = onlyowid.copy()
                onlyowid.loc[:,&#39;location&#39;] = onlyowid[&#39;location&#39;].apply(lambda x : &#39;owid_&#39;+x)
            mypandas = mypandasori

        if self.db == &#39;dpc&#39;:
            gd = self.geo.get_data()[[&#39;name_region&#39;,&#39;code_region&#39;]]
            A=[&#39;P.A. Bolzano&#39;,&#39;P.A. Trento&#39;]
            tmp=mypandas.loc[mypandas.location.isin(A)].groupby(&#39;date&#39;).sum()
            tmp[&#39;location&#39;]=&#39;Trentino-Alto Adige&#39;
            mypandas = mypandas.loc[~mypandas.location.isin(A)]
            tmp = tmp.reset_index()
            mypandas = mypandas.append(tmp)
            uniqloc = list(mypandas[&#39;location&#39;].unique())
            sub2reg = dict(gd.values)
            #collections.OrderedDict(zip(uniqloc,list(gd.loc[gd.name_region.isin(uniqloc)][&#39;code_region&#39;])))
            mypandas[&#39;codelocation&#39;] = mypandas[&#39;location&#39;].map(sub2reg)
        if self.db == &#39;dgs&#39;:
            gd = self.geo.get_data()[[&#39;name_region&#39;,&#39;name_region&#39;]]
            mypandas = mypandas.reset_index(drop=True)
            mypandas[&#39;location&#39;] = mypandas[&#39;location&#39;].apply(lambda x: x.title().replace(&#39;Do&#39;, &#39;do&#39;).replace(&#39;Da&#39;,&#39;da&#39;).replace(&#39;De&#39;,&#39;de&#39;))
            uniqloc = list(mypandas[&#39;location&#39;].unique())
            sub2reg = dict(gd.values)
            #sub2reg = collections.OrderedDict(zip(uniqloc,list(gd.loc[gd.name_subregion.isin(uniqloc)][&#39;name_region&#39;])))
            mypandas[&#39;location&#39;] = mypandas[&#39;location&#39;].map(sub2reg)
            mypandas = mypandas.loc[~mypandas.location.isnull()]

         # filling subregions.
            gd = self.geo.get_data()[[&#39;code_region&#39;,&#39;name_region&#39;]]
            uniqloc = list(mypandas[&#39;location&#39;].unique())
            name2code = collections.OrderedDict(zip(uniqloc,list(gd.loc[gd.name_region.isin(uniqloc)][&#39;code_region&#39;])))
            mypandas = mypandas.loc[~mypandas.location.isnull()]

        codename = None
        location_is_code = False
        uniqloc = list(mypandas[&#39;location&#39;].unique()) # if possible location from csv are codelocation

        if self.db_world:
            uniqloc = [s for s in uniqloc if &#39;OWID_&#39; not in s]
            db=self.get_db()
            if self.db == &#39;govcy&#39;:
                db=None
            codename = collections.OrderedDict(zip(uniqloc,self.geo.to_standard(uniqloc,output=&#39;list&#39;,db=db,interpret_region=True)))
            self.slocation = list(codename.values())
            location_is_code = True
        else:
            if self.database_type[self.db][1] == &#39;region&#39; :
                if self.db == &#39;covid19india&#39;:
                    mypandas = mypandas.loc[~mypandas.location.isnull()]
                    uniqloc = list(mypandas[&#39;location&#39;].unique())
                temp = self.geo.get_region_list()[[&#39;name_region&#39;,&#39;code_region&#39;]]
                #codename = collections.OrderedDict(zip(uniqloc,list(temp.loc[temp.name_region.isin(uniqloc)][&#39;code_region&#39;])))
                codename=dict(temp.values)
                self.slocation = uniqloc
                if self.db == &#39;obepine&#39;:
                    codename = {v:k for k,v in codename.items()}
                    location_is_code = True

            elif self.database_type[self.db][1] == &#39;subregion&#39;:
                temp = self.geo_all[[&#39;code_subregion&#39;,&#39;name_subregion&#39;]]
                codename=dict(temp.loc[temp.code_subregion.isin(uniqloc)].values)
                if self.db in [&#39;phe&#39;,&#39;covidtracking&#39;,&#39;spf&#39;,&#39;escovid19data&#39;,&#39;opencovid19&#39;,&#39;minciencia&#39;,&#39;moh&#39;,&#39;risklayer&#39;,&#39;insee&#39;]:
                    #codename={i:list(temp.loc[temp.code_subregion.isin([i])][&#39;name_subregion&#39;])[0] for i in uniqloc if not temp.loc[temp.code_subregion.isin([i])][&#39;name_subregion&#39;].empty }
                    #codename = collections.OrderedDict(zip(uniqloc,list(temp.loc[temp.code_subregion.isin(uniqloc)][&#39;name_subregion&#39;])))
                    self.slocation = list(codename.values())
                    location_is_code = True
                else:
                    #codename=dict(temp.loc[temp.code_subregion.isin(uniqloc)][[&#39;code_subregion&#39;,&#39;name_subregion&#39;]].values)
                    #codename={i:list(temp.loc[temp.code_subregion.isin([i])][&#39;code_subregion&#39;])[0] for i in uniqloc if not temp.loc[temp.code_subregion.isin([i])][&#39;code_subregion&#39;].empty }
                    #codename = collections.OrderedDict(zip(uniqloc,list(temp.loc[temp.name_subregion.isin(uniqloc)][&#39;code_subregion&#39;])))
                    #print(codename)
                    self.slocation = uniqloc
            else:
                CoaDbError(&#39;Granularity problem , neither region nor sub_region ...&#39;)

        if self.db == &#39;dgs&#39;:
            mypandas = mypandas.reset_index(drop=True)

        if self.db != &#39;spfnational&#39;:
            mypandas = mypandas.groupby([&#39;location&#39;,&#39;date&#39;]).sum(min_count=1).reset_index() # summing in case of multiple dates (e.g. in opencovid19 data). But keep nan if any

        if self.db == &#39;govcy&#39;:
            location_is_code=False

        mypandas = fill_missing_dates(mypandas)

        if location_is_code:
            if self.db != &#39;dgs&#39;:
                mypandas[&#39;codelocation&#39;] =  mypandas[&#39;location&#39;].astype(str)
            mypandas[&#39;location&#39;] = mypandas[&#39;location&#39;].map(codename)
            if self.db == &#39;obepine&#39;:
                mypandas = mypandas.dropna(subset=[&#39;location&#39;])
                self.slocation = list(mypandas.codelocation.unique())
            mypandas = mypandas.loc[~mypandas.location.isnull()]
        else:
            mypandas[&#39;codelocation&#39;] =  mypandas[&#39;location&#39;].map(codename).astype(str)
        if self.db == &#39;owid&#39;:
            onlyowid[&#39;codelocation&#39;] = onlyowid[&#39;location&#39;]
            mypandas = mypandas.append(onlyowid)
        self.mainpandas  = mypandas
        self.dates  = self.mainpandas[&#39;date&#39;]

   def get_mainpandas(self,**kwargs):
       &#39;&#39;&#39;
            * defaut :
                 - location = None
                 - date = None
                 - selected_col = None
                Return the csv file to the mainpandas structure
                index | location              | date      | keywords1       |  keywords2    | ...| keywordsn
                -----------------------------------------------------------------------------------------
                0     |        location1      |    1      |  l1-val1-1      |  l1-val2-1    | ...|  l1-valn-1
                1     |        location1      |    2      |  l1-val1-2      |  l1-val2-2    | ...|  l1-valn-2
                2     |        location1      |    3      |  l1-val1-3      |  l1-val2-3    | ...|  l1-valn-3
                                 ...
                p     |       locationp       |    1      |   lp-val1-1     |  lp-val2-1    | ...| lp-valn-1
                ...
            * location : list of location (None : all location)
            * date : latest date to retrieve (None : max date)
            * selected_col: column to keep according to get_available_keys_words (None : all get_available_keys_words)
                            N.B. location column is added
        &#39;&#39;&#39;
       kwargs_test(kwargs,[&#39;location&#39;, &#39;date&#39;, &#39;selected_col&#39;],
                    &#39;Bad args used in the get_stats() function.&#39;)

       location = kwargs.get(&#39;location&#39;, None)
       selected_col = kwargs.get(&#39;selected_col&#39;, None)
       watch_date = kwargs.get(&#39;date&#39;, None)
       if location:
            if not isinstance(location, list):
                clist = ([location]).copy()
            else:
                clist = (location).copy()
            if not all(isinstance(c, str) for c in clist):
                raise CoaWhereError(&#34;Location via the where keyword should be given as strings. &#34;)
            if self.db_world:
                self.geo.set_standard(&#39;name&#39;)
                if self.db == &#39;owid&#39;:
                    owid_name = [c for c in clist if c.startswith(&#39;owid_&#39;)]
                    clist = [c for c in clist if not c.startswith(&#39;owid_&#39;)]
                clist=self.geo.to_standard(clist,output=&#39;list&#39;, interpret_region=True)
            else:
                clist=clist+self.geo.get_subregions_from_list_of_region_names(clist)
                if clist in [&#39;FRA&#39;,&#39;USA&#39;,&#39;ITA&#39;] :
                    clist=self.geo_all[&#39;code_subregion&#39;].to_list()

            clist=list(set(clist)) # to suppress duplicate countries
            diff_locations = list(set(clist) - set(self.get_locations()))
            clist = [i for i in clist if i not in diff_locations]
            filtered_pandas = self.mainpandas.copy()
            if len(clist) == 0 and len(owid_name) == 0:
                raise CoaWhereError(&#39;Not a correct location found according to the where option given.&#39;)
            if self.db == &#39;owid&#39;:
                clist+=owid_name
            filtered_pandas = filtered_pandas.loc[filtered_pandas.location.isin(clist)]
            if watch_date:
                check_valid_date(watch_date)
                mydate = pd.to_datetime(watch_date).date()
            else :
                mydate = filtered_pandas.date.max()
            filtered_pandas = filtered_pandas.loc[filtered_pandas.date==mydate].reset_index(drop=True)
            if selected_col:
                l = selected_col
            else:
                l=list(self.get_available_keys_words())
            l.insert(0, &#39;location&#39;)
            filtered_pandas = filtered_pandas[l]
            return filtered_pandas
       self.mainpandas = self.mainpandas.reset_index(drop=True)
       return self.mainpandas

   @staticmethod
   def flat_list(matrix):
        &#39;&#39;&#39; Flatten list function used in covid19 methods&#39;&#39;&#39;
        flatten_matrix = []
        for sublist in matrix:
            if isinstance(sublist,list):
                for val in sublist:
                    flatten_matrix.append(val)
            else:
                flatten_matrix.append(sublist)
        return flatten_matrix

   def get_dates(self):
        &#39;&#39;&#39; Return all dates available in the current database as datetime format&#39;&#39;&#39;
        return self.dates.values

   def get_locations(self):
        &#39;&#39;&#39; Return available location countries / regions in the current database
            Using the geo method standardization
        &#39;&#39;&#39;
        return self.slocation

   def return_nonan_dates_pandas(self, df = None, field = None):
         &#39;&#39;&#39; Check if for last date all values are nan, if yes check previous date and loop until false&#39;&#39;&#39;
         watchdate = df.date.max()
         boolval = True
         j = 0
         while (boolval):
             boolval = df.loc[df.date == (watchdate - dt.timedelta(days=j))][field].dropna().empty
             j += 1
         df = df.loc[df.date &lt;= watchdate - dt.timedelta(days=j - 1)]
         boolval = True
         j = 0
         watchdate = df.date.min()
         while (boolval):
             boolval = df.loc[df.date == (watchdate + dt.timedelta(days=j))][field].dropna().empty
             j += 1
         df = df.loc[df.date &gt;= watchdate - dt.timedelta(days=j - 1)]
         return df

   def get_stats(self, **kwargs):
        &#39;&#39;&#39;
        Return the pandas pandas_datase
         - index: only an incremental value
         - location: list of location used in the database selected (using geo standardization)
         - &#39;which&#39; :  return the keyword values selected from the avalailable keywords keepted seems
            self.get_available_keys_words()

         - &#39;option&#39; :default none
            * &#39;nonneg&#39; In some cases negatives values can appeared due to a database updated, nonneg option
                will smooth the curve during all the period considered
            * &#39;nofillnan&#39; if you do not want that NaN values are filled, which is the default behaviour
            * &#39;smooth7&#39; moving average, window of 7 days
            * &#39;sumall&#39; sum data over all locations

        keys are keyswords from the selected database
                location        | date      | keywords          |  daily            |  weekly
                -----------------------------------------------------------------------
                location1       |    1      |  val1-1           |  daily1-1          |  diff1-1
                location1       |    2      |  val1-2           |  daily1-2          |  diff1-2
                location1       |    3      |  val1-3           |  daily1-3          |  diff1-3
                    ...             ...                     ...
                location1       | last-date |  val1-lastdate    |  cumul1-lastdate   |   diff1-lastdate
                    ...
                location-i      |    1      |  vali-1           |  dailyi-1          |  diffi-1
                location-i      |    2      |  vali-1           |  daily1i-2         |  diffi-2
                location-i      |    3      |  vali-1           |  daily1i-3         |  diffi-3
                    ...

        &#39;&#39;&#39;
        kwargs_test(kwargs,[&#39;location&#39;,&#39;which&#39;,&#39;option&#39;],
            &#39;Bad args used in the get_stats() function.&#39;)
        wallname = None
        if not &#39;location&#39; in kwargs or kwargs[&#39;location&#39;] is None.__class__ or kwargs[&#39;location&#39;] == None:
            if get_db_list_dict()[self.db][0] == &#39;WW&#39;:
                kwargs[&#39;location&#39;] = &#39;world&#39;
            else:
                kwargs[&#39;location&#39;] = self.slocation #self.geo_all[&#39;code_subregion&#39;].to_list()
            wallname = get_db_list_dict()[self.db][2]
        else:
            kwargs[&#39;location&#39;] = kwargs[&#39;location&#39;]

        option = kwargs.get(&#39;option&#39;, &#39;fillnan&#39;)
        fillnan = True # default
        sumall = False # default
        sumallandsmooth7 = False
        if kwargs[&#39;which&#39;] not in self.get_available_keys_words():
            raise CoaKeyError(kwargs[&#39;which&#39;]+&#39; is not a available for &#39; + self.db + &#39; database name. &#39;
            &#39;See get_available_keys_words() for the full list.&#39;)

        #while for last date all values are nan previous date
        mainpandas = self.return_nonan_dates_pandas(self.get_mainpandas(),kwargs[&#39;which&#39;])
        devorigclist = None
        origclistlist = None
        origlistlistloc = None
        if option and &#39;sumall&#39; in option:
            if not isinstance(kwargs[&#39;location&#39;], list):
                kwargs[&#39;location&#39;] = [[kwargs[&#39;location&#39;]]]
            else:
                if isinstance(kwargs[&#39;location&#39;][0], list):
                    kwargs[&#39;location&#39;] = kwargs[&#39;location&#39;]
                else:
                    kwargs[&#39;location&#39;] = [kwargs[&#39;location&#39;]]
        if not isinstance(kwargs[&#39;location&#39;], list):
            listloc = ([kwargs[&#39;location&#39;]]).copy()
            if not all(isinstance(c, str) for c in listloc):
                raise CoaWhereError(&#34;Location via the where keyword should be given as strings. &#34;)
            origclist = listloc
        else:
            listloc = (kwargs[&#39;location&#39;]).copy()
            origclist = listloc
            if any(isinstance(c, list) for c in listloc):
                if all(isinstance(c, list) for c in listloc):
                    origlistlistloc = listloc
                else:
                    raise CoaWhereError(&#34;In the case of sumall all locations must have the same types i.e\
                    list or string but both is not accepted, could be confusing&#34;)
        owid_name=&#39;&#39;
        if self.db_world:
            self.geo.set_standard(&#39;name&#39;)
            if origlistlistloc != None:
                #fulllist = [ i if isinstance(i, list) else [i] for i in origclist ]
                fulllist = []
                for deploy in origlistlistloc:
                    d=[]
                    for i in deploy:
                        if not self.geo.get_GeoRegion().is_region(i):
                            d.append(self.geo.to_standard(i,output=&#39;list&#39;,interpret_region=True)[0])
                        else:
                            d.append(self.geo.get_GeoRegion().is_region(i))
                    fulllist.append(d)
                dicooriglist = { &#39;,&#39;.join(i):self.geo.to_standard(i,output=&#39;list&#39;,interpret_region=True) for i in fulllist}
                location_exploded = list(dicooriglist.values())
            else:
                owid_name = [c for c in origclist if c.startswith(&#39;owid_&#39;)]
                clist = [c for c in origclist if not c.startswith(&#39;owid_&#39;)]
                location_exploded = self.geo.to_standard(listloc,output=&#39;list&#39;,interpret_region=True)
                if len(owid_name) !=0 :
                    location_exploded += owid_name
        else:
            def explosion(listloc,typeloc=&#39;subregion&#39;):
                exploded = []
                a=self.geo.get_data()
                for i in listloc:
                    if typeloc == &#39;subregion&#39;:
                        if self.geo.is_region(i):
                            i = [self.geo.is_region(i)]
                            tmp = self.geo.get_subregions_from_list_of_region_names(i,output=&#39;name&#39;)
                        elif self.geo.is_subregion(i):
                           tmp = self.geo.is_subregion(i)
                        else:
                            raise CoaTypeError(i + &#39;: not subregion nor region ... what is it ?&#39;)
                    elif typeloc == &#39;region&#39;:
                        tmp = self.geo.get_region_list()
                        if i.isdigit():
                            tmp = list(tmp.loc[tmp.code_region==i][&#39;name_region&#39;])
                        elif self.geo.is_region(i):
                            tmp = self.geo.get_regions_from_macroregion(name=i,output=&#39;name&#39;)
                            if get_db_list_dict()[self.db][0] in [&#39;USA, FRA, ESP, PRT&#39;]:
                                tmp = tmp[:-1]
                        else:
                            if self.geo.is_subregion(i):
                                raise CoaTypeError(i+ &#39; is a subregion ... not compatible with a region DB granularity?&#39;)
                            else:
                                raise CoaTypeError(i + &#39;: not subregion nor region ... what is it ?&#39;)
                    else:
                        raise CoaTypeError(&#39;Not subregion nor region requested, don\&#39;t know what to do ?&#39;)
                    if exploded:
                        exploded.append(tmp)
                    else:
                        exploded=[tmp]
                return DataBase.flat_list(exploded)

            if origlistlistloc != None:
                dicooriglist={&#39;,&#39;.join(i):explosion(i,self.database_type[self.db][1]) for i in origlistlistloc}
                #origlistlistloc = DataBase.flat_list(list(dicooriglist.values()))
                #location_exploded = origlistlistloc
            else:
                listloc = explosion(listloc,self.database_type[self.db][1])
                listloc = DataBase.flat_list(listloc)
                location_exploded = listloc
        def sticky(lname):
            if len(lname)&gt;0:
                tmp=&#39;&#39;
                for i in lname:
                    tmp += i+&#39;, &#39;
                lname=tmp[:-2]
            return [lname]

        pdcluster = pd.DataFrame()
        j=0

        if origlistlistloc != None:
            for k,v in dicooriglist.items():
                tmp  = mainpandas.copy()
                if any(isinstance(c, list) for c in v):
                    v=v[0]
                tmp = tmp.loc[tmp.location.isin(v)]
                code = tmp.codelocation.unique()
                tmp[&#39;clustername&#39;] = [k]*len(tmp)
                if pdcluster.empty:
                    pdcluster = tmp
                else:
                    pdcluster = pdcluster.append(tmp)
                j+=1
            pdfiltered = pdcluster[[&#39;location&#39;,&#39;date&#39;,&#39;codelocation&#39;,kwargs[&#39;which&#39;],&#39;clustername&#39;]]
        else:
            pdfiltered = mainpandas.loc[mainpandas.location.isin(location_exploded)]
            pdfiltered = pdfiltered[[&#39;location&#39;,&#39;date&#39;,&#39;codelocation&#39;,kwargs[&#39;which&#39;]]]
            pdfiltered[&#39;clustername&#39;] = pdfiltered[&#39;location&#39;].copy()
        if not isinstance(option,list):
            option=[option]
        if &#39;fillnan&#39; not in option and &#39;nofillnan&#39; not in option:
            option.insert(0, &#39;fillnan&#39;)
        if &#39;nonneg&#39; in option:
            option.remove(&#39;nonneg&#39;)
            option.insert(0, &#39;nonneg&#39;)
        if &#39;smooth7&#39; in  option and &#39;sumall&#39; in  option:
            option.remove(&#39;sumall&#39;)
            option.remove(&#39;smooth7&#39;)
            option+=[&#39;sumallandsmooth7&#39;]
        for o in option:
            if o == &#39;nonneg&#39;:
                if kwargs[&#39;which&#39;].startswith(&#39;cur_&#39;):
                    raise CoaKeyError(&#39;The option nonneg cannot be used with instantaneous data, such as cur_ which variables.&#39;)
                cluster=list(pdfiltered.clustername.unique())
                separated = [ pdfiltered.loc[pdfiltered.clustername==i] for i in cluster]
                reconstructed = pd.DataFrame()
                for sub in separated:
                    location = list(sub.location.unique())
                    for loca in location:
                        pdloc = sub.loc[sub.location == loca][kwargs[&#39;which&#39;]]
                        try:
                            y0=pdloc.values[0] # integrated offset at t=0
                        except:
                            y0=0
                        if np.isnan(y0):
                            y0=0
                        pa = pdloc.diff()
                        yy = pa.values
                        ind = list(pa.index)
                        where_nan = np.isnan(yy)
                        yy[where_nan] = 0.
                        indices=np.where(yy &lt; 0)[0]
                        for kk in np.where(yy &lt; 0)[0]:
                            k = int(kk)
                            val_to_repart = -yy[k]
                            if k &lt; np.size(yy)-1:
                                yy[k] = (yy[k+1]+yy[k-1])/2
                            else:
                                yy[k] = yy[k-1]
                            val_to_repart = val_to_repart + yy[k]
                            s = np.nansum(yy[0:k])
                            if not any([i !=0 for i in yy[0:k]]) == True and s == 0:
                                yy[0:k] = 0.
                            elif s == 0:
                                yy[0:k] = np.nan*np.ones(k)
                            else:
                                yy[0:k] = yy[0:k]*(1-float(val_to_repart)/s)
                        sub=sub.copy()
                        sub.loc[ind,kwargs[&#39;which&#39;]]=np.cumsum(yy)+y0 # do not forget the offset
                    if reconstructed.empty:
                        reconstructed = sub
                    else:
                        reconstructed=reconstructed.append(sub)
                    pdfiltered = reconstructed
            elif o == &#39;nofillnan&#39;:
                pdfiltered_nofillnan = pdfiltered.copy().reset_index(drop=True)
                fillnan=False
            elif o == &#39;fillnan&#39;:
                fillnan=True
                # fill with previous value
                pdfiltered = pdfiltered.reset_index(drop=True)
                pdfiltered_nofillnan = pdfiltered.copy()

                pdfiltered.loc[:,kwargs[&#39;which&#39;]] =\
                pdfiltered.groupby([&#39;location&#39;,&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.bfill())
                #if kwargs[&#39;which&#39;].startswith(&#39;total_&#39;) or kwargs[&#39;which&#39;].startswith(&#39;tot_&#39;):
                #    pdfiltered.loc[:,kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.ffill())
                if pdfiltered.loc[pdfiltered.date == pdfiltered.date.max()][kwargs[&#39;which&#39;]].isnull().values.any():
                    print(kwargs[&#39;which&#39;], &#34;has been selected. Some missing data has been interpolated from previous data.&#34;)
                    print(&#34;This warning appear right now due to some missing values at the latest date &#34;, pdfiltered.date.max(),&#34;.&#34;)
                    print(&#34;Use the option=&#39;nofillnan&#39; if you want to only display the original data&#34;)
                    pdfiltered.loc[:,kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;location&#39;,&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.ffill())
                    pdfiltered = pdfiltered[pdfiltered[kwargs[&#39;which&#39;]].notna()]
            elif o == &#39;smooth7&#39;:
                pdfiltered[kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;location&#39;])[kwargs[&#39;which&#39;]].rolling(7,min_periods=7).mean().reset_index(level=0,drop=True)
                inx7=pdfiltered.groupby(&#39;location&#39;).head(7).index
                pdfiltered.loc[inx7, kwargs[&#39;which&#39;]] = pdfiltered[kwargs[&#39;which&#39;]].fillna(method=&#34;bfill&#34;)
                fillnan=True
            elif o == &#39;sumall&#39;:
                sumall = True
            elif o == &#39;sumallandsmooth7&#39;:
                sumall = True
                sumallandsmooth7 = True
            elif o != None and o != &#39;&#39; and o != &#39;sumallandsmooth7&#39;:
                raise CoaKeyError(&#39;The option &#39;+o+&#39; is not recognized in get_stats. See get_available_options() for list.&#39;)
        pdfiltered = pdfiltered.reset_index(drop=True)

        # if sumall set, return only integrate val
        tmppandas=pd.DataFrame()
        if sumall:
            if origlistlistloc != None:
               uniqcluster = pdfiltered.clustername.unique()
               if kwargs[&#39;which&#39;].startswith(&#39;cur_idx_&#39;):
                  tmp = pdfiltered.groupby([&#39;clustername&#39;,&#39;date&#39;]).mean().reset_index()
               else:
                  tmp = pdfiltered.groupby([&#39;clustername&#39;,&#39;date&#39;]).sum().reset_index()#.loc[pdfiltered.clustername.isin(uniqcluster)].\

               codescluster = {i:list(pdfiltered.loc[pdfiltered.clustername==i][&#39;codelocation&#39;].unique()) for i in uniqcluster}
               namescluster = {i:list(pdfiltered.loc[pdfiltered.clustername==i][&#39;location&#39;].unique()) for i in uniqcluster}
               tmp[&#39;codelocation&#39;] = tmp[&#39;clustername&#39;].map(codescluster)
               tmp[&#39;location&#39;] = tmp[&#39;clustername&#39;].map(namescluster)

               pdfiltered = tmp
               pdfiltered = pdfiltered.drop_duplicates([&#39;date&#39;,&#39;clustername&#39;])
               if sumallandsmooth7:
                   pdfiltered[kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;clustername&#39;])[kwargs[&#39;which&#39;]].rolling(7,min_periods=7).mean().reset_index(level=0,drop=True)
                   pdfiltered.loc[:,kwargs[&#39;which&#39;]] =\
                   pdfiltered.groupby([&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.bfill())
            # computing daily, cumul and weekly
            else:
                if kwargs[&#39;which&#39;].startswith(&#39;cur_idx_&#39;):
                    tmp = pdfiltered.groupby([&#39;date&#39;]).mean().reset_index()
                else:
                    tmp = pdfiltered.groupby([&#39;date&#39;]).sum().reset_index()
                uniqloc = list(pdfiltered.location.unique())
                uniqcodeloc = list(pdfiltered.codelocation.unique())
                tmp.loc[:,&#39;location&#39;] = [&#39;dummy&#39;]*len(tmp)
                tmp.loc[:,&#39;codelocation&#39;] = [&#39;dummy&#39;]*len(tmp)
                tmp.loc[:,&#39;clustername&#39;] = [&#39;dummy&#39;]*len(tmp)
                for i in range(len(tmp)):
                    tmp.at[i,&#39;location&#39;] = uniqloc #sticky(uniqloc)
                    tmp.at[i,&#39;codelocation&#39;] = uniqcodeloc #sticky(uniqcodeloc)
                    tmp.at[i,&#39;clustername&#39;] =  sticky(uniqloc)[0]
                pdfiltered = tmp
        else:
            if self.db_world :
                pdfiltered[&#39;clustername&#39;] = pdfiltered[&#39;location&#39;].apply(lambda x: self.geo.to_standard(x)[0] if not x.startswith(&#34;owid_&#34;) else x)
            else:
                pdfiltered[&#39;clustername&#39;] = pdfiltered[&#39;location&#39;]

        if &#39;cur_&#39; in kwargs[&#39;which&#39;] or &#39;total_&#39; in kwargs[&#39;which&#39;] or &#39;tot_&#39; in kwargs[&#39;which&#39;]:
            pdfiltered[&#39;cumul&#39;] = pdfiltered[kwargs[&#39;which&#39;]]
        else:
            pdfiltered[&#39;cumul&#39;] = pdfiltered_nofillnan.groupby(&#39;clustername&#39;)[kwargs[&#39;which&#39;]].cumsum()
            if fillnan:
                pdfiltered.loc[:,&#39;cumul&#39;] =\
                pdfiltered.groupby(&#39;clustername&#39;)[&#39;cumul&#39;].apply(lambda x: x.ffill())

        pdfiltered[&#39;daily&#39;] = pdfiltered.groupby(&#39;clustername&#39;)[&#39;cumul&#39;].diff()
        inx = pdfiltered.groupby(&#39;clustername&#39;).head(1).index
        pdfiltered[&#39;weekly&#39;] = pdfiltered.groupby(&#39;clustername&#39;)[&#39;cumul&#39;].diff(7)
        inx7=pdfiltered.groupby(&#39;clustername&#39;).head(7).index
        #First value of diff is always NaN
        pdfiltered.loc[inx, &#39;daily&#39;] = pdfiltered[&#39;daily&#39;].fillna(method=&#34;bfill&#34;)
        pdfiltered.loc[inx7, &#39;weekly&#39;] = pdfiltered[&#39;weekly&#39;].fillna(method=&#34;bfill&#34;)

        unifiedposition=[&#39;location&#39;, &#39;date&#39;, kwargs[&#39;which&#39;], &#39;daily&#39;, &#39;cumul&#39;, &#39;weekly&#39;, &#39;codelocation&#39;,&#39;clustername&#39;]
        pdfiltered = pdfiltered[unifiedposition]

        if wallname != None and sumall == True:
               pdfiltered.loc[:,&#39;clustername&#39;] = wallname

        pdfiltered = pdfiltered.drop(columns=&#39;cumul&#39;)
        verb(&#34;Here the information I\&#39;ve got on &#34;, kwargs[&#39;which&#39;],&#34; : &#34;, self.get_keyword_definition(kwargs[&#39;which&#39;]))
        return pdfiltered

   def merger(self,**kwargs):
        &#39;&#39;&#39;
        Merge two or more pycoa pandas from get_stats operation
        &#39;coapandas&#39;: list (min 2D) of pandas from stats
        &#39;&#39;&#39;

        coapandas = kwargs.get(&#39;coapandas&#39;, None)

        if coapandas is None or not isinstance(coapandas, list) or len(coapandas)&lt;=1:
            raise CoaKeyError(&#39;coapandas value must be at least a list of 2 elements ... &#39;)

        def renamecol(pandy):
            torename=[&#39;daily&#39;,&#39;cumul&#39;,&#39;weekly&#39;]
            return pandy.rename(columns={i:pandy.columns[2]+&#39;_&#39;+i  for i in torename})
        base = coapandas[0].copy()
        coapandas = [ renamecol(p) for p in coapandas ]
        base = coapandas[0].copy()
        if not &#39;clustername&#39; in base.columns:
            raise CoaKeyError(&#39;No &#34;clustername&#34; in your pandas columns ... don\&#39;t know what to do &#39;)

        j=1
        for p in coapandas[1:]:
            [ p.drop([i],axis=1, inplace=True) for i in [&#39;location&#39;,&#39;where&#39;,&#39;codelocation&#39;] if i in p.columns ]
            #p.drop([&#39;location&#39;,&#39;codelocation&#39;],axis=1, inplace=True)
            base = pd.merge(base,p,on=[&#39;date&#39;,&#39;clustername&#39;],how=&#34;inner&#34;)#,suffixes=(&#39;&#39;, &#39;_drop&#39;))
            #base.drop([col for col in base.columns if &#39;drop&#39; in col], axis=1, inplace=True)
        return base

   def appender(self,**kwargs):
      &#39;&#39;&#39;
      Append two or more pycoa pandas from get_stats operation
      &#39;coapandas&#39;: list (min 2D) of pandas from stats
      &#39;&#39;&#39;

      coapandas = kwargs.get(&#39;coapandas&#39;, None)
      if coapandas is None or not isinstance(coapandas, list) or len(coapandas)&lt;=1:
          raise CoaKeyError(&#39;coapandas value must be at least a list of 2 elements ... &#39;)

      coapandas = [ p.rename(columns={p.columns[2]:&#39;cases&#39;}) for p in coapandas ]
      m = pd.concat(coapandas).reset_index(drop=True)
      #m[&#39;clustername&#39;]=m.m(&#39;location&#39;)[&#39;clustername&#39;].fillna(method=&#39;bfill&#39;)
      #m[&#39;codelocation&#39;]=m.groupby(&#39;location&#39;)[&#39;codelocation&#39;].fillna(method=&#39;bfill&#39;)
      m=m.drop(columns=[&#39;codelocation&#39;,&#39;clustername&#39;])
      return fill_missing_dates(m)

   def saveoutput(self,**kwargs):
       &#39;&#39;&#39;
       saveoutput pycoas pandas as an  output file selected by output argument
       &#39;pandas&#39;: pycoa pandas
       &#39;saveformat&#39;: excel or csv (default excel)
       &#39;savename&#39;: pycoaout (default)
       &#39;&#39;&#39;
       possibleformat=[&#39;excel&#39;,&#39;csv&#39;]
       saveformat = &#39;excel&#39;
       savename = &#39;pycoaout&#39;
       pandyori = &#39;&#39;
       if &#39;saveformat&#39; in kwargs:
            saveformat = kwargs[&#39;saveformat&#39;]
       if saveformat not in possibleformat:
           raise CoaKeyError(&#39;Output option &#39;+saveformat+&#39; is not recognized.&#39;)
       if &#39;savename&#39; in kwargs and kwargs[&#39;savename&#39;] != &#39;&#39;:
          savename = kwargs[&#39;savename&#39;]

       if not &#39;pandas&#39; in kwargs:
          raise CoaKeyError(&#39;Absolute needed variable : the pandas desired &#39;)
       else:
          pandyori = kwargs[&#39;pandas&#39;]
       pandy = pandyori
       pandy[&#39;date&#39;] = pd.to_datetime(pandy[&#39;date&#39;])
       pandy[&#39;date&#39;]=pandy[&#39;date&#39;].apply(lambda x: x.strftime(&#39;%Y-%m-%d&#39;))
       if saveformat == &#39;excel&#39;:
           pandy.to_excel(savename+&#39;.xlsx&#39;,index=False, na_rep=&#39;NAN&#39;)
       elif saveformat == &#39;csv&#39;:
           pandy.to_csv(savename+&#39;.csv&#39;, encoding=&#39;utf-8&#39;, index=False, float_format=&#39;%.4f&#39;,na_rep=&#39;NAN&#39;)

   ## https://www.kaggle.com/freealf/estimation-of-rt-from-cases
   def smooth_cases(self,cases):
        new_cases = cases

        smoothed = new_cases.rolling(7,
            win_type=&#39;gaussian&#39;,
            min_periods=1,
            center=True).mean(std=2).round()
            #center=False).mean(std=2).round()

        zeros = smoothed.index[smoothed.eq(0)]
        if len(zeros) == 0:
            idx_start = 0
        else:
            last_zero = zeros.max()
            idx_start = smoothed.index.get_loc(last_zero) + 1
        smoothed = smoothed.iloc[idx_start:]
        original = new_cases.loc[smoothed.index]

        return smoothed
   def get_posteriors(self,sr, window=7, min_periods=1):
        # We create an array for every possible value of Rt
        R_T_MAX = 12
        r_t_range = np.linspace(0, R_T_MAX, R_T_MAX*100+1)

        # Gamma is 1/serial interval
        # https://wwwnc.cdc.gov/eid/article/26/6/20-0357_article
        GAMMA = 1/7

        lam = sr[:-1].values * np.exp(GAMMA * (r_t_range[:, None] - 1))

        # Note: if you want to have a Uniform prior you can use the following line instead.
        # I chose the gamma distribution because of our prior knowledge of the likely value
        # of R_t.

        # prior0 = np.full(len(r_t_range), np.log(1/len(r_t_range)))
        prior0 = np.log(sps.gamma(a=3).pdf(r_t_range) + 1e-14)

        likelihoods = pd.DataFrame(
            # Short-hand way of concatenating the prior and likelihoods
            data = np.c_[prior0, sps.poisson.logpmf(sr[1:].values, lam)],
            index = r_t_range,
            columns = sr.index)

        # Perform a rolling sum of log likelihoods. This is the equivalent
        # of multiplying the original distributions. Exponentiate to move
        # out of log.
        posteriors = likelihoods.rolling(window,
                                     axis=1,
                                     min_periods=min_periods).sum()
        posteriors = np.exp(posteriors)

        # Normalize to 1.0
        posteriors = posteriors.div(posteriors.sum(axis=0), axis=1)

        return posteriors</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="coa.covid19.DataBase.factory"><code class="name flex">
<span>def <span class="ident">factory</span></span>(<span>db_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Return an instance to DataBase and to CocoDisplay methods
This is recommended to avoid mismatch in labeled figures</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def factory(db_name):
    &#39;&#39;&#39;
     Return an instance to DataBase and to CocoDisplay methods
     This is recommended to avoid mismatch in labeled figures
    &#39;&#39;&#39;
    datab = DataBase(db_name)
    return  datab, datab.get_display()</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.flat_list"><code class="name flex">
<span>def <span class="ident">flat_list</span></span>(<span>matrix)</span>
</code></dt>
<dd>
<div class="desc"><p>Flatten list function used in covid19 methods</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def flat_list(matrix):
     &#39;&#39;&#39; Flatten list function used in covid19 methods&#39;&#39;&#39;
     flatten_matrix = []
     for sublist in matrix:
         if isinstance(sublist,list):
             for val in sublist:
                 flatten_matrix.append(val)
         else:
             flatten_matrix.append(sublist)
     return flatten_matrix</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="coa.covid19.DataBase.appender"><code class="name flex">
<span>def <span class="ident">appender</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Append two or more pycoa pandas from get_stats operation
'coapandas': list (min 2D) of pandas from stats</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def appender(self,**kwargs):
   &#39;&#39;&#39;
   Append two or more pycoa pandas from get_stats operation
   &#39;coapandas&#39;: list (min 2D) of pandas from stats
   &#39;&#39;&#39;

   coapandas = kwargs.get(&#39;coapandas&#39;, None)
   if coapandas is None or not isinstance(coapandas, list) or len(coapandas)&lt;=1:
       raise CoaKeyError(&#39;coapandas value must be at least a list of 2 elements ... &#39;)

   coapandas = [ p.rename(columns={p.columns[2]:&#39;cases&#39;}) for p in coapandas ]
   m = pd.concat(coapandas).reset_index(drop=True)
   #m[&#39;clustername&#39;]=m.m(&#39;location&#39;)[&#39;clustername&#39;].fillna(method=&#39;bfill&#39;)
   #m[&#39;codelocation&#39;]=m.groupby(&#39;location&#39;)[&#39;codelocation&#39;].fillna(method=&#39;bfill&#39;)
   m=m.drop(columns=[&#39;codelocation&#39;,&#39;clustername&#39;])
   return fill_missing_dates(m)</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.csv2pandas"><code class="name flex">
<span>def <span class="ident">csv2pandas</span></span>(<span>self, url, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Parse and convert the database cvs file to a pandas structure</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def csv2pandas(self,url,**kwargs):
     &#39;&#39;&#39;
     Parse and convert the database cvs file to a pandas structure
     &#39;&#39;&#39;
     self.database_url.append(url)
     kwargs_test(kwargs,[&#39;cast&#39;,&#39;separator&#39;,&#39;encoding&#39;,&#39;constraints&#39;,&#39;rename_columns&#39;,&#39;drop_field&#39;,&#39;quotechar&#39;],
         &#39;Bad args used in the csv2pandas() function.&#39;)

     cast = kwargs.get(&#39;cast&#39;, None)
     dico_cast = {}
     if cast:
         for key,val in cast.items():
             dico_cast[key] = val
     separator = kwargs.get(&#39;separator&#39;, &#39;;&#39;)
     if separator:
         separator = separator
     encoding = kwargs.get(&#39;encoding&#39;, None)
     if encoding:
         encoding = encoding
     quoting=0
     if self.db == &#39;obepine&#39;:
           quoting=3
     pandas_db = pandas.read_csv(get_local_from_url(url,7200),sep=separator,dtype=dico_cast, encoding = encoding,
         keep_default_na=False,na_values=&#39;&#39;,header=0,quoting=quoting) # cached for 2 hours

     #pandas_db = pandas.read_csv(self.database_url,sep=separator,dtype=dico_cast, encoding = encoding )
     constraints = kwargs.get(&#39;constraints&#39;, None)
     rename_columns = kwargs.get(&#39;rename_columns&#39;, None)
     drop_field = kwargs.get(&#39;drop_field&#39;, None)
     if self.db == &#39;obepine&#39;:
         pandas_db = pandas_db.rename(columns=rename_columns)
         pandas_db = pandas_db.applymap(lambda x: x.replace(&#39;&#34;&#39;, &#39;&#39;))
     if constraints:
         for key,val in constraints.items():
             pandas_db = pandas_db.loc[pandas_db[key] == val]
             pandas_db = pandas_db.drop(columns=key)
     if drop_field:
         for key,val in drop_field.items():
             for i in val:
                 pandas_db =  pandas_db[pandas_db[key] != i ]
     if rename_columns:
         for key,val in rename_columns.items():
             pandas_db = pandas_db.rename(columns={key:val})
     if &#39;semaine&#39; in  pandas_db.columns:
         pandas_db[&#39;semaine&#39;] = [ week_to_date(i) for i in pandas_db[&#39;semaine&#39;]]
         #pandas_db = pandas_db.drop_duplicates(subset=[&#39;semaine&#39;])
         pandas_db = pandas_db.rename(columns={&#39;semaine&#39;:&#39;date&#39;})
     pandas_db[&#39;date&#39;] = pandas.to_datetime(pandas_db[&#39;date&#39;],errors=&#39;coerce&#39;).dt.date
     #self.dates  = pandas_db[&#39;date&#39;]
     if self.database_type[self.db][1] == &#39;nation&#39; and  self.database_type[self.db][0] in [&#39;FRA&#39;,&#39;CYP&#39;]:
         pandas_db[&#39;location&#39;] = self.database_type[self.db][2]
     pandas_db = pandas_db.sort_values([&#39;location&#39;,&#39;date&#39;])
     return pandas_db</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_available_database"><code class="name flex">
<span>def <span class="ident">get_available_database</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return all the available Covid19 database</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_available_database(self):
     &#39;&#39;&#39;
     Return all the available Covid19 database
     &#39;&#39;&#39;
     return self.database_name</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_available_keys_words"><code class="name flex">
<span>def <span class="ident">get_available_keys_words</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return all the available keyswords for the database selected
Key-words are for:
- jhu : ['deaths','confirmed','recovered']
* the data are cumulative i.e for a date it represents the total cases
For more information please have a look to <a href="https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data">https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data</a>
- 'owid' : ['total_deaths','total_cases','reproduction_rate','icu_patients','hosp_patients','total_tests',
'positive_rate','total_vaccinations']
For more information please have a look to <a href="https://github.com/owid/covid-19-data/tree/master/public/data/">https://github.com/owid/covid-19-data/tree/master/public/data/</a>
- 'spf' : ['hosp', 'rea', 'rad', 'dc', 'incid_hosp', 'incid_rea', 'incid_dc',
'incid_rad', 'P', 'T', 'tx_incid', 'R', 'taux_occupation_sae', 'tx_pos']
No translation have been done for french keywords data
For more information please have a look to
<a href="https://www.data.gouv.fr/fr/organizations/sante-publique-france/">https://www.data.gouv.fr/fr/organizations/sante-publique-france/</a>
- 'opencovid19' :['cas_confirmes', 'deces',
'reanimation', 'hospitalises','nouvelles_hospitalisations', 'nouvelles_reanimations', 'gueris', 'depistes']
- 'opencovid19national' :['cas_confirmes', 'cas_ehpad', 'cas_confirmes_ehpad', 'cas_possibles_ehpad', 'deces', 'deces_ehpad',
'reanimation', 'hospitalises','nouvelles_hospitalisations', 'nouvelles_reanimations', 'gueris', 'depistes']</p>
<p>No translation have been done for french keywords data
For more information please have a look to <a href="https://github.com/opencovid19-fr">https://github.com/opencovid19-fr</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_available_keys_words(self):
     &#39;&#39;&#39;
     Return all the available keyswords for the database selected
     Key-words are for:
     - jhu : [&#39;deaths&#39;,&#39;confirmed&#39;,&#39;recovered&#39;]
                         * the data are cumulative i.e for a date it represents the total cases
         For more information please have a look to https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data
     - &#39;owid&#39; : [&#39;total_deaths&#39;,&#39;total_cases&#39;,&#39;reproduction_rate&#39;,&#39;icu_patients&#39;,&#39;hosp_patients&#39;,&#39;total_tests&#39;,
                 &#39;positive_rate&#39;,&#39;total_vaccinations&#39;]
     For more information please have a look to https://github.com/owid/covid-19-data/tree/master/public/data/
     - &#39;spf&#39; : [&#39;hosp&#39;, &#39;rea&#39;, &#39;rad&#39;, &#39;dc&#39;, &#39;incid_hosp&#39;, &#39;incid_rea&#39;, &#39;incid_dc&#39;,
                 &#39;incid_rad&#39;, &#39;P&#39;, &#39;T&#39;, &#39;tx_incid&#39;, &#39;R&#39;, &#39;taux_occupation_sae&#39;, &#39;tx_pos&#39;]
         No translation have been done for french keywords data
     For more information please have a look to  https://www.data.gouv.fr/fr/organizations/sante-publique-france/
     - &#39;opencovid19&#39; :[&#39;cas_confirmes&#39;, &#39;deces&#39;,
     &#39;reanimation&#39;, &#39;hospitalises&#39;,&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;, &#39;gueris&#39;, &#39;depistes&#39;]
     - &#39;opencovid19national&#39; :[&#39;cas_confirmes&#39;, &#39;cas_ehpad&#39;, &#39;cas_confirmes_ehpad&#39;, &#39;cas_possibles_ehpad&#39;, &#39;deces&#39;, &#39;deces_ehpad&#39;,
     &#39;reanimation&#39;, &#39;hospitalises&#39;,&#39;nouvelles_hospitalisations&#39;, &#39;nouvelles_reanimations&#39;, &#39;gueris&#39;, &#39;depistes&#39;]

     No translation have been done for french keywords data
     For more information please have a look to https://github.com/opencovid19-fr
     &#39;&#39;&#39;
     return self.available_keys_words</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_available_options"><code class="name flex">
<span>def <span class="ident">get_available_options</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return available options for the get_stats method</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_available_options(self):
     &#39;&#39;&#39;
     Return available options for the get_stats method
     &#39;&#39;&#39;
     o=self.available_options
     return o</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_dates"><code class="name flex">
<span>def <span class="ident">get_dates</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return all dates available in the current database as datetime format</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dates(self):
     &#39;&#39;&#39; Return all dates available in the current database as datetime format&#39;&#39;&#39;
     return self.dates.values</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_db"><code class="name flex">
<span>def <span class="ident">get_db</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the current covid19 database selected. See get_available_database() for full list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_db(self):
     &#39;&#39;&#39;
     Return the current covid19 database selected. See get_available_database() for full list
     &#39;&#39;&#39;
     return self.db</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_display"><code class="name flex">
<span>def <span class="ident">get_display</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the instance of CocoDisplay initialized by factory</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_display(self):
    &#39;&#39;&#39; Return the instance of CocoDisplay initialized by factory&#39;&#39;&#39;
    return self.codisp</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_keyword_definition"><code class="name flex">
<span>def <span class="ident">get_keyword_definition</span></span>(<span>self, keys)</span>
</code></dt>
<dd>
<div class="desc"><p>Return definition on the selected keword</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_keyword_definition(self,keys):
    &#39;&#39;&#39;
         Return definition on the selected keword
    &#39;&#39;&#39;
    value = self.databaseinfo.generic_info(self.get_db(),keys)[0]
    return value</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_keyword_url"><code class="name flex">
<span>def <span class="ident">get_keyword_url</span></span>(<span>self, keys)</span>
</code></dt>
<dd>
<div class="desc"><p>Return url where the keyword have been parsed</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_keyword_url(self,keys):
    &#39;&#39;&#39;
     Return url where the keyword have been parsed
    &#39;&#39;&#39;
    value = self.databaseinfo.generic_info(self.get_db(),keys)[1]
    master  = self.databaseinfo.generic_info(self.get_db(),keys)[2]
    return value, master</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_locations"><code class="name flex">
<span>def <span class="ident">get_locations</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return available location countries / regions in the current database
Using the geo method standardization</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_locations(self):
     &#39;&#39;&#39; Return available location countries / regions in the current database
         Using the geo method standardization
     &#39;&#39;&#39;
     return self.slocation</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_mainpandas"><code class="name flex">
<span>def <span class="ident">get_mainpandas</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><ul>
<li>defaut :<ul>
<li>location = None</li>
<li>date = None</li>
<li>selected_col = None
Return the csv file to the mainpandas structure
index | location
| date
| keywords1
|
keywords2
| &hellip;| keywordsn</li>
</ul>
<hr>
<p>0
|
location1
|
1
|
l1-val1-1
|
l1-val2-1
| &hellip;|
l1-valn-1
1
|
location1
|
2
|
l1-val1-2
|
l1-val2-2
| &hellip;|
l1-valn-2
2
|
location1
|
3
|
l1-val1-3
|
l1-val2-3
| &hellip;|
l1-valn-3
&hellip;
p
|
locationp
|
1
|
lp-val1-1
|
lp-val2-1
| &hellip;| lp-valn-1
&hellip;</p>
</li>
<li>location : list of location (None : all location)</li>
<li>date : latest date to retrieve (None : max date)</li>
<li>selected_col: column to keep according to get_available_keys_words (None : all get_available_keys_words)
N.B. location column is added</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mainpandas(self,**kwargs):
    &#39;&#39;&#39;
         * defaut :
              - location = None
              - date = None
              - selected_col = None
             Return the csv file to the mainpandas structure
             index | location              | date      | keywords1       |  keywords2    | ...| keywordsn
             -----------------------------------------------------------------------------------------
             0     |        location1      |    1      |  l1-val1-1      |  l1-val2-1    | ...|  l1-valn-1
             1     |        location1      |    2      |  l1-val1-2      |  l1-val2-2    | ...|  l1-valn-2
             2     |        location1      |    3      |  l1-val1-3      |  l1-val2-3    | ...|  l1-valn-3
                              ...
             p     |       locationp       |    1      |   lp-val1-1     |  lp-val2-1    | ...| lp-valn-1
             ...
         * location : list of location (None : all location)
         * date : latest date to retrieve (None : max date)
         * selected_col: column to keep according to get_available_keys_words (None : all get_available_keys_words)
                         N.B. location column is added
     &#39;&#39;&#39;
    kwargs_test(kwargs,[&#39;location&#39;, &#39;date&#39;, &#39;selected_col&#39;],
                 &#39;Bad args used in the get_stats() function.&#39;)

    location = kwargs.get(&#39;location&#39;, None)
    selected_col = kwargs.get(&#39;selected_col&#39;, None)
    watch_date = kwargs.get(&#39;date&#39;, None)
    if location:
         if not isinstance(location, list):
             clist = ([location]).copy()
         else:
             clist = (location).copy()
         if not all(isinstance(c, str) for c in clist):
             raise CoaWhereError(&#34;Location via the where keyword should be given as strings. &#34;)
         if self.db_world:
             self.geo.set_standard(&#39;name&#39;)
             if self.db == &#39;owid&#39;:
                 owid_name = [c for c in clist if c.startswith(&#39;owid_&#39;)]
                 clist = [c for c in clist if not c.startswith(&#39;owid_&#39;)]
             clist=self.geo.to_standard(clist,output=&#39;list&#39;, interpret_region=True)
         else:
             clist=clist+self.geo.get_subregions_from_list_of_region_names(clist)
             if clist in [&#39;FRA&#39;,&#39;USA&#39;,&#39;ITA&#39;] :
                 clist=self.geo_all[&#39;code_subregion&#39;].to_list()

         clist=list(set(clist)) # to suppress duplicate countries
         diff_locations = list(set(clist) - set(self.get_locations()))
         clist = [i for i in clist if i not in diff_locations]
         filtered_pandas = self.mainpandas.copy()
         if len(clist) == 0 and len(owid_name) == 0:
             raise CoaWhereError(&#39;Not a correct location found according to the where option given.&#39;)
         if self.db == &#39;owid&#39;:
             clist+=owid_name
         filtered_pandas = filtered_pandas.loc[filtered_pandas.location.isin(clist)]
         if watch_date:
             check_valid_date(watch_date)
             mydate = pd.to_datetime(watch_date).date()
         else :
             mydate = filtered_pandas.date.max()
         filtered_pandas = filtered_pandas.loc[filtered_pandas.date==mydate].reset_index(drop=True)
         if selected_col:
             l = selected_col
         else:
             l=list(self.get_available_keys_words())
         l.insert(0, &#39;location&#39;)
         filtered_pandas = filtered_pandas[l]
         return filtered_pandas
    self.mainpandas = self.mainpandas.reset_index(drop=True)
    return self.mainpandas</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_posteriors"><code class="name flex">
<span>def <span class="ident">get_posteriors</span></span>(<span>self, sr, window=7, min_periods=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_posteriors(self,sr, window=7, min_periods=1):
     # We create an array for every possible value of Rt
     R_T_MAX = 12
     r_t_range = np.linspace(0, R_T_MAX, R_T_MAX*100+1)

     # Gamma is 1/serial interval
     # https://wwwnc.cdc.gov/eid/article/26/6/20-0357_article
     GAMMA = 1/7

     lam = sr[:-1].values * np.exp(GAMMA * (r_t_range[:, None] - 1))

     # Note: if you want to have a Uniform prior you can use the following line instead.
     # I chose the gamma distribution because of our prior knowledge of the likely value
     # of R_t.

     # prior0 = np.full(len(r_t_range), np.log(1/len(r_t_range)))
     prior0 = np.log(sps.gamma(a=3).pdf(r_t_range) + 1e-14)

     likelihoods = pd.DataFrame(
         # Short-hand way of concatenating the prior and likelihoods
         data = np.c_[prior0, sps.poisson.logpmf(sr[1:].values, lam)],
         index = r_t_range,
         columns = sr.index)

     # Perform a rolling sum of log likelihoods. This is the equivalent
     # of multiplying the original distributions. Exponentiate to move
     # out of log.
     posteriors = likelihoods.rolling(window,
                                  axis=1,
                                  min_periods=min_periods).sum()
     posteriors = np.exp(posteriors)

     # Normalize to 1.0
     posteriors = posteriors.div(posteriors.sum(axis=0), axis=1)

     return posteriors</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.get_stats"><code class="name flex">
<span>def <span class="ident">get_stats</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the pandas pandas_datase
- index: only an incremental value
- location: list of location used in the database selected (using geo standardization)
- 'which' :
return the keyword values selected from the avalailable keywords keepted seems
self.get_available_keys_words()</p>
<ul>
<li>'option' :default none<ul>
<li>'nonneg' In some cases negatives values can appeared due to a database updated, nonneg option
will smooth the curve during all the period considered</li>
<li>'nofillnan' if you do not want that NaN values are filled, which is the default behaviour</li>
<li>'smooth7' moving average, window of 7 days</li>
<li>'sumall' sum data over all locations</li>
</ul>
</li>
</ul>
<p>keys are keyswords from the selected database
location
| date
| keywords
|
daily
|
weekly
-----------------------------------------------------------------------
location1
|
1
|
val1-1
|
daily1-1
|
diff1-1
location1
|
2
|
val1-2
|
daily1-2
|
diff1-2
location1
|
3
|
val1-3
|
daily1-3
|
diff1-3
&hellip;
&hellip;
&hellip;
location1
| last-date |
val1-lastdate
|
cumul1-lastdate
|
diff1-lastdate
&hellip;
location-i
|
1
|
vali-1
|
dailyi-1
|
diffi-1
location-i
|
2
|
vali-1
|
daily1i-2
|
diffi-2
location-i
|
3
|
vali-1
|
daily1i-3
|
diffi-3
&hellip;</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_stats(self, **kwargs):
     &#39;&#39;&#39;
     Return the pandas pandas_datase
      - index: only an incremental value
      - location: list of location used in the database selected (using geo standardization)
      - &#39;which&#39; :  return the keyword values selected from the avalailable keywords keepted seems
         self.get_available_keys_words()

      - &#39;option&#39; :default none
         * &#39;nonneg&#39; In some cases negatives values can appeared due to a database updated, nonneg option
             will smooth the curve during all the period considered
         * &#39;nofillnan&#39; if you do not want that NaN values are filled, which is the default behaviour
         * &#39;smooth7&#39; moving average, window of 7 days
         * &#39;sumall&#39; sum data over all locations

     keys are keyswords from the selected database
             location        | date      | keywords          |  daily            |  weekly
             -----------------------------------------------------------------------
             location1       |    1      |  val1-1           |  daily1-1          |  diff1-1
             location1       |    2      |  val1-2           |  daily1-2          |  diff1-2
             location1       |    3      |  val1-3           |  daily1-3          |  diff1-3
                 ...             ...                     ...
             location1       | last-date |  val1-lastdate    |  cumul1-lastdate   |   diff1-lastdate
                 ...
             location-i      |    1      |  vali-1           |  dailyi-1          |  diffi-1
             location-i      |    2      |  vali-1           |  daily1i-2         |  diffi-2
             location-i      |    3      |  vali-1           |  daily1i-3         |  diffi-3
                 ...

     &#39;&#39;&#39;
     kwargs_test(kwargs,[&#39;location&#39;,&#39;which&#39;,&#39;option&#39;],
         &#39;Bad args used in the get_stats() function.&#39;)
     wallname = None
     if not &#39;location&#39; in kwargs or kwargs[&#39;location&#39;] is None.__class__ or kwargs[&#39;location&#39;] == None:
         if get_db_list_dict()[self.db][0] == &#39;WW&#39;:
             kwargs[&#39;location&#39;] = &#39;world&#39;
         else:
             kwargs[&#39;location&#39;] = self.slocation #self.geo_all[&#39;code_subregion&#39;].to_list()
         wallname = get_db_list_dict()[self.db][2]
     else:
         kwargs[&#39;location&#39;] = kwargs[&#39;location&#39;]

     option = kwargs.get(&#39;option&#39;, &#39;fillnan&#39;)
     fillnan = True # default
     sumall = False # default
     sumallandsmooth7 = False
     if kwargs[&#39;which&#39;] not in self.get_available_keys_words():
         raise CoaKeyError(kwargs[&#39;which&#39;]+&#39; is not a available for &#39; + self.db + &#39; database name. &#39;
         &#39;See get_available_keys_words() for the full list.&#39;)

     #while for last date all values are nan previous date
     mainpandas = self.return_nonan_dates_pandas(self.get_mainpandas(),kwargs[&#39;which&#39;])
     devorigclist = None
     origclistlist = None
     origlistlistloc = None
     if option and &#39;sumall&#39; in option:
         if not isinstance(kwargs[&#39;location&#39;], list):
             kwargs[&#39;location&#39;] = [[kwargs[&#39;location&#39;]]]
         else:
             if isinstance(kwargs[&#39;location&#39;][0], list):
                 kwargs[&#39;location&#39;] = kwargs[&#39;location&#39;]
             else:
                 kwargs[&#39;location&#39;] = [kwargs[&#39;location&#39;]]
     if not isinstance(kwargs[&#39;location&#39;], list):
         listloc = ([kwargs[&#39;location&#39;]]).copy()
         if not all(isinstance(c, str) for c in listloc):
             raise CoaWhereError(&#34;Location via the where keyword should be given as strings. &#34;)
         origclist = listloc
     else:
         listloc = (kwargs[&#39;location&#39;]).copy()
         origclist = listloc
         if any(isinstance(c, list) for c in listloc):
             if all(isinstance(c, list) for c in listloc):
                 origlistlistloc = listloc
             else:
                 raise CoaWhereError(&#34;In the case of sumall all locations must have the same types i.e\
                 list or string but both is not accepted, could be confusing&#34;)
     owid_name=&#39;&#39;
     if self.db_world:
         self.geo.set_standard(&#39;name&#39;)
         if origlistlistloc != None:
             #fulllist = [ i if isinstance(i, list) else [i] for i in origclist ]
             fulllist = []
             for deploy in origlistlistloc:
                 d=[]
                 for i in deploy:
                     if not self.geo.get_GeoRegion().is_region(i):
                         d.append(self.geo.to_standard(i,output=&#39;list&#39;,interpret_region=True)[0])
                     else:
                         d.append(self.geo.get_GeoRegion().is_region(i))
                 fulllist.append(d)
             dicooriglist = { &#39;,&#39;.join(i):self.geo.to_standard(i,output=&#39;list&#39;,interpret_region=True) for i in fulllist}
             location_exploded = list(dicooriglist.values())
         else:
             owid_name = [c for c in origclist if c.startswith(&#39;owid_&#39;)]
             clist = [c for c in origclist if not c.startswith(&#39;owid_&#39;)]
             location_exploded = self.geo.to_standard(listloc,output=&#39;list&#39;,interpret_region=True)
             if len(owid_name) !=0 :
                 location_exploded += owid_name
     else:
         def explosion(listloc,typeloc=&#39;subregion&#39;):
             exploded = []
             a=self.geo.get_data()
             for i in listloc:
                 if typeloc == &#39;subregion&#39;:
                     if self.geo.is_region(i):
                         i = [self.geo.is_region(i)]
                         tmp = self.geo.get_subregions_from_list_of_region_names(i,output=&#39;name&#39;)
                     elif self.geo.is_subregion(i):
                        tmp = self.geo.is_subregion(i)
                     else:
                         raise CoaTypeError(i + &#39;: not subregion nor region ... what is it ?&#39;)
                 elif typeloc == &#39;region&#39;:
                     tmp = self.geo.get_region_list()
                     if i.isdigit():
                         tmp = list(tmp.loc[tmp.code_region==i][&#39;name_region&#39;])
                     elif self.geo.is_region(i):
                         tmp = self.geo.get_regions_from_macroregion(name=i,output=&#39;name&#39;)
                         if get_db_list_dict()[self.db][0] in [&#39;USA, FRA, ESP, PRT&#39;]:
                             tmp = tmp[:-1]
                     else:
                         if self.geo.is_subregion(i):
                             raise CoaTypeError(i+ &#39; is a subregion ... not compatible with a region DB granularity?&#39;)
                         else:
                             raise CoaTypeError(i + &#39;: not subregion nor region ... what is it ?&#39;)
                 else:
                     raise CoaTypeError(&#39;Not subregion nor region requested, don\&#39;t know what to do ?&#39;)
                 if exploded:
                     exploded.append(tmp)
                 else:
                     exploded=[tmp]
             return DataBase.flat_list(exploded)

         if origlistlistloc != None:
             dicooriglist={&#39;,&#39;.join(i):explosion(i,self.database_type[self.db][1]) for i in origlistlistloc}
             #origlistlistloc = DataBase.flat_list(list(dicooriglist.values()))
             #location_exploded = origlistlistloc
         else:
             listloc = explosion(listloc,self.database_type[self.db][1])
             listloc = DataBase.flat_list(listloc)
             location_exploded = listloc
     def sticky(lname):
         if len(lname)&gt;0:
             tmp=&#39;&#39;
             for i in lname:
                 tmp += i+&#39;, &#39;
             lname=tmp[:-2]
         return [lname]

     pdcluster = pd.DataFrame()
     j=0

     if origlistlistloc != None:
         for k,v in dicooriglist.items():
             tmp  = mainpandas.copy()
             if any(isinstance(c, list) for c in v):
                 v=v[0]
             tmp = tmp.loc[tmp.location.isin(v)]
             code = tmp.codelocation.unique()
             tmp[&#39;clustername&#39;] = [k]*len(tmp)
             if pdcluster.empty:
                 pdcluster = tmp
             else:
                 pdcluster = pdcluster.append(tmp)
             j+=1
         pdfiltered = pdcluster[[&#39;location&#39;,&#39;date&#39;,&#39;codelocation&#39;,kwargs[&#39;which&#39;],&#39;clustername&#39;]]
     else:
         pdfiltered = mainpandas.loc[mainpandas.location.isin(location_exploded)]
         pdfiltered = pdfiltered[[&#39;location&#39;,&#39;date&#39;,&#39;codelocation&#39;,kwargs[&#39;which&#39;]]]
         pdfiltered[&#39;clustername&#39;] = pdfiltered[&#39;location&#39;].copy()
     if not isinstance(option,list):
         option=[option]
     if &#39;fillnan&#39; not in option and &#39;nofillnan&#39; not in option:
         option.insert(0, &#39;fillnan&#39;)
     if &#39;nonneg&#39; in option:
         option.remove(&#39;nonneg&#39;)
         option.insert(0, &#39;nonneg&#39;)
     if &#39;smooth7&#39; in  option and &#39;sumall&#39; in  option:
         option.remove(&#39;sumall&#39;)
         option.remove(&#39;smooth7&#39;)
         option+=[&#39;sumallandsmooth7&#39;]
     for o in option:
         if o == &#39;nonneg&#39;:
             if kwargs[&#39;which&#39;].startswith(&#39;cur_&#39;):
                 raise CoaKeyError(&#39;The option nonneg cannot be used with instantaneous data, such as cur_ which variables.&#39;)
             cluster=list(pdfiltered.clustername.unique())
             separated = [ pdfiltered.loc[pdfiltered.clustername==i] for i in cluster]
             reconstructed = pd.DataFrame()
             for sub in separated:
                 location = list(sub.location.unique())
                 for loca in location:
                     pdloc = sub.loc[sub.location == loca][kwargs[&#39;which&#39;]]
                     try:
                         y0=pdloc.values[0] # integrated offset at t=0
                     except:
                         y0=0
                     if np.isnan(y0):
                         y0=0
                     pa = pdloc.diff()
                     yy = pa.values
                     ind = list(pa.index)
                     where_nan = np.isnan(yy)
                     yy[where_nan] = 0.
                     indices=np.where(yy &lt; 0)[0]
                     for kk in np.where(yy &lt; 0)[0]:
                         k = int(kk)
                         val_to_repart = -yy[k]
                         if k &lt; np.size(yy)-1:
                             yy[k] = (yy[k+1]+yy[k-1])/2
                         else:
                             yy[k] = yy[k-1]
                         val_to_repart = val_to_repart + yy[k]
                         s = np.nansum(yy[0:k])
                         if not any([i !=0 for i in yy[0:k]]) == True and s == 0:
                             yy[0:k] = 0.
                         elif s == 0:
                             yy[0:k] = np.nan*np.ones(k)
                         else:
                             yy[0:k] = yy[0:k]*(1-float(val_to_repart)/s)
                     sub=sub.copy()
                     sub.loc[ind,kwargs[&#39;which&#39;]]=np.cumsum(yy)+y0 # do not forget the offset
                 if reconstructed.empty:
                     reconstructed = sub
                 else:
                     reconstructed=reconstructed.append(sub)
                 pdfiltered = reconstructed
         elif o == &#39;nofillnan&#39;:
             pdfiltered_nofillnan = pdfiltered.copy().reset_index(drop=True)
             fillnan=False
         elif o == &#39;fillnan&#39;:
             fillnan=True
             # fill with previous value
             pdfiltered = pdfiltered.reset_index(drop=True)
             pdfiltered_nofillnan = pdfiltered.copy()

             pdfiltered.loc[:,kwargs[&#39;which&#39;]] =\
             pdfiltered.groupby([&#39;location&#39;,&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.bfill())
             #if kwargs[&#39;which&#39;].startswith(&#39;total_&#39;) or kwargs[&#39;which&#39;].startswith(&#39;tot_&#39;):
             #    pdfiltered.loc[:,kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.ffill())
             if pdfiltered.loc[pdfiltered.date == pdfiltered.date.max()][kwargs[&#39;which&#39;]].isnull().values.any():
                 print(kwargs[&#39;which&#39;], &#34;has been selected. Some missing data has been interpolated from previous data.&#34;)
                 print(&#34;This warning appear right now due to some missing values at the latest date &#34;, pdfiltered.date.max(),&#34;.&#34;)
                 print(&#34;Use the option=&#39;nofillnan&#39; if you want to only display the original data&#34;)
                 pdfiltered.loc[:,kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;location&#39;,&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.ffill())
                 pdfiltered = pdfiltered[pdfiltered[kwargs[&#39;which&#39;]].notna()]
         elif o == &#39;smooth7&#39;:
             pdfiltered[kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;location&#39;])[kwargs[&#39;which&#39;]].rolling(7,min_periods=7).mean().reset_index(level=0,drop=True)
             inx7=pdfiltered.groupby(&#39;location&#39;).head(7).index
             pdfiltered.loc[inx7, kwargs[&#39;which&#39;]] = pdfiltered[kwargs[&#39;which&#39;]].fillna(method=&#34;bfill&#34;)
             fillnan=True
         elif o == &#39;sumall&#39;:
             sumall = True
         elif o == &#39;sumallandsmooth7&#39;:
             sumall = True
             sumallandsmooth7 = True
         elif o != None and o != &#39;&#39; and o != &#39;sumallandsmooth7&#39;:
             raise CoaKeyError(&#39;The option &#39;+o+&#39; is not recognized in get_stats. See get_available_options() for list.&#39;)
     pdfiltered = pdfiltered.reset_index(drop=True)

     # if sumall set, return only integrate val
     tmppandas=pd.DataFrame()
     if sumall:
         if origlistlistloc != None:
            uniqcluster = pdfiltered.clustername.unique()
            if kwargs[&#39;which&#39;].startswith(&#39;cur_idx_&#39;):
               tmp = pdfiltered.groupby([&#39;clustername&#39;,&#39;date&#39;]).mean().reset_index()
            else:
               tmp = pdfiltered.groupby([&#39;clustername&#39;,&#39;date&#39;]).sum().reset_index()#.loc[pdfiltered.clustername.isin(uniqcluster)].\

            codescluster = {i:list(pdfiltered.loc[pdfiltered.clustername==i][&#39;codelocation&#39;].unique()) for i in uniqcluster}
            namescluster = {i:list(pdfiltered.loc[pdfiltered.clustername==i][&#39;location&#39;].unique()) for i in uniqcluster}
            tmp[&#39;codelocation&#39;] = tmp[&#39;clustername&#39;].map(codescluster)
            tmp[&#39;location&#39;] = tmp[&#39;clustername&#39;].map(namescluster)

            pdfiltered = tmp
            pdfiltered = pdfiltered.drop_duplicates([&#39;date&#39;,&#39;clustername&#39;])
            if sumallandsmooth7:
                pdfiltered[kwargs[&#39;which&#39;]] = pdfiltered.groupby([&#39;clustername&#39;])[kwargs[&#39;which&#39;]].rolling(7,min_periods=7).mean().reset_index(level=0,drop=True)
                pdfiltered.loc[:,kwargs[&#39;which&#39;]] =\
                pdfiltered.groupby([&#39;clustername&#39;])[kwargs[&#39;which&#39;]].apply(lambda x: x.bfill())
         # computing daily, cumul and weekly
         else:
             if kwargs[&#39;which&#39;].startswith(&#39;cur_idx_&#39;):
                 tmp = pdfiltered.groupby([&#39;date&#39;]).mean().reset_index()
             else:
                 tmp = pdfiltered.groupby([&#39;date&#39;]).sum().reset_index()
             uniqloc = list(pdfiltered.location.unique())
             uniqcodeloc = list(pdfiltered.codelocation.unique())
             tmp.loc[:,&#39;location&#39;] = [&#39;dummy&#39;]*len(tmp)
             tmp.loc[:,&#39;codelocation&#39;] = [&#39;dummy&#39;]*len(tmp)
             tmp.loc[:,&#39;clustername&#39;] = [&#39;dummy&#39;]*len(tmp)
             for i in range(len(tmp)):
                 tmp.at[i,&#39;location&#39;] = uniqloc #sticky(uniqloc)
                 tmp.at[i,&#39;codelocation&#39;] = uniqcodeloc #sticky(uniqcodeloc)
                 tmp.at[i,&#39;clustername&#39;] =  sticky(uniqloc)[0]
             pdfiltered = tmp
     else:
         if self.db_world :
             pdfiltered[&#39;clustername&#39;] = pdfiltered[&#39;location&#39;].apply(lambda x: self.geo.to_standard(x)[0] if not x.startswith(&#34;owid_&#34;) else x)
         else:
             pdfiltered[&#39;clustername&#39;] = pdfiltered[&#39;location&#39;]

     if &#39;cur_&#39; in kwargs[&#39;which&#39;] or &#39;total_&#39; in kwargs[&#39;which&#39;] or &#39;tot_&#39; in kwargs[&#39;which&#39;]:
         pdfiltered[&#39;cumul&#39;] = pdfiltered[kwargs[&#39;which&#39;]]
     else:
         pdfiltered[&#39;cumul&#39;] = pdfiltered_nofillnan.groupby(&#39;clustername&#39;)[kwargs[&#39;which&#39;]].cumsum()
         if fillnan:
             pdfiltered.loc[:,&#39;cumul&#39;] =\
             pdfiltered.groupby(&#39;clustername&#39;)[&#39;cumul&#39;].apply(lambda x: x.ffill())

     pdfiltered[&#39;daily&#39;] = pdfiltered.groupby(&#39;clustername&#39;)[&#39;cumul&#39;].diff()
     inx = pdfiltered.groupby(&#39;clustername&#39;).head(1).index
     pdfiltered[&#39;weekly&#39;] = pdfiltered.groupby(&#39;clustername&#39;)[&#39;cumul&#39;].diff(7)
     inx7=pdfiltered.groupby(&#39;clustername&#39;).head(7).index
     #First value of diff is always NaN
     pdfiltered.loc[inx, &#39;daily&#39;] = pdfiltered[&#39;daily&#39;].fillna(method=&#34;bfill&#34;)
     pdfiltered.loc[inx7, &#39;weekly&#39;] = pdfiltered[&#39;weekly&#39;].fillna(method=&#34;bfill&#34;)

     unifiedposition=[&#39;location&#39;, &#39;date&#39;, kwargs[&#39;which&#39;], &#39;daily&#39;, &#39;cumul&#39;, &#39;weekly&#39;, &#39;codelocation&#39;,&#39;clustername&#39;]
     pdfiltered = pdfiltered[unifiedposition]

     if wallname != None and sumall == True:
            pdfiltered.loc[:,&#39;clustername&#39;] = wallname

     pdfiltered = pdfiltered.drop(columns=&#39;cumul&#39;)
     verb(&#34;Here the information I\&#39;ve got on &#34;, kwargs[&#39;which&#39;],&#34; : &#34;, self.get_keyword_definition(kwargs[&#39;which&#39;]))
     return pdfiltered</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.merger"><code class="name flex">
<span>def <span class="ident">merger</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Merge two or more pycoa pandas from get_stats operation
'coapandas': list (min 2D) of pandas from stats</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merger(self,**kwargs):
     &#39;&#39;&#39;
     Merge two or more pycoa pandas from get_stats operation
     &#39;coapandas&#39;: list (min 2D) of pandas from stats
     &#39;&#39;&#39;

     coapandas = kwargs.get(&#39;coapandas&#39;, None)

     if coapandas is None or not isinstance(coapandas, list) or len(coapandas)&lt;=1:
         raise CoaKeyError(&#39;coapandas value must be at least a list of 2 elements ... &#39;)

     def renamecol(pandy):
         torename=[&#39;daily&#39;,&#39;cumul&#39;,&#39;weekly&#39;]
         return pandy.rename(columns={i:pandy.columns[2]+&#39;_&#39;+i  for i in torename})
     base = coapandas[0].copy()
     coapandas = [ renamecol(p) for p in coapandas ]
     base = coapandas[0].copy()
     if not &#39;clustername&#39; in base.columns:
         raise CoaKeyError(&#39;No &#34;clustername&#34; in your pandas columns ... don\&#39;t know what to do &#39;)

     j=1
     for p in coapandas[1:]:
         [ p.drop([i],axis=1, inplace=True) for i in [&#39;location&#39;,&#39;where&#39;,&#39;codelocation&#39;] if i in p.columns ]
         #p.drop([&#39;location&#39;,&#39;codelocation&#39;],axis=1, inplace=True)
         base = pd.merge(base,p,on=[&#39;date&#39;,&#39;clustername&#39;],how=&#34;inner&#34;)#,suffixes=(&#39;&#39;, &#39;_drop&#39;))
         #base.drop([col for col in base.columns if &#39;drop&#39; in col], axis=1, inplace=True)
     return base</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.return_jhu_pandas"><code class="name flex">
<span>def <span class="ident">return_jhu_pandas</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>For center for Systems Science and Engineering (CSSE) at Johns Hopkins University
COVID-19 Data Repository by the see homepage: <a href="https://github.com/CSSEGISandData/COVID-19">https://github.com/CSSEGISandData/COVID-19</a>
return a structure : pandas location - date - keywords
for jhu location are countries (location uses geo standard)
for jhu-usa location are Province_State (location uses geo standard)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def return_jhu_pandas(self):
     &#39;&#39;&#39; For center for Systems Science and Engineering (CSSE) at Johns Hopkins University
         COVID-19 Data Repository by the see homepage: https://github.com/CSSEGISandData/COVID-19
         return a structure : pandas location - date - keywords
         for jhu location are countries (location uses geo standard)
         for jhu-usa location are Province_State (location uses geo standard)
         &#39;&#39;&#39;
     base_url = &#34;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/&#34;+\
                             &#34;csse_covid_19_data/csse_covid_19_time_series/&#34;
     base_name = &#34;time_series_covid19_&#34;
     # previous are default for actual jhu db

     pandas_jhu = {}

     if self.db == &#39;jhu&#39;: # worldwide
         extension =  &#34;_global.csv&#34;
         jhu_files_ext = [&#39;deaths&#39;, &#39;confirmed&#39;]
     elif self.db == &#39;jhu-usa&#39;: # &#39;USA&#39;
         extension = &#34;_US.csv&#34;
         jhu_files_ext = [&#39;deaths&#39;,&#39;confirmed&#39;]
     elif self.db == &#39;rki&#39;: # &#39;DEU&#39;
         base_url = &#39;https://github.com/jgehrcke/covid-19-germany-gae/raw/master/&#39;
         jhu_files_ext = [&#39;deaths&#39;,&#39;cases&#39;]
         extension = &#39;-rki-by-ags.csv&#39;
         base_name = &#39;&#39;
     elif self.db == &#39;imed&#39;: # &#39;GRC&#39;
         base_url = &#39;https://raw.githubusercontent.com/iMEdD-Lab/open-data/master/COVID-19/greece_&#39;
         jhu_files_ext = [&#39;deaths&#39;,&#39;cases&#39;]
         extension = &#39;_v2.csv&#39;
         base_name = &#39;&#39;
     else:
         raise CoaDbError(&#39;Unknown JHU like db &#39;+str(self.db))

     self.available_keys_words = []
     if self.db == &#39;rki&#39;:
             self.available_keys_words = [&#39;tot_deaths&#39;,&#39;tot_cases&#39;]
     pandas_list = []
     for ext in jhu_files_ext:
         fileName = base_name + ext + extension
         url = base_url + fileName
         self.database_url.append(url)
         pandas_jhu_db = pandas.read_csv(get_local_from_url(url,7200), sep = &#39;,&#39;) # cached for 2 hours
         if self.db == &#39;jhu&#39;:
             pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;Country/Region&#39;:&#39;location&#39;})
             pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;Province/State&#39;,&#39;Lat&#39;,&#39;Long&#39;])
             pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
             pandas_jhu_db = pandas_jhu_db.loc[~pandas_jhu_db.location.isin([&#39;Diamond Princess&#39;])]
         elif self.db == &#39;jhu-usa&#39;:
             pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;Province_State&#39;:&#39;location&#39;})
             pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;UID&#39;,&#39;iso2&#39;,&#39;iso3&#39;,&#39;code3&#39;,&#39;FIPS&#39;,
                                 &#39;Admin2&#39;,&#39;Country_Region&#39;,&#39;Lat&#39;,&#39;Long_&#39;,&#39;Combined_Key&#39;])
             if &#39;Population&#39; in pandas_jhu_db.columns:
                 pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;,&#39;Population&#39;],var_name=&#34;date&#34;,value_name=ext)
             else:
                 pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
             removethose=[&#39;American Samoa&#39;,&#39;Diamond Princess&#39;,&#39;Grand Princess&#39;,&#39;Guam&#39;,
             &#39;Northern Mariana Islands&#39;,&#39;Puerto Rico&#39;,&#39;Virgin Islands&#39;]
             pandas_jhu_db = pandas_jhu_db.loc[~pandas_jhu_db.location.isin(removethose)]
         elif self.db == &#39;rki&#39;:
             pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;sum_&#39;+ext])
             pandas_jhu_db = pandas_jhu_db.set_index(&#39;time_iso8601&#39;).T.reset_index().rename(columns={&#39;index&#39;:&#39;location&#39;})
             pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
             pandas_jhu_db[&#39;location&#39;] = pandas_jhu_db.location.astype(str)
             pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;deaths&#39;:&#39;tot_deaths&#39;,&#39;cases&#39;:&#39;tot_cases&#39;})
         elif self.db == &#39;imed&#39;:
             pandas_jhu_db = pandas_jhu_db.rename(columns={&#39;county_normalized&#39;:&#39;location&#39;})
             pandas_jhu_db = pandas_jhu_db.drop(columns=[&#39;Γεωγραφικό Διαμέρισμα&#39;,&#39;Περιφέρεια&#39;,&#39;county&#39;,&#39;pop_11&#39;])
             ext=&#39;tot_&#39;+ext
             pandas_jhu_db = pandas_jhu_db.melt(id_vars=[&#34;location&#34;],var_name=&#34;date&#34;,value_name=ext)
             self.available_keys_words += [ext]
         else:
             raise CoaTypeError(&#39;jhu nor jhu-usa database selected ... &#39;)

         pandas_jhu_db=pandas_jhu_db.groupby([&#39;location&#39;,&#39;date&#39;]).sum().reset_index()
         pandas_list.append(pandas_jhu_db)

     if &#39;jhu&#39; in self.db:
         pandas_list = [pan.rename(columns={i:&#39;tot_&#39;+i for i in jhu_files_ext}) for pan in pandas_list]
         self.available_keys_words = [&#39;tot_&#39;+i for i in jhu_files_ext]
     uniqloc = list(pandas_list[0][&#39;location&#39;].unique())
     oldloc = uniqloc
     codedico={}
     toremove = None
     newloc = None
     location_is_code = False
     if self.db_world:
         d_loc_s = collections.OrderedDict(zip(uniqloc,self.geo.to_standard(uniqloc,output=&#39;list&#39;,db=self.get_db(),interpret_region=True)))
         self.slocation = list(d_loc_s.values())
         g=coge.GeoManager(&#39;iso3&#39;)
         codename = collections.OrderedDict(zip(self.slocation,g.to_standard(self.slocation,output=&#39;list&#39;,db=self.get_db(),interpret_region=True)))
     else:
         if self.database_type[self.db][1] == &#39;subregion&#39;:
             pdcodename = self.geo.get_subregion_list()
             self.slocation = uniqloc
             codename = collections.OrderedDict(zip(self.slocation,list(pdcodename.loc[pdcodename.code_subregion.isin(self.slocation)][&#39;name_subregion&#39;])))
             if self.db == &#39;jhu-usa&#39;:
                 d_loc_s = collections.OrderedDict(zip(uniqloc,list(pdcodename.loc[pdcodename.name_subregion.isin(uniqloc)][&#39;code_subregion&#39;])))
                 self.slocation = list(d_loc_s.keys())
                 codename = d_loc_s
             if self.db == &#39;rki&#39;:
                 d_loc_s = collections.OrderedDict(zip(uniqloc,list(pdcodename.loc[pdcodename.code_subregion.isin(uniqloc)][&#39;name_subregion&#39;])))
                 self.slocation = list(d_loc_s.values())
                 codename = d_loc_s
                 location_is_code = True
                 def notuse():
                     count_values=collections.Counter(d_loc_s.values())
                     duplicates_location = list({k:v for k,v in count_values.items() if v&gt;1}.keys())
                     def findkeywithvalue(dico,what):
                         a=[]
                         for k,v in dico.items():
                             if v == what:
                                 a.append(k)
                         return a
                     codedupli={i:findkeywithvalue(d_loc_s,i) for i in duplicates_location}
         elif self.database_type[self.db][1] == &#39;region&#39;:
             codename = self.geo.get_data().set_index(&#39;name_region&#39;)[&#39;code_region&#39;].to_dict()
             self.slocation = list(codename.keys())


     result = reduce(lambda x, y: pd.merge(x, y, on = [&#39;location&#39;,&#39;date&#39;]), pandas_list)

     if location_is_code:
         result[&#39;codelocation&#39;] = result[&#39;location&#39;]
         result[&#39;location&#39;] = result[&#39;location&#39;].map(codename)
     else:
         if self.db == &#39;jhu&#39;:
             result[&#39;location&#39;] = result[&#39;location&#39;].map(d_loc_s)
         result[&#39;codelocation&#39;] = result[&#39;location&#39;].map(codename)
     result = result.loc[result.location.isin(self.slocation)]

     tmp = pd.DataFrame()
     if &#39;Kosovo&#39; in uniqloc:
         #Kosovo is Serbia ! with geo.to_standard
         tmp=(result.loc[result.location.isin([&#39;Serbia&#39;])]).groupby(&#39;date&#39;).sum().reset_index()
         tmp[&#39;location&#39;] = &#39;Serbia&#39;
         tmp[&#39;codelocation&#39;] = &#39;SRB&#39;
         kw = [i for i in self.available_keys_words]
         colpos=[&#39;location&#39;, &#39;date&#39;] + kw + [&#39;codelocation&#39;]
         tmp = tmp[colpos]
         result = result.loc[~result.location.isin([&#39;Serbia&#39;])]
         result = result.append(tmp)

     result[&#39;date&#39;] = pd.to_datetime(result[&#39;date&#39;],errors=&#39;coerce&#39;).dt.date
     result = result.sort_values(by=[&#39;location&#39;,&#39;date&#39;])
     result = result.reset_index(drop=True)
     self.mainpandas = fill_missing_dates(result)
     self.dates  = self.mainpandas[&#39;date&#39;]</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.return_nonan_dates_pandas"><code class="name flex">
<span>def <span class="ident">return_nonan_dates_pandas</span></span>(<span>self, df=None, field=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Check if for last date all values are nan, if yes check previous date and loop until false</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def return_nonan_dates_pandas(self, df = None, field = None):
      &#39;&#39;&#39; Check if for last date all values are nan, if yes check previous date and loop until false&#39;&#39;&#39;
      watchdate = df.date.max()
      boolval = True
      j = 0
      while (boolval):
          boolval = df.loc[df.date == (watchdate - dt.timedelta(days=j))][field].dropna().empty
          j += 1
      df = df.loc[df.date &lt;= watchdate - dt.timedelta(days=j - 1)]
      boolval = True
      j = 0
      watchdate = df.date.min()
      while (boolval):
          boolval = df.loc[df.date == (watchdate + dt.timedelta(days=j))][field].dropna().empty
          j += 1
      df = df.loc[df.date &gt;= watchdate - dt.timedelta(days=j - 1)]
      return df</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.return_structured_pandas"><code class="name flex">
<span>def <span class="ident">return_structured_pandas</span></span>(<span>self, mypandas, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the mainpandas core of the PyCoA structure</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def return_structured_pandas(self,mypandas,**kwargs):
     &#39;&#39;&#39;
     Return the mainpandas core of the PyCoA structure
     &#39;&#39;&#39;
     kwargs_test(kwargs,[&#39;columns_skipped&#39;,&#39;columns_keeped&#39;],
         &#39;Bad args used in the return_structured_pandas function.&#39;)
     columns_skipped = kwargs.get(&#39;columns_skipped&#39;, None)
     absolutlyneeded = [&#39;date&#39;,&#39;location&#39;]
     defaultkeept = list(set(mypandas.columns.to_list()) - set(absolutlyneeded))
     columns_keeped  = kwargs.get(&#39;columns_keeped&#39;, defaultkeept)
     if columns_skipped:
         columns_keeped = [x for x in mypandas.columns.values.tolist() if x not in columns_skipped + absolutlyneeded]
     mypandas = mypandas[absolutlyneeded + columns_keeped]

     self.available_keys_words = columns_keeped #+ absolutlyneeded
     not_un_nation_dict={&#39;Kosovo&#39;:&#39;Serbia&#39;}
     for subpart_country, main_country in not_un_nation_dict.items() :
         tmp=(mypandas.loc[mypandas.location.isin([subpart_country,main_country])].groupby(&#39;date&#39;).sum())
         tmp[&#39;location&#39;]=main_country
         mypandas = mypandas.loc[~mypandas.location.isin([subpart_country,main_country])]
         tmp = tmp.reset_index()
         cols = tmp.columns.tolist()
         cols = cols[0:1] + cols[-1:] + cols[1:-1]
         tmp = tmp[cols]
         mypandas = mypandas.append(tmp)
     if &#39;iso_code&#39; in mypandas.columns:
         mypandas[&#39;iso_code&#39;] = mypandas[&#39;iso_code&#39;].dropna().astype(str)
         mypandasori=mypandas.copy()
         strangeiso3tokick = [i for i in mypandasori[&#39;iso_code&#39;].dropna().unique() if not len(i)==3 ]
         mypandasori = mypandas.loc[~mypandas.iso_code.isin(strangeiso3tokick)]
         self.available_keys_words.remove(&#39;iso_code&#39;)
         mypandasori = mypandasori.drop(columns=[&#39;location&#39;])
         mypandasori = mypandasori.rename(columns={&#39;iso_code&#39;:&#39;location&#39;})
         if self.db == &#39;owid&#39;:
             onlyowid = mypandas.loc[mypandas.iso_code.isin(strangeiso3tokick)]
             onlyowid = onlyowid.copy()
             onlyowid.loc[:,&#39;location&#39;] = onlyowid[&#39;location&#39;].apply(lambda x : &#39;owid_&#39;+x)
         mypandas = mypandasori

     if self.db == &#39;dpc&#39;:
         gd = self.geo.get_data()[[&#39;name_region&#39;,&#39;code_region&#39;]]
         A=[&#39;P.A. Bolzano&#39;,&#39;P.A. Trento&#39;]
         tmp=mypandas.loc[mypandas.location.isin(A)].groupby(&#39;date&#39;).sum()
         tmp[&#39;location&#39;]=&#39;Trentino-Alto Adige&#39;
         mypandas = mypandas.loc[~mypandas.location.isin(A)]
         tmp = tmp.reset_index()
         mypandas = mypandas.append(tmp)
         uniqloc = list(mypandas[&#39;location&#39;].unique())
         sub2reg = dict(gd.values)
         #collections.OrderedDict(zip(uniqloc,list(gd.loc[gd.name_region.isin(uniqloc)][&#39;code_region&#39;])))
         mypandas[&#39;codelocation&#39;] = mypandas[&#39;location&#39;].map(sub2reg)
     if self.db == &#39;dgs&#39;:
         gd = self.geo.get_data()[[&#39;name_region&#39;,&#39;name_region&#39;]]
         mypandas = mypandas.reset_index(drop=True)
         mypandas[&#39;location&#39;] = mypandas[&#39;location&#39;].apply(lambda x: x.title().replace(&#39;Do&#39;, &#39;do&#39;).replace(&#39;Da&#39;,&#39;da&#39;).replace(&#39;De&#39;,&#39;de&#39;))
         uniqloc = list(mypandas[&#39;location&#39;].unique())
         sub2reg = dict(gd.values)
         #sub2reg = collections.OrderedDict(zip(uniqloc,list(gd.loc[gd.name_subregion.isin(uniqloc)][&#39;name_region&#39;])))
         mypandas[&#39;location&#39;] = mypandas[&#39;location&#39;].map(sub2reg)
         mypandas = mypandas.loc[~mypandas.location.isnull()]

      # filling subregions.
         gd = self.geo.get_data()[[&#39;code_region&#39;,&#39;name_region&#39;]]
         uniqloc = list(mypandas[&#39;location&#39;].unique())
         name2code = collections.OrderedDict(zip(uniqloc,list(gd.loc[gd.name_region.isin(uniqloc)][&#39;code_region&#39;])))
         mypandas = mypandas.loc[~mypandas.location.isnull()]

     codename = None
     location_is_code = False
     uniqloc = list(mypandas[&#39;location&#39;].unique()) # if possible location from csv are codelocation

     if self.db_world:
         uniqloc = [s for s in uniqloc if &#39;OWID_&#39; not in s]
         db=self.get_db()
         if self.db == &#39;govcy&#39;:
             db=None
         codename = collections.OrderedDict(zip(uniqloc,self.geo.to_standard(uniqloc,output=&#39;list&#39;,db=db,interpret_region=True)))
         self.slocation = list(codename.values())
         location_is_code = True
     else:
         if self.database_type[self.db][1] == &#39;region&#39; :
             if self.db == &#39;covid19india&#39;:
                 mypandas = mypandas.loc[~mypandas.location.isnull()]
                 uniqloc = list(mypandas[&#39;location&#39;].unique())
             temp = self.geo.get_region_list()[[&#39;name_region&#39;,&#39;code_region&#39;]]
             #codename = collections.OrderedDict(zip(uniqloc,list(temp.loc[temp.name_region.isin(uniqloc)][&#39;code_region&#39;])))
             codename=dict(temp.values)
             self.slocation = uniqloc
             if self.db == &#39;obepine&#39;:
                 codename = {v:k for k,v in codename.items()}
                 location_is_code = True

         elif self.database_type[self.db][1] == &#39;subregion&#39;:
             temp = self.geo_all[[&#39;code_subregion&#39;,&#39;name_subregion&#39;]]
             codename=dict(temp.loc[temp.code_subregion.isin(uniqloc)].values)
             if self.db in [&#39;phe&#39;,&#39;covidtracking&#39;,&#39;spf&#39;,&#39;escovid19data&#39;,&#39;opencovid19&#39;,&#39;minciencia&#39;,&#39;moh&#39;,&#39;risklayer&#39;,&#39;insee&#39;]:
                 #codename={i:list(temp.loc[temp.code_subregion.isin([i])][&#39;name_subregion&#39;])[0] for i in uniqloc if not temp.loc[temp.code_subregion.isin([i])][&#39;name_subregion&#39;].empty }
                 #codename = collections.OrderedDict(zip(uniqloc,list(temp.loc[temp.code_subregion.isin(uniqloc)][&#39;name_subregion&#39;])))
                 self.slocation = list(codename.values())
                 location_is_code = True
             else:
                 #codename=dict(temp.loc[temp.code_subregion.isin(uniqloc)][[&#39;code_subregion&#39;,&#39;name_subregion&#39;]].values)
                 #codename={i:list(temp.loc[temp.code_subregion.isin([i])][&#39;code_subregion&#39;])[0] for i in uniqloc if not temp.loc[temp.code_subregion.isin([i])][&#39;code_subregion&#39;].empty }
                 #codename = collections.OrderedDict(zip(uniqloc,list(temp.loc[temp.name_subregion.isin(uniqloc)][&#39;code_subregion&#39;])))
                 #print(codename)
                 self.slocation = uniqloc
         else:
             CoaDbError(&#39;Granularity problem , neither region nor sub_region ...&#39;)

     if self.db == &#39;dgs&#39;:
         mypandas = mypandas.reset_index(drop=True)

     if self.db != &#39;spfnational&#39;:
         mypandas = mypandas.groupby([&#39;location&#39;,&#39;date&#39;]).sum(min_count=1).reset_index() # summing in case of multiple dates (e.g. in opencovid19 data). But keep nan if any

     if self.db == &#39;govcy&#39;:
         location_is_code=False

     mypandas = fill_missing_dates(mypandas)

     if location_is_code:
         if self.db != &#39;dgs&#39;:
             mypandas[&#39;codelocation&#39;] =  mypandas[&#39;location&#39;].astype(str)
         mypandas[&#39;location&#39;] = mypandas[&#39;location&#39;].map(codename)
         if self.db == &#39;obepine&#39;:
             mypandas = mypandas.dropna(subset=[&#39;location&#39;])
             self.slocation = list(mypandas.codelocation.unique())
         mypandas = mypandas.loc[~mypandas.location.isnull()]
     else:
         mypandas[&#39;codelocation&#39;] =  mypandas[&#39;location&#39;].map(codename).astype(str)
     if self.db == &#39;owid&#39;:
         onlyowid[&#39;codelocation&#39;] = onlyowid[&#39;location&#39;]
         mypandas = mypandas.append(onlyowid)
     self.mainpandas  = mypandas
     self.dates  = self.mainpandas[&#39;date&#39;]</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.saveoutput"><code class="name flex">
<span>def <span class="ident">saveoutput</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>saveoutput pycoas pandas as an
output file selected by output argument
'pandas': pycoa pandas
'saveformat': excel or csv (default excel)
'savename': pycoaout (default)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def saveoutput(self,**kwargs):
    &#39;&#39;&#39;
    saveoutput pycoas pandas as an  output file selected by output argument
    &#39;pandas&#39;: pycoa pandas
    &#39;saveformat&#39;: excel or csv (default excel)
    &#39;savename&#39;: pycoaout (default)
    &#39;&#39;&#39;
    possibleformat=[&#39;excel&#39;,&#39;csv&#39;]
    saveformat = &#39;excel&#39;
    savename = &#39;pycoaout&#39;
    pandyori = &#39;&#39;
    if &#39;saveformat&#39; in kwargs:
         saveformat = kwargs[&#39;saveformat&#39;]
    if saveformat not in possibleformat:
        raise CoaKeyError(&#39;Output option &#39;+saveformat+&#39; is not recognized.&#39;)
    if &#39;savename&#39; in kwargs and kwargs[&#39;savename&#39;] != &#39;&#39;:
       savename = kwargs[&#39;savename&#39;]

    if not &#39;pandas&#39; in kwargs:
       raise CoaKeyError(&#39;Absolute needed variable : the pandas desired &#39;)
    else:
       pandyori = kwargs[&#39;pandas&#39;]
    pandy = pandyori
    pandy[&#39;date&#39;] = pd.to_datetime(pandy[&#39;date&#39;])
    pandy[&#39;date&#39;]=pandy[&#39;date&#39;].apply(lambda x: x.strftime(&#39;%Y-%m-%d&#39;))
    if saveformat == &#39;excel&#39;:
        pandy.to_excel(savename+&#39;.xlsx&#39;,index=False, na_rep=&#39;NAN&#39;)
    elif saveformat == &#39;csv&#39;:
        pandy.to_csv(savename+&#39;.csv&#39;, encoding=&#39;utf-8&#39;, index=False, float_format=&#39;%.4f&#39;,na_rep=&#39;NAN&#39;)</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.set_display"><code class="name flex">
<span>def <span class="ident">set_display</span></span>(<span>self, db, geo)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the CocoDisplay</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_display(self,db,geo):
    &#39;&#39;&#39; Set the CocoDisplay &#39;&#39;&#39;
    self.codisp = codisplay.CocoDisplay(db, geo)</code></pre>
</details>
</dd>
<dt id="coa.covid19.DataBase.smooth_cases"><code class="name flex">
<span>def <span class="ident">smooth_cases</span></span>(<span>self, cases)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def smooth_cases(self,cases):
     new_cases = cases

     smoothed = new_cases.rolling(7,
         win_type=&#39;gaussian&#39;,
         min_periods=1,
         center=True).mean(std=2).round()
         #center=False).mean(std=2).round()

     zeros = smoothed.index[smoothed.eq(0)]
     if len(zeros) == 0:
         idx_start = 0
     else:
         last_zero = zeros.max()
         idx_start = smoothed.index.get_loc(last_zero) + 1
     smoothed = smoothed.iloc[idx_start:]
     original = new_cases.loc[smoothed.index]

     return smoothed</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#about">About :</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="coa" href="index.html">coa</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="coa.covid19.DataBase" href="#coa.covid19.DataBase">DataBase</a></code></h4>
<ul class="">
<li><code><a title="coa.covid19.DataBase.appender" href="#coa.covid19.DataBase.appender">appender</a></code></li>
<li><code><a title="coa.covid19.DataBase.csv2pandas" href="#coa.covid19.DataBase.csv2pandas">csv2pandas</a></code></li>
<li><code><a title="coa.covid19.DataBase.factory" href="#coa.covid19.DataBase.factory">factory</a></code></li>
<li><code><a title="coa.covid19.DataBase.flat_list" href="#coa.covid19.DataBase.flat_list">flat_list</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_available_database" href="#coa.covid19.DataBase.get_available_database">get_available_database</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_available_keys_words" href="#coa.covid19.DataBase.get_available_keys_words">get_available_keys_words</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_available_options" href="#coa.covid19.DataBase.get_available_options">get_available_options</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_dates" href="#coa.covid19.DataBase.get_dates">get_dates</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_db" href="#coa.covid19.DataBase.get_db">get_db</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_display" href="#coa.covid19.DataBase.get_display">get_display</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_keyword_definition" href="#coa.covid19.DataBase.get_keyword_definition">get_keyword_definition</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_keyword_url" href="#coa.covid19.DataBase.get_keyword_url">get_keyword_url</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_locations" href="#coa.covid19.DataBase.get_locations">get_locations</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_mainpandas" href="#coa.covid19.DataBase.get_mainpandas">get_mainpandas</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_posteriors" href="#coa.covid19.DataBase.get_posteriors">get_posteriors</a></code></li>
<li><code><a title="coa.covid19.DataBase.get_stats" href="#coa.covid19.DataBase.get_stats">get_stats</a></code></li>
<li><code><a title="coa.covid19.DataBase.merger" href="#coa.covid19.DataBase.merger">merger</a></code></li>
<li><code><a title="coa.covid19.DataBase.return_jhu_pandas" href="#coa.covid19.DataBase.return_jhu_pandas">return_jhu_pandas</a></code></li>
<li><code><a title="coa.covid19.DataBase.return_nonan_dates_pandas" href="#coa.covid19.DataBase.return_nonan_dates_pandas">return_nonan_dates_pandas</a></code></li>
<li><code><a title="coa.covid19.DataBase.return_structured_pandas" href="#coa.covid19.DataBase.return_structured_pandas">return_structured_pandas</a></code></li>
<li><code><a title="coa.covid19.DataBase.saveoutput" href="#coa.covid19.DataBase.saveoutput">saveoutput</a></code></li>
<li><code><a title="coa.covid19.DataBase.set_display" href="#coa.covid19.DataBase.set_display">set_display</a></code></li>
<li><code><a title="coa.covid19.DataBase.smooth_cases" href="#coa.covid19.DataBase.smooth_cases">smooth_cases</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>